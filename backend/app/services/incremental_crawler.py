"""
Â¢ûÈáèÊäìÂèñÊúçÂä°ÔºöÂÜ∑ÁÉ≠ÂèåÂÜô + Ê∞¥‰ΩçÁ∫øÊú∫Âà∂
"""
from __future__ import annotations

import logging
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Tuple

from sqlalchemy import select
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.posts_storage import PostRaw, PostHot
from app.models.community_cache import CommunityCache
from app.services.reddit_client import RedditPost, RedditAPIClient

logger = logging.getLogger(__name__)


def _unix_to_datetime(unix_timestamp: float) -> datetime:
    """Â∞Ü Unix Êó∂Èó¥Êà≥ËΩ¨Êç¢‰∏∫ UTC datetime"""
    return datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)


class IncrementalCrawler:
    """
    Â¢ûÈáèÊäìÂèñÂô®ÔºöÂÆûÁé∞ÂÜ∑ÁÉ≠ÂèåÂÜô + Ê∞¥‰ΩçÁ∫øÊú∫Âà∂
    
    Ê†∏ÂøÉÂéüÂàôÔºö
    1. ÂÖàÂÜôÂÜ∑Â∫ìÔºàÊåÅ‰πÖÂ±ÇÔºâÔºåÂÜçÂÜôÁÉ≠ÁºìÂ≠ò
    2. ‰ΩøÁî®Ê∞¥‰ΩçÁ∫øÈÅøÂÖçÈáçÂ§çÊäìÂèñ
    3. ÂéªÈáçÁ≠ñÁï•Ôºö(source, source_post_id, text_norm_hash)
    4. SCD2 ÁâàÊú¨ËøΩË∏™
    """
    
    def __init__(
        self,
        db: AsyncSession,
        reddit_client: RedditAPIClient,
        hot_cache_ttl_hours: int = 24,
    ):
        self.db = db
        self.reddit_client = reddit_client
        self.hot_cache_ttl_hours = hot_cache_ttl_hours
    
    async def crawl_community_incremental(
        self,
        community_name: str,
        limit: int = 100,
        time_filter: str = "month",
    ) -> dict:
        """
        Â¢ûÈáèÊäìÂèñÂçï‰∏™Á§æÂå∫
        
        Args:
            community_name: Á§æÂå∫ÂêçÔºàÂ¶Ç "r/Entrepreneur"Ôºâ
            limit: ÊØèÊ¨°ÊäìÂèñÁöÑÂ∏ñÂ≠êÊï∞
            time_filter: Êó∂Èó¥ËåÉÂõ¥Ôºàweek/monthÔºâ
        
        Returns:
            {
                "community": str,
                "new_posts": int,
                "updated_posts": int,
                "duplicates": int,
                "watermark_updated": bool,
            }
        """
        start_time = datetime.now(timezone.utc)
        logger.info(f"üîÑ ÂºÄÂßãÂ¢ûÈáèÊäìÂèñÁ§æÂå∫: {community_name}")
        
        # 1. Ëé∑ÂèñÊ∞¥‰ΩçÁ∫ø
        watermark = await self._get_watermark(community_name)
        logger.info(f"üìç Ê∞¥‰ΩçÁ∫ø: last_seen_created_at={watermark}")
        
        # 2. ÊäìÂèñÊñ∞Â∏ñÂ≠ê
        raw_name = community_name[2:] if community_name.lower().startswith("r/") else community_name
        posts = await self.reddit_client.fetch_subreddit_posts(
            raw_name,
            limit=limit,
            time_filter=time_filter,
            sort="top",
        )
        
        if not posts:
            logger.warning(f"‚ö†Ô∏è {community_name}: Êú™ÊäìÂèñÂà∞‰ªª‰ΩïÂ∏ñÂ≠ê")
            return {
                "community": community_name,
                "new_posts": 0,
                "updated_posts": 0,
                "duplicates": 0,
                "watermark_updated": False,
            }
        
        # 3. ËøáÊª§ÔºöÂè™‰øùÁïôÊñ∞‰∫éÊ∞¥‰ΩçÁ∫øÁöÑÂ∏ñÂ≠ê
        if watermark:
            posts = [p for p in posts if _unix_to_datetime(p.created_utc) > watermark]
            logger.info(f"üîç ËøáÊª§ÂêéÂâ©‰Ωô {len(posts)} Êù°Êñ∞Â∏ñÂ≠êÔºàÊ∞¥‰ΩçÁ∫ø‰πãÂêéÔºâ")
        
        if not posts:
            logger.info(f"‚úÖ {community_name}: Êó†Êñ∞Â∏ñÂ≠êÔºåË∑≥Ëøá")
            return {
                "community": community_name,
                "new_posts": 0,
                "updated_posts": 0,
                "duplicates": 0,
                "watermark_updated": False,
            }
        
        # 4. ÂèåÂÜôÔºöÂÖàÂÜ∑Â∫ìÔºåÂÜçÁÉ≠ÁºìÂ≠ò
        new_count, updated_count, dup_count = await self._dual_write(
            community_name, posts
        )
        
        # 5. Êõ¥Êñ∞Ê∞¥‰ΩçÁ∫ø
        latest_post = max(posts, key=lambda p: p.created_utc)
        await self._update_watermark(
            community_name,
            latest_post.id,
            _unix_to_datetime(latest_post.created_utc),
            total_fetched=len(posts),
            dedup_rate=(dup_count / len(posts) * 100) if posts else 0,
        )
        
        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        logger.info(
            f"‚úÖ {community_name}: Êñ∞Â¢û {new_count}, Êõ¥Êñ∞ {updated_count}, "
            f"ÂéªÈáç {dup_count}, ËÄóÊó∂ {duration:.2f}s"
        )
        
        return {
            "community": community_name,
            "new_posts": new_count,
            "updated_posts": updated_count,
            "duplicates": dup_count,
            "watermark_updated": True,
            "duration_seconds": duration,
        }
    
    async def _get_watermark(self, community_name: str) -> Optional[datetime]:
        """Ëé∑ÂèñÁ§æÂå∫ÁöÑÊ∞¥‰ΩçÁ∫øÔºàÊúÄÂêéÊäìÂèñÁöÑÂ∏ñÂ≠êÂàõÂª∫Êó∂Èó¥Ôºâ"""
        result = await self.db.execute(
            select(CommunityCache.last_seen_created_at)
            .where(CommunityCache.community_name == community_name)
        )
        row = result.scalar_one_or_none()
        return row if row else None
    
    async def _dual_write(
        self,
        community_name: str,
        posts: List[RedditPost],
    ) -> Tuple[int, int, int]:
        """
        ÂèåÂÜôÔºöÂÖàÂÜ∑Â∫ìÔºåÂÜçÁÉ≠ÁºìÂ≠ò
        
        Returns:
            (new_count, updated_count, duplicate_count)
        """
        new_count = 0
        updated_count = 0
        dup_count = 0
        
        for post in posts:
            # 1. ÂÜôÂÖ•ÂÜ∑Â∫ìÔºàÂ¢ûÈáè upsertÔºâ
            is_new, is_updated = await self._upsert_to_cold_storage(community_name, post)
            
            if is_new:
                new_count += 1
            elif is_updated:
                updated_count += 1
            else:
                dup_count += 1
            
            # 2. ÂÜôÂÖ•ÁÉ≠ÁºìÂ≠òÔºàË¶ÜÁõñÂºèÔºâ
            await self._upsert_to_hot_cache(community_name, post)
        
        # Êèê‰∫§‰∫ãÂä°
        await self.db.commit()
        
        return new_count, updated_count, dup_count
    
    async def _upsert_to_cold_storage(
        self,
        community_name: str,
        post: RedditPost,
    ) -> Tuple[bool, bool]:
        """
        Upsert Âà∞ÂÜ∑Â∫ìÔºàposts_rawÔºâ
        
        Returns:
            (is_new, is_updated)
        """
        # ÊûÑÈÄ† upsert ËØ≠Âè•
        stmt = pg_insert(PostRaw).values(
            source="reddit",
            source_post_id=post.id,
            version=1,
            created_at=_unix_to_datetime(post.created_utc),
            fetched_at=datetime.now(timezone.utc),
            author_id=post.author,
            author_name=post.author,
            title=post.title,
            body=post.selftext or "",
            url=post.url,
            subreddit=community_name,
            score=post.score,
            num_comments=post.num_comments,
            extra_data={
                "permalink": post.permalink,
                "upvote_ratio": getattr(post, "upvote_ratio", None),
            },
        )

        # ON CONFLICT: Â¶ÇÊûúÂ∑≤Â≠òÂú®ÔºåÊ£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÊõ¥Êñ∞ÁâàÊú¨
        stmt = stmt.on_conflict_do_update(
            index_elements=["source", "source_post_id", "version"],
            set_={
                "score": stmt.excluded.score,
                "num_comments": stmt.excluded.num_comments,
                "fetched_at": stmt.excluded.fetched_at,
            },
        )
        
        # ÊâßË°åÂπ∂ËøîÂõûÊòØÂê¶Êñ∞Â¢û/Êõ¥Êñ∞
        # Ê≥®ÊÑèÔºöËøôÈáåÁÆÄÂåñÂ§ÑÁêÜÔºåÂÆûÈôÖÂ∫îËØ•Ê£ÄÊü• text_norm_hash Âà§Êñ≠ÊòØÂê¶ÁºñËæë
        await self.db.execute(stmt)
        
        # TODO: ÂÆûÁé∞ SCD2 ÁâàÊú¨ËøΩË∏™ÔºàÊ£ÄÊµãÁºñËæëÔºâ
        return True, False  # ÊöÇÊó∂ËøîÂõû (is_new=True, is_updated=False)
    
    async def _upsert_to_hot_cache(
        self,
        community_name: str,
        post: RedditPost,
    ) -> None:
        """Upsert Âà∞ÁÉ≠ÁºìÂ≠òÔºàposts_hotÔºâ"""
        expires_at = datetime.now(timezone.utc) + timedelta(hours=self.hot_cache_ttl_hours)
        
        stmt = pg_insert(PostHot).values(
            source="reddit",
            source_post_id=post.id,
            created_at=_unix_to_datetime(post.created_utc),
            cached_at=datetime.now(timezone.utc),
            expires_at=expires_at,
            title=post.title,
            body=post.selftext or "",
            subreddit=community_name,
            score=post.score,
            num_comments=post.num_comments,
            extra_data={
                "permalink": post.permalink,
            },
        )

        # ON CONFLICT: Ë¶ÜÁõñÂºèÊõ¥Êñ∞
        stmt = stmt.on_conflict_do_update(
            index_elements=["source", "source_post_id"],
            set_={
                "cached_at": stmt.excluded.cached_at,
                "expires_at": stmt.excluded.expires_at,
                "score": stmt.excluded.score,
                "num_comments": stmt.excluded.num_comments,
                "title": stmt.excluded.title,
                "body": stmt.excluded.body,
                "extra_data": stmt.excluded.extra_data,
            },
        )
        
        await self.db.execute(stmt)
    
    async def _update_watermark(
        self,
        community_name: str,
        last_seen_post_id: str,
        last_seen_created_at: datetime,
        total_fetched: int,
        dedup_rate: float,
    ) -> None:
        """Êõ¥Êñ∞Ê∞¥‰ΩçÁ∫ø"""
        await self.db.execute(
            pg_insert(CommunityCache)
            .values(
                community_name=community_name,
                last_seen_post_id=last_seen_post_id,
                last_seen_created_at=last_seen_created_at,
                total_posts_fetched=total_fetched,
                dedup_rate=dedup_rate,
                last_crawled_at=datetime.now(timezone.utc),
            )
            .on_conflict_do_update(
                index_elements=["community_name"],
                set_={
                    "last_seen_post_id": last_seen_post_id,
                    "last_seen_created_at": last_seen_created_at,
                    "total_posts_fetched": CommunityCache.total_posts_fetched + total_fetched,
                    "dedup_rate": dedup_rate,
                    "last_crawled_at": datetime.now(timezone.utc),
                },
            )
        )
        await self.db.commit()


__all__ = ["IncrementalCrawler"]

