"""
Â¢ûÈáèÊäìÂèñÊúçÂä°ÔºöÂÜ∑ÁÉ≠ÂèåÂÜô + Ê∞¥‰ΩçÁ∫øÊú∫Âà∂
"""
from __future__ import annotations

import logging
from datetime import datetime, timedelta, timezone
from typing import Any, List, Optional, Tuple

from sqlalchemy import select
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.community_cache import CommunityCache
from app.models.crawl_metrics import CrawlMetrics
from app.models.posts_storage import PostHot, PostRaw
from app.services.reddit_client import RedditAPIClient, RedditPost

logger = logging.getLogger(__name__)


def _unix_to_datetime(unix_timestamp: float) -> datetime:
    """Â∞Ü Unix Êó∂Èó¥Êà≥ËΩ¨Êç¢‰∏∫ UTC datetime"""
    return datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)


class IncrementalCrawler:
    """
    Â¢ûÈáèÊäìÂèñÂô®ÔºöÂÆûÁé∞ÂÜ∑ÁÉ≠ÂèåÂÜô + Ê∞¥‰ΩçÁ∫øÊú∫Âà∂

    Ê†∏ÂøÉÂéüÂàôÔºö
    1. ÂÖàÂÜôÂÜ∑Â∫ìÔºàÊåÅ‰πÖÂ±ÇÔºâÔºåÂÜçÂÜôÁÉ≠ÁºìÂ≠ò
    2. ‰ΩøÁî®Ê∞¥‰ΩçÁ∫øÈÅøÂÖçÈáçÂ§çÊäìÂèñ
    3. ÂéªÈáçÁ≠ñÁï•Ôºö(source, source_post_id, text_norm_hash)
    4. SCD2 ÁâàÊú¨ËøΩË∏™
    """

    def __init__(
        self,
        db: AsyncSession,
        reddit_client: RedditAPIClient,
        hot_cache_ttl_hours: int = 24,
    ):
        self.db = db
        self.reddit_client = reddit_client
        self.hot_cache_ttl_hours = hot_cache_ttl_hours

    async def crawl_community_incremental(
        self,
        community_name: str,
        limit: int = 100,
        time_filter: str = "month",
        sort: str = "top",
    ) -> dict[str, Any]:
        """
        Â¢ûÈáèÊäìÂèñÂçï‰∏™Á§æÂå∫

        Args:
            community_name: Á§æÂå∫ÂêçÔºàÂ¶Ç "r/Entrepreneur"Ôºâ
            limit: ÊØèÊ¨°ÊäìÂèñÁöÑÂ∏ñÂ≠êÊï∞
            time_filter: Êó∂Èó¥ËåÉÂõ¥Ôºàweek/month/year/allÔºâ
            sort: ÊéíÂ∫èÁ≠ñÁï•Ôºàtop/new/hot/risingÔºâ

        Returns:
            {
                "community": str,
                "new_posts": int,
                "updated_posts": int,
                "duplicates": int,
                "watermark_updated": bool,
            }
        """
        start_time = datetime.now(timezone.utc)
        logger.info(f"üîÑ ÂºÄÂßãÂ¢ûÈáèÊäìÂèñÁ§æÂå∫: {community_name}")

        # 1. Ëé∑ÂèñÊ∞¥‰ΩçÁ∫ø
        watermark = await self._get_watermark(community_name)
        logger.info(f"üìç Ê∞¥‰ΩçÁ∫ø: last_seen_created_at={watermark}")

        # 2. ÊäìÂèñÊñ∞Â∏ñÂ≠ê
        raw_name = (
            community_name[2:]
            if community_name.lower().startswith("r/")
            else community_name
        )
        try:
            posts = await self.reddit_client.fetch_subreddit_posts(
                raw_name,
                limit=limit,
                time_filter=time_filter,
                sort=sort,
            )
        except Exception as e:
            logger.error(f"‚ùå {community_name}: ÊäìÂèñÂ§±Ë¥• - {e}")
            # ËÆ∞ÂΩïÂ§±Ë¥•ÊåáÊ†á
            now = datetime.now(timezone.utc)
            await self.db.execute(
                pg_insert(CommunityCache)
                .values(
                    community_name=community_name,
                    last_crawled_at=now,
                    posts_cached=0,
                    ttl_seconds=3600,
                    quality_score=Decimal("0.50"),
                    hit_count=0,
                    crawl_priority=50,
                    crawl_frequency_hours=2,
                    is_active=True,
                    empty_hit=0,
                    success_hit=0,
                    failure_hit=1,
                    avg_valid_posts=0,
                    quality_tier="medium",
                )
                .on_conflict_do_update(
                    index_elements=["community_name"],
                    set_={
                        "last_crawled_at": now,
                        "failure_hit": CommunityCache.failure_hit + 1,
                    },
                )
            )
            await self.db.commit()
            raise

        if not posts:
            logger.warning(f"‚ö†Ô∏è {community_name}: Êú™ÊäìÂèñÂà∞‰ªª‰ΩïÂ∏ñÂ≠ê")
            # ËÆ°ÂÖ• empty_hit
            now = datetime.now(timezone.utc)
            await self.db.execute(
                pg_insert(CommunityCache)
                .values(
                    community_name=community_name,
                    last_crawled_at=now,
                )
                .on_conflict_do_update(
                    index_elements=["community_name"],
                    set_={
                        "empty_hit": CommunityCache.empty_hit + 1,
                        "last_crawled_at": now,
                    },
                )
            )
            await self.db.commit()

            # T1.4 ÂüãÁÇπÔºöËÆ∞ÂΩïÁ©∫ÁªìÊûú
            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            await self._record_crawl_metrics(
                empty_crawls=1,
                avg_latency_seconds=duration,
            )

            return {
                "community": community_name,
                "new_posts": 0,
                "updated_posts": 0,
                "duplicates": 0,
                "watermark_updated": False,
            }

        # 3. ËøáÊª§ÔºöÂè™‰øùÁïôÊñ∞‰∫éÊ∞¥‰ΩçÁ∫øÁöÑÂ∏ñÂ≠ê
        if watermark:
            posts = [p for p in posts if _unix_to_datetime(p.created_utc) > watermark]
            logger.info(f"üîç ËøáÊª§ÂêéÂâ©‰Ωô {len(posts)} Êù°Êñ∞Â∏ñÂ≠êÔºàÊ∞¥‰ΩçÁ∫ø‰πãÂêéÔºâ")

        if not posts:
            logger.info(f"‚úÖ {community_name}: Êó†Êñ∞Â∏ñÂ≠êÔºåË∑≥Ëøá")
            return {
                "community": community_name,
                "new_posts": 0,
                "updated_posts": 0,
                "duplicates": 0,
                "watermark_updated": False,
            }

        # 4. ÂèåÂÜôÔºöÂÖàÂÜ∑Â∫ìÔºåÂÜçÁÉ≠ÁºìÂ≠ò
        new_count, updated_count, dup_count = await self._dual_write(
            community_name, posts
        )

        # 5. Êõ¥Êñ∞Ê∞¥‰ΩçÁ∫ø
        latest_post = max(posts, key=lambda p: p.created_utc)
        await self._update_watermark(
            community_name,
            latest_post.id,
            _unix_to_datetime(latest_post.created_utc),
            total_fetched=len(posts),
            new_valid_posts=new_count,
            dedup_rate=(dup_count / len(posts) * 100) if posts else 0,
        )

        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        logger.info(
            f"‚úÖ {community_name}: Êñ∞Â¢û {new_count}, Êõ¥Êñ∞ {updated_count}, "
            f"ÂéªÈáç {dup_count}, ËÄóÊó∂ {duration:.2f}s"
        )

        # T1.4 ÂüãÁÇπÔºöËÆ∞ÂΩïÊàêÂäüÊäìÂèñ
        await self._record_crawl_metrics(
            successful_crawls=1,
            total_new_posts=new_count,
            total_updated_posts=updated_count,
            total_duplicates=dup_count,
            avg_latency_seconds=duration,
        )

        return {
            "community": community_name,
            "new_posts": new_count,
            "updated_posts": updated_count,
            "duplicates": dup_count,
            "watermark_updated": True,
            "duration_seconds": duration,
        }

    async def _get_watermark(self, community_name: str) -> Optional[datetime]:
        """Ëé∑ÂèñÁ§æÂå∫ÁöÑÊ∞¥‰ΩçÁ∫øÔºàÊúÄÂêéÊäìÂèñÁöÑÂ∏ñÂ≠êÂàõÂª∫Êó∂Èó¥Ôºâ"""
        result = await self.db.execute(
            select(CommunityCache.last_seen_created_at).where(
                CommunityCache.community_name == community_name
            )
        )
        row = result.scalar_one_or_none()
        return row if row else None

    async def _dual_write(
        self,
        community_name: str,
        posts: List[RedditPost],
    ) -> Tuple[int, int, int]:
        """
        ÂèåÂÜôÔºöÂÖàÂÜ∑Â∫ìÔºåÂÜçÁÉ≠ÁºìÂ≠ò

        Returns:
            (new_count, updated_count, duplicate_count)
        """
        new_count = 0
        updated_count = 0
        dup_count = 0

        for post in posts:
            # 1. ÂÜôÂÖ•ÂÜ∑Â∫ìÔºàÂ¢ûÈáè upsertÔºâ
            is_new, is_updated = await self._upsert_to_cold_storage(
                community_name, post
            )

            if is_new:
                new_count += 1
            elif is_updated:
                updated_count += 1
            else:
                dup_count += 1

            # 2. ÂÜôÂÖ•ÁÉ≠ÁºìÂ≠òÔºàË¶ÜÁõñÂºèÔºâ
            await self._upsert_to_hot_cache(community_name, post)

        # Êèê‰∫§‰∫ãÂä°
        await self.db.commit()

        return new_count, updated_count, dup_count

    async def _upsert_to_cold_storage(
        self,
        community_name: str,
        post: RedditPost,
    ) -> Tuple[bool, bool]:
        """
        Upsert Âà∞ÂÜ∑Â∫ìÔºàposts_rawÔºâ

        Returns:
            (is_new, is_updated)
        """
        # ÂÖàÊ£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
        existing = await self.db.execute(
            select(PostRaw).where(
                PostRaw.source == "reddit",
                PostRaw.source_post_id == post.id,
                PostRaw.version == 1,
            )
        )
        existing_post = existing.scalar_one_or_none()

        # ÊûÑÈÄ† upsert ËØ≠Âè•
        # Ê≥®ÊÑèÔºöÂØπ‰∫é metadata ÂàóÔºå‰ΩøÁî® PostRaw.extra_dataÔºàPython Â±ûÊÄßÂêçÔºâ
        # SQLAlchemy ‰ºöËá™Âä®Êò†Â∞ÑÂà∞Êï∞ÊçÆÂ∫ìÂàó "metadata"
        stmt = pg_insert(PostRaw).values(
            source="reddit",
            source_post_id=post.id,
            version=1,
            created_at=_unix_to_datetime(post.created_utc),
            fetched_at=datetime.now(timezone.utc),
            author_id=post.author,
            author_name=post.author,
            title=post.title,
            body=post.selftext or "",
            url=post.url,
            subreddit=community_name,
            score=post.score,
            num_comments=post.num_comments,
        )
        # ÂçïÁã¨ËÆæÁΩÆ extra_dataÔºåÈÅøÂÖç‰∏é metadata ‰øùÁïôÂ≠óÂÜ≤Á™Å
        stmt = stmt.values({
            PostRaw.extra_data: {
                "permalink": post.permalink,
                "upvote_ratio": getattr(post, "upvote_ratio", None),
            }
        })

        if existing_post:
            # Â∑≤Â≠òÂú®ÔºöÊ£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÊõ¥Êñ∞
            is_updated = (
                existing_post.score != post.score
                or existing_post.num_comments != post.num_comments
            )

            if is_updated:
                # ON CONFLICT: Êõ¥Êñ∞ score/num_comments
                stmt = stmt.on_conflict_do_update(
                    index_elements=["source", "source_post_id", "version"],
                    set_={
                        "score": stmt.excluded.score,
                        "num_comments": stmt.excluded.num_comments,
                        "fetched_at": stmt.excluded.fetched_at,
                    },
                )
                await self.db.execute(stmt)
                return False, True  # (is_new=False, is_updated=True)
            else:
                # Êó†ÂèòÂåñÔºåË∑≥Ëøá
                return False, False  # (is_new=False, is_updated=False)
        else:
            # ‰∏çÂ≠òÂú®ÔºöÊñ∞Â¢û
            stmt = stmt.on_conflict_do_nothing(
                index_elements=["source", "source_post_id", "version"]
            )
            await self.db.execute(stmt)
            return True, False  # (is_new=True, is_updated=False)

    async def _upsert_to_hot_cache(
        self,
        community_name: str,
        post: RedditPost,
    ) -> None:
        """Upsert Âà∞ÁÉ≠ÁºìÂ≠òÔºàposts_hotÔºâ"""
        expires_at = datetime.now(timezone.utc) + timedelta(
            hours=self.hot_cache_ttl_hours
        )

        # Ê≥®ÊÑèÔºöÂØπ‰∫é metadata ÂàóÔºå‰ΩøÁî® PostHot.extra_dataÔºàPython Â±ûÊÄßÂêçÔºâ
        # SQLAlchemy ‰ºöËá™Âä®Êò†Â∞ÑÂà∞Êï∞ÊçÆÂ∫ìÂàó "metadata"
        stmt = pg_insert(PostHot).values(
            source="reddit",
            source_post_id=post.id,
            created_at=_unix_to_datetime(post.created_utc),
            cached_at=datetime.now(timezone.utc),
            expires_at=expires_at,
            title=post.title,
            body=post.selftext or "",
            subreddit=community_name,
            score=post.score,
            num_comments=post.num_comments,
        )
        # ÂçïÁã¨ËÆæÁΩÆ extra_dataÔºåÈÅøÂÖç‰∏é metadata ‰øùÁïôÂ≠óÂÜ≤Á™Å
        stmt = stmt.values({
            PostHot.extra_data: {
                "permalink": post.permalink,
            }
        })

        # ON CONFLICT: Ë¶ÜÁõñÂºèÊõ¥Êñ∞
        # Ê≥®ÊÑèÔºöÂØπ‰∫é metadata ÂàóÔºå‰ΩøÁî® PostHot.__table__.c.metadata
        stmt = stmt.on_conflict_do_update(
            index_elements=["source", "source_post_id"],
            set_={
                "cached_at": stmt.excluded.cached_at,
                "expires_at": stmt.excluded.expires_at,
                "score": stmt.excluded.score,
                "num_comments": stmt.excluded.num_comments,
                "title": stmt.excluded.title,
                "body": stmt.excluded.body,
                "metadata": stmt.excluded.metadata,
            },
        )

        await self.db.execute(stmt)

    async def _update_watermark(
        self,
        community_name: str,
        last_seen_post_id: str,
        last_seen_created_at: datetime,
        total_fetched: int,
        new_valid_posts: int,
        dedup_rate: float,
    ) -> None:
        """Êõ¥Êñ∞Ê∞¥‰ΩçÁ∫ø"""
        await self.db.execute(
            pg_insert(CommunityCache)
            .values(
                community_name=community_name,
                last_seen_post_id=last_seen_post_id,
                last_seen_created_at=last_seen_created_at,
                total_posts_fetched=total_fetched,
                dedup_rate=dedup_rate,
                last_crawled_at=datetime.now(timezone.utc),
                success_hit=1,
                avg_valid_posts=new_valid_posts,
            )
            .on_conflict_do_update(
                index_elements=["community_name"],
                set_={
                    "last_seen_post_id": last_seen_post_id,
                    "last_seen_created_at": last_seen_created_at,
                    "total_posts_fetched": CommunityCache.total_posts_fetched
                    + total_fetched,
                    "dedup_rate": dedup_rate,
                    "last_crawled_at": datetime.now(timezone.utc),
                    "success_hit": CommunityCache.success_hit + 1,
                    "avg_valid_posts": new_valid_posts,
                },
            )
        )
        await self.db.commit()

    async def _record_crawl_metrics(
        self,
        successful_crawls: int = 0,
        empty_crawls: int = 0,
        failed_crawls: int = 0,
        total_new_posts: int = 0,
        total_updated_posts: int = 0,
        total_duplicates: int = 0,
        avg_latency_seconds: float = 0.0,
    ) -> None:
        """
        ËÆ∞ÂΩïÊäìÂèñÊåáÊ†áÂà∞ crawl_metrics Ë°®ÔºàT1.4 ÂüãÁÇπÔºâ

        ÊØèÂ∞èÊó∂Ê±áÊÄª‰∏ÄÊ¨°ÔºåËÆ∞ÂΩïÂΩìÂâçÂ∞èÊó∂ÁöÑÊäìÂèñÁªüËÆ°
        """
        now = datetime.now(timezone.utc)
        metric_date = now.date()
        metric_hour = now.hour

        # Êü•ËØ¢ÂΩìÂâçÂ∞èÊó∂ÁöÑ valid_posts_24hÔºà‰ªé posts_hot Ë°®Ôºâ
        from sqlalchemy import func
        result = await self.db.execute(
            select(func.count(PostHot.source_post_id)).where(
                PostHot.cached_at >= now - timedelta(hours=24)
            )
        )
        valid_posts_24h = result.scalar() or 0

        # Êü•ËØ¢Ê¥ªË∑ÉÁ§æÂå∫ÊÄªÊï∞
        result = await self.db.execute(
            select(func.count(CommunityCache.community_name)).where(
                CommunityCache.is_active == True  # noqa: E712
            )
        )
        total_communities = result.scalar() or 0

        # ËÆ°ÁÆóÁºìÂ≠òÂëΩ‰∏≠ÁéáÔºàÂéªÈáçÁéáÁöÑÂÄíÊï∞Ôºâ
        total_posts = total_new_posts + total_updated_posts + total_duplicates
        cache_hit_rate = (
            (total_duplicates / total_posts * 100) if total_posts > 0 else 0.0
        )

        # Upsert Âà∞ crawl_metrics Ë°®
        await self.db.execute(
            pg_insert(CrawlMetrics)
            .values(
                metric_date=metric_date,
                metric_hour=metric_hour,
                cache_hit_rate=cache_hit_rate,
                valid_posts_24h=valid_posts_24h,
                total_communities=total_communities,
                successful_crawls=successful_crawls,
                empty_crawls=empty_crawls,
                failed_crawls=failed_crawls,
                avg_latency_seconds=avg_latency_seconds,
                total_new_posts=total_new_posts,
                total_updated_posts=total_updated_posts,
                total_duplicates=total_duplicates,
            )
            .on_conflict_do_update(
                index_elements=["metric_date", "metric_hour"],
                set_={
                    "cache_hit_rate": cache_hit_rate,
                    "valid_posts_24h": valid_posts_24h,
                    "total_communities": total_communities,
                    "successful_crawls": CrawlMetrics.successful_crawls + successful_crawls,
                    "empty_crawls": CrawlMetrics.empty_crawls + empty_crawls,
                    "failed_crawls": CrawlMetrics.failed_crawls + failed_crawls,
                    "avg_latency_seconds": avg_latency_seconds,
                    "total_new_posts": CrawlMetrics.total_new_posts + total_new_posts,
                    "total_updated_posts": CrawlMetrics.total_updated_posts + total_updated_posts,
                    "total_duplicates": CrawlMetrics.total_duplicates + total_duplicates,
                },
            )
        )
        await self.db.commit()

        logger.info(
            f"üìä ÂüãÁÇπËÆ∞ÂΩï: {metric_date} {metric_hour}:00 - "
            f"ÊàêÂäü={successful_crawls}, Á©∫ÁªìÊûú={empty_crawls}, Â§±Ë¥•={failed_crawls}, "
            f"Êñ∞Â¢û={total_new_posts}, Êõ¥Êñ∞={total_updated_posts}, ÂéªÈáç={total_duplicates}"
        )


__all__ = ["IncrementalCrawler"]
