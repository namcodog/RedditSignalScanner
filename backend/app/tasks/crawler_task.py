from __future__ import annotations

import asyncio
import logging
import os
from datetime import datetime, timedelta, timezone
from typing import Any, Iterable, List, Sequence, Tuple, TypeVar, cast

from celery.utils.log import get_task_logger  # type: ignore[import-untyped]
from sqlalchemy.dialects.postgresql import insert as pg_insert

from app.core.celery_app import celery_app
from app.core.config import Settings, get_settings
from app.db.session import SessionFactory
from app.models.community_cache import CommunityCache
from app.models.crawl_metrics import CrawlMetrics
from app.services.cache_manager import DEFAULT_CACHE_TTL_SECONDS, CacheManager
from app.services.community_cache_service import upsert_community_cache
from app.services.community_pool_loader import CommunityPoolLoader, CommunityProfile
from app.services.incremental_crawler import IncrementalCrawler

T = TypeVar("T")
from app.services.reddit_client import RedditAPIClient, RedditAPIError, RedditPost
from app.services.tiered_scheduler import TieredScheduler

logger = get_task_logger(__name__)
_MODULE_LOGGER = logging.getLogger(__name__)

DEFAULT_BATCH_SIZE = int(os.getenv("CRAWLER_BATCH_SIZE", "12"))
# æ•°æ®åº“æ“ä½œï¼šä½å¹¶å‘ï¼ˆ2ï¼‰é¿å… "concurrent operations are not permitted" é”™è¯¯
# å‚è€ƒï¼šhttps://docs.sqlalchemy.org/en/20/_modules/examples/asyncio/gather_orm_statements.html
DEFAULT_MAX_CONCURRENCY = int(os.getenv("CRAWLER_MAX_CONCURRENCY", "2"))
DEFAULT_POST_LIMIT = int(os.getenv("CRAWLER_POST_LIMIT", "100"))
DEFAULT_TIME_FILTER = os.getenv("CRAWLER_TIME_FILTER", "month")  # week/month/year/all
DEFAULT_SORT = os.getenv("CRAWLER_SORT", "top")  # top/new/hot/rising
DEFAULT_HOT_CACHE_TTL_HOURS = int(os.getenv("HOT_CACHE_TTL_HOURS", "24"))


def _chunked(
    items: Sequence[T], size: int
) -> Iterable[Sequence[T]]:
    if size <= 0:
        size = len(items) or 1
    for index in range(0, len(items), size):
        yield items[index : index + size]


async def _build_cache_manager(settings: Settings) -> CacheManager:
    return CacheManager(
        redis_url=settings.reddit_cache_redis_url,
        cache_ttl_seconds=settings.reddit_cache_ttl_seconds,
    )


async def _build_reddit_client(settings: Settings) -> RedditAPIClient:
    if not settings.reddit_client_id or not settings.reddit_client_secret:
        raise RuntimeError("Reddit API credentials are not configured.")

    return RedditAPIClient(
        settings.reddit_client_id,
        settings.reddit_client_secret,
        settings.reddit_user_agent,
        rate_limit=settings.reddit_rate_limit,
        rate_limit_window=settings.reddit_rate_limit_window_seconds,
        request_timeout=settings.reddit_request_timeout_seconds,
        max_concurrency=settings.reddit_max_concurrency,
    )


async def _crawl_single(
    community_name: str,
    *,
    settings: Settings,
    cache_manager: CacheManager,
    reddit_client: RedditAPIClient,
    post_limit: int,
) -> dict[str, Any]:
    start_time = datetime.now(timezone.utc)
    logger.info("å¼€å§‹çˆ¬å–ç¤¾åŒº: %s", community_name)

    # Reddit API æœŸæœ›çš„ä¸å¸¦å‰ç¼€åç§°ï¼Œä¾‹å¦‚ 'Entrepreneur' è€Œä¸æ˜¯ 'r/Entrepreneur'
    raw_name = str(community_name).strip()
    api_subreddit = raw_name[2:] if raw_name.lower().startswith("r/") else raw_name

    posts: List[RedditPost] = await reddit_client.fetch_subreddit_posts(
        api_subreddit,
        limit=post_limit,
        time_filter="week",
        sort="top",
    )

    # å†…éƒ¨ç¼“å­˜ä¸æ•°æ®åº“ä»ç„¶ä½¿ç”¨å¸¦å‰ç¼€çš„ç¤¾åŒºåï¼Œä¿è¯ä¸ç¤¾åŒºæ± /é”®å‘½åä¸€è‡´
    await cache_manager.set_cached_posts(community_name, posts)
    await upsert_community_cache(
        community_name,
        posts_cached=len(posts),
        ttl_seconds=settings.reddit_cache_ttl_seconds or DEFAULT_CACHE_TTL_SECONDS,
        quality_score=None,
    )

    duration = (datetime.now(timezone.utc) - start_time).total_seconds()
    logger.info("âœ… %s: ç¼“å­˜ %s ä¸ªå¸–å­, è€—æ—¶ %.2f ç§’", community_name, len(posts), duration)

    return {
        "community": community_name,
        "posts_count": len(posts),
        "status": "success",
        "duration_seconds": duration,
    }


async def _crawl_seeds_impl(force_refresh: bool = False) -> dict[str, Any]:
    """æ—§ç‰ˆæŠ“å–ï¼šåªå†™ Redis ç¼“å­˜ï¼ˆä¿ç•™ç”¨äºå…¼å®¹ï¼‰"""
    settings = get_settings()
    cache_manager = await _build_cache_manager(settings)
    reddit_client = await _build_reddit_client(settings)

    async with reddit_client:
        async with SessionFactory() as db:
            loader = CommunityPoolLoader(db)
            if force_refresh:
                await loader.load_seed_communities()

            seeds = await loader.load_community_pool(force_refresh=force_refresh)
        # çˆ¬å–æ‰€æœ‰æ´»è·ƒç¤¾åŒºï¼ˆhigh, medium, low ä¼˜å…ˆçº§ï¼‰
        seed_profiles = [
            profile
            for profile in seeds
            if profile.tier.lower() in ("high", "medium", "low")
        ]

        if not seed_profiles:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç¤¾åŒºï¼Œæ£€æŸ¥ tier å­—æ®µ")
            return {"status": "skipped", "reason": "no_communities_to_crawl"}

        results: List[dict[str, Any]] = []
        semaphore = asyncio.Semaphore(max(1, DEFAULT_MAX_CONCURRENCY))

        async def runner(profile: CommunityProfile) -> dict[str, Any]:
            async with semaphore:
                return await _crawl_single(
                    profile.name,
                    settings=settings,
                    cache_manager=cache_manager,
                    reddit_client=reddit_client,
                    post_limit=DEFAULT_POST_LIMIT,
                )

        for batch in _chunked(seed_profiles, DEFAULT_BATCH_SIZE):
            batch_results = await asyncio.gather(
                *[runner(profile) for profile in batch],
                return_exceptions=True,
            )
            for profile, outcome in zip(batch, batch_results):
                if isinstance(outcome, Exception):
                    logger.warning("âŒ %s: æ‰¹é‡çˆ¬å–å¤±è´¥ - %s", profile.name, outcome)
                    results.append(
                        {
                            "community": profile.name,
                            "status": "failed",
                            "error": str(outcome),
                        }
                    )
                else:
                    results.append(cast(dict[str, Any], outcome))

        success_count = sum(1 for item in results if item.get("status") == "success")
        failure_count = len(results) - success_count

        # âœ… åœ¨è¿”å›ä¹‹å‰ï¼Œå†™å…¥ crawl_metrics å’Œæ‰§è¡Œ tier åˆ†é…
        async with SessionFactory() as metrics_db:
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_new = sum(r.get("posts_count", 0) for r in results if r.get("status") == "success")
            duration_values = [
                float(r.get("duration_seconds", 0))
                for r in results
                if isinstance(r.get("duration_seconds"), (int, float))
            ]
            avg_latency = (
                sum(duration_values) / len(duration_values) if duration_values else 0.0
            )
            empty_count = sum(
                1
                for r in results
                if r.get("status") == "success" and r.get("posts_count", 0) == 0
            )

            # å…ˆè®¡ç®— tier_assignments
            tier_assignments: dict[str, Any] | None = None
            try:
                scheduler = TieredScheduler(metrics_db)
                tier_assignments = await scheduler.calculate_assignments()
                await scheduler.apply_assignments(tier_assignments)
            except Exception:
                logger.exception("åˆ·æ–° quality_tier å¤±è´¥")

            # å†å†™å…¥ crawl_metricsï¼ˆåŒ…å« tier_assignmentsï¼‰
            try:
                now = datetime.now(timezone.utc)
                cache_hit_rate = (success_count / max(1, len(seed_profiles))) * 100.0
                logger.info(
                    f"å‡†å¤‡å†™å…¥ crawl_metrics: total={len(seed_profiles)}, success={success_count}, empty={empty_count}, failed={failure_count}"
                )

                # ä¼˜å…ˆå°è¯•ä½¿ç”¨ PostgreSQL UPSERTï¼›è‹¥ä¸å¯ç”¨ï¼ˆå¦‚æµ‹è¯•æ›¿æ¢äº†æ¨¡å‹æˆ–æ— æ‰§è¡Œå™¨ï¼‰ï¼Œåˆ™é€€å› ORM add()
                used_upsert = False
                try:
                    if hasattr(CrawlMetrics, "__table__") or hasattr(CrawlMetrics, "__mapper__"):
                        stmt = pg_insert(CrawlMetrics).values(
                            metric_date=now.date(),
                            metric_hour=now.hour,
                            cache_hit_rate=cache_hit_rate,
                            valid_posts_24h=total_new,
                            total_communities=len(seed_profiles),
                            successful_crawls=success_count,
                            empty_crawls=empty_count,
                            failed_crawls=failure_count,
                            avg_latency_seconds=avg_latency,
                            total_new_posts=total_new,
                            total_updated_posts=0,  # æ—§ç‰ˆæŠ“å–ä¸æ”¯æŒæ›´æ–°æ£€æµ‹
                            total_duplicates=0,      # æ—§ç‰ˆæŠ“å–ä¸æ”¯æŒå»é‡æ£€æµ‹
                            tier_assignments=tier_assignments,
                        )
                        stmt = stmt.on_conflict_do_update(
                            constraint="uq_crawl_metrics_date_hour",
                            set_={
                                "cache_hit_rate": stmt.excluded.cache_hit_rate,
                                "valid_posts_24h": stmt.excluded.valid_posts_24h,
                                "total_communities": stmt.excluded.total_communities,
                                "successful_crawls": CrawlMetrics.successful_crawls + success_count,
                                "empty_crawls": CrawlMetrics.empty_crawls + empty_count,
                                "failed_crawls": CrawlMetrics.failed_crawls + failure_count,
                                "avg_latency_seconds": stmt.excluded.avg_latency_seconds,
                                "total_new_posts": CrawlMetrics.total_new_posts + total_new,
                                "total_updated_posts": CrawlMetrics.total_updated_posts + 0,
                                "total_duplicates": CrawlMetrics.total_duplicates + 0,
                                "tier_assignments": stmt.excluded.tier_assignments,
                            },
                        )
                        await metrics_db.execute(stmt)
                        await metrics_db.commit()
                        logger.info("âœ… crawl_metrics upsert æˆåŠŸ")
                        used_upsert = True
                except Exception as exc:
                    logger.warning("crawl_metrics upsert å¤±è´¥ï¼Œå›é€€åˆ° add()ï¼š%s", exc)

                if not used_upsert:
                    metrics_obj = CrawlMetrics(
                        metric_date=now.date(),
                        metric_hour=now.hour,
                        cache_hit_rate=cache_hit_rate,
                        valid_posts_24h=total_new,
                        total_communities=len(seed_profiles),
                        successful_crawls=success_count,
                        empty_crawls=empty_count,
                        failed_crawls=failure_count,
                        avg_latency_seconds=avg_latency,
                        total_new_posts=total_new,
                        total_updated_posts=0,
                        total_duplicates=0,
                        tier_assignments=tier_assignments,
                    )
                    metrics_db.add(metrics_obj)
                    await metrics_db.commit()
                    logger.info("âœ… crawl_metrics æŒä¹…åŒ–æˆåŠŸï¼ˆfallback add()ï¼‰")
            except Exception:
                logger.exception("å†™å…¥ crawl_metrics å¤±è´¥")
                try:
                    await metrics_db.rollback()
                except Exception:
                    logger.exception("å›æ»š crawl_metrics äº‹åŠ¡å¤±è´¥")

        return {
            "status": "completed",
            "total": len(seed_profiles),
            "succeeded": success_count,
            "failed": failure_count,
            "communities": results,
            "tier_assignments": tier_assignments or {},
        }


async def _mark_failure_hit(community_name: str) -> None:
    """æ ‡è®°ç¤¾åŒºæŠ“å–å¤±è´¥æ¬¡æ•°ï¼Œä½¿ç”¨ AUTOCOMMIT é¿å…å¹¶å‘å†²çª"""
    now = datetime.now(timezone.utc)
    async with SessionFactory() as db:
        # ä½¿ç”¨ AUTOCOMMIT éš”ç¦»çº§åˆ«å‡å°‘äº‹åŠ¡ç«äº‰
        await db.connection(execution_options={"isolation_level": "AUTOCOMMIT"})
        await db.execute(
            pg_insert(CommunityCache)
            .values(community_name=community_name, last_crawled_at=now)
            .on_conflict_do_update(
                index_elements=["community_name"],
                set_={
                    "failure_hit": CommunityCache.failure_hit + 1,
                    "last_crawled_at": now,
                },
            )
        )
        # AUTOCOMMIT æ¨¡å¼ä¸‹ä¸éœ€è¦æ‰‹åŠ¨ commit
        # await db.commit()


async def _crawl_seeds_incremental_impl(force_refresh: bool = False) -> dict[str, Any]:
    """æ–°ç‰ˆå¢é‡æŠ“å–ï¼šå†·çƒ­åŒå†™ + æ°´ä½çº¿æœºåˆ¶"""
    settings = get_settings()
    reddit_client = await _build_reddit_client(settings)

    async with reddit_client:
        async with SessionFactory() as db:
            loader = CommunityPoolLoader(db)
            if force_refresh:
                await loader.load_seed_communities()

            seeds = await loader.load_community_pool(force_refresh=force_refresh)
            seed_profiles = [
                profile
                for profile in seeds
                if profile.tier.lower() in ("high", "medium", "low")
            ]

            if not seed_profiles:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç¤¾åŒº")
                return {"status": "skipped", "reason": "no_communities_to_crawl"}

            # åˆ›å»ºå¢é‡æŠ“å–å™¨
            results: List[dict[str, Any]] = []
            semaphore = asyncio.Semaphore(max(1, DEFAULT_MAX_CONCURRENCY))

            async def runner(profile: CommunityProfile) -> dict[str, Any]:
                async with semaphore:
                    async with SessionFactory() as crawl_session:
                        # ä½¿ç”¨ AUTOCOMMIT éš”ç¦»çº§åˆ«å‡å°‘äº‹åŠ¡ç«äº‰
                        # å‚è€ƒï¼šhttps://docs.sqlalchemy.org/en/20/_modules/examples/asyncio/gather_orm_statements.html
                        await crawl_session.connection(
                            execution_options={"isolation_level": "AUTOCOMMIT"}
                        )
                        crawler = IncrementalCrawler(
                            db=crawl_session,
                            reddit_client=reddit_client,
                            hot_cache_ttl_hours=DEFAULT_HOT_CACHE_TTL_HOURS,
                        )
                        return await crawler.crawl_community_incremental(
                            profile.name,
                            limit=DEFAULT_POST_LIMIT,
                            time_filter=DEFAULT_TIME_FILTER,
                            sort=DEFAULT_SORT,
                        )

            for batch in _chunked(seed_profiles, DEFAULT_BATCH_SIZE):
                batch_results = await asyncio.gather(
                    *[runner(profile) for profile in batch],
                    return_exceptions=True,
                )
                for profile, outcome in zip(batch, batch_results):
                    if isinstance(outcome, Exception):
                        logger.warning("âŒ %s: å¢é‡çˆ¬å–å¤±è´¥ - %s", profile.name, outcome)
                        # å¤±è´¥è®¡æ•°ï¼ˆfailure_hit += 1ï¼‰
                        try:
                            await _mark_failure_hit(profile.name)
                        except Exception:
                            logger.exception("è®¡æ•° failure_hit å¤±è´¥ï¼š%s", profile.name)
                        results.append(
                            {
                                "community": profile.name,
                                "status": "failed",
                                "error": str(outcome),
                            }
                        )
                    else:
                        results.append(cast(dict[str, Any], outcome))

            total_new = sum(r.get("new_posts", 0) for r in results)
            total_updated = sum(r.get("updated_posts", 0) for r in results)
            total_dup = sum(r.get("duplicates", 0) for r in results)
            success_count = sum(1 for r in results if r.get("watermark_updated", False))
            failed_count = sum(1 for r in results if r.get("status") == "failed")
            empty_count = sum(
                1
                for r in results
                if r.get("status") != "failed" and r.get("new_posts", 0) == 0
            )
            duration_values = [
                float(r.get("duration_seconds", 0))
                for r in results
                if isinstance(r.get("duration_seconds"), (int, float))
            ]
            avg_latency = (
                sum(duration_values) / len(duration_values) if duration_values else 0.0
            )

            # æŒ‡æ ‡ç›‘æ§ï¼ˆT1.3ï¼‰
            tier_assignments: dict[str, Any] | None = None

            # å…ˆè®¡ç®— tier_assignments
            try:
                scheduler = TieredScheduler(db)
                tier_assignments = await scheduler.calculate_assignments()
                await scheduler.apply_assignments(tier_assignments)
            except Exception:
                _MODULE_LOGGER.exception("åˆ·æ–° quality_tier å¤±è´¥")

            # å†å†™å…¥ crawl_metricsï¼ˆåŒ…å« tier_assignmentsï¼‰
            try:
                now = datetime.now(timezone.utc)
                cache_hit_rate = (success_count / max(1, len(seed_profiles))) * 100.0
                _MODULE_LOGGER.info(
                    f"å‡†å¤‡å†™å…¥ crawl_metrics: total={len(seed_profiles)}, success={success_count}, empty={empty_count}, failed={failed_count}"
                )

                used_upsert = False
                try:
                    if hasattr(CrawlMetrics, "__table__") or hasattr(CrawlMetrics, "__mapper__"):
                        stmt = pg_insert(CrawlMetrics).values(
                            metric_date=now.date(),
                            metric_hour=now.hour,
                            cache_hit_rate=cache_hit_rate,
                            valid_posts_24h=total_new,  # æš‚ä»¥æœ¬è½®æ–°å¢ä½œä¸ºè¿‘ä¼¼ï¼Œåç»­åœ¨ T1.4/T1.7 ä¼˜åŒ–å£å¾„
                            total_communities=len(seed_profiles),
                            successful_crawls=success_count,
                            empty_crawls=empty_count,
                            failed_crawls=failed_count,
                            avg_latency_seconds=avg_latency,
                            total_new_posts=total_new,
                            total_updated_posts=total_updated,
                            total_duplicates=total_dup,
                            tier_assignments=tier_assignments,
                        )
                        stmt = stmt.on_conflict_do_update(
                            constraint="uq_crawl_metrics_date_hour",
                            set_={
                                "cache_hit_rate": stmt.excluded.cache_hit_rate,
                                "valid_posts_24h": stmt.excluded.valid_posts_24h,
                                "total_communities": stmt.excluded.total_communities,
                                "successful_crawls": CrawlMetrics.successful_crawls + success_count,
                                "empty_crawls": CrawlMetrics.empty_crawls + empty_count,
                                "failed_crawls": CrawlMetrics.failed_crawls + failed_count,
                                "avg_latency_seconds": stmt.excluded.avg_latency_seconds,
                                "total_new_posts": CrawlMetrics.total_new_posts + total_new,
                                "total_updated_posts": CrawlMetrics.total_updated_posts + total_updated,
                                "total_duplicates": CrawlMetrics.total_duplicates + total_dup,
                                "tier_assignments": stmt.excluded.tier_assignments,
                            },
                        )
                        await db.execute(stmt)
                        await db.commit()
                        _MODULE_LOGGER.info("âœ… crawl_metrics upsert æˆåŠŸ")
                        used_upsert = True
                except Exception as exc:
                    _MODULE_LOGGER.warning("crawl_metrics upsert å¤±è´¥ï¼Œå›é€€åˆ° add()ï¼š%s", exc)

                if not used_upsert:
                    metrics_obj = CrawlMetrics(
                        metric_date=now.date(),
                        metric_hour=now.hour,
                        cache_hit_rate=cache_hit_rate,
                        valid_posts_24h=total_new,
                        total_communities=len(seed_profiles),
                        successful_crawls=success_count,
                        empty_crawls=empty_count,
                        failed_crawls=failed_count,
                        avg_latency_seconds=avg_latency,
                        total_new_posts=total_new,
                        total_updated_posts=total_updated,
                        total_duplicates=total_dup,
                        tier_assignments=tier_assignments,
                    )
                    db.add(metrics_obj)
                    await db.commit()
                    _MODULE_LOGGER.info("âœ… crawl_metrics æŒä¹…åŒ–æˆåŠŸï¼ˆfallback add()ï¼‰")
            except Exception:
                _MODULE_LOGGER.exception("å†™å…¥ crawl_metrics å¤±è´¥")
                try:
                    await db.rollback()
                except Exception:
                    _MODULE_LOGGER.exception("å›æ»š crawl_metrics äº‹åŠ¡å¤±è´¥")

            return {
                "status": "completed",
                "mode": "incremental",
                "total": len(seed_profiles),
                "succeeded": success_count,
                "failed": len(results) - success_count,
                "total_new_posts": total_new,
                "total_updated_posts": total_updated,
                "total_duplicates": total_dup,
                "communities": results,
                "tier_assignments": tier_assignments or {},
            }


async def _crawl_single_impl(community_name: str) -> dict[str, Any]:
    settings = get_settings()
    cache_manager = await _build_cache_manager(settings)
    reddit_client = await _build_reddit_client(settings)
    async with reddit_client:
        return await _crawl_single(
            community_name,
            settings=settings,
            cache_manager=cache_manager,
            reddit_client=reddit_client,
            post_limit=DEFAULT_POST_LIMIT,
        )


@celery_app.task(name="tasks.crawler.crawl_community", bind=True, max_retries=3)  # type: ignore[misc]
def crawl_community(self: Any, community_name: str) -> dict[str, Any]:
    if not community_name:
        raise ValueError("community_name is required")
    try:
        return asyncio.run(_crawl_single_impl(community_name))
    except RedditAPIError as exc:
        logger.error("âŒ %s: Reddit API é”™è¯¯ - %s", community_name, exc)
        raise self.retry(exc=exc, countdown=60)
    except Exception as exc:  # pragma: no cover - Celery will capture traceback
        logger.exception("âŒ %s: æœªé¢„æœŸçš„é”™è¯¯", community_name)
        raise self.retry(exc=exc, countdown=60)


@celery_app.task(name="tasks.crawler.crawl_seed_communities")  # type: ignore[misc]
def crawl_seed_communities(force_refresh: bool = False) -> dict[str, Any]:
    """æ—§ç‰ˆæŠ“å–ä»»åŠ¡ï¼ˆåªå†™ Redis ç¼“å­˜ï¼‰"""
    try:
        return asyncio.run(_crawl_seeds_impl(force_refresh=force_refresh))
    except Exception:  # pragma: no cover - Celery records full traceback
        logger.exception("âŒ ç§å­ç¤¾åŒºæ‰¹é‡çˆ¬å–å¤±è´¥")
        raise


@celery_app.task(name="tasks.crawler.crawl_seed_communities_incremental")  # type: ignore[misc]
def crawl_seed_communities_incremental(force_refresh: bool = False) -> dict[str, Any]:
    """æ–°ç‰ˆå¢é‡æŠ“å–ä»»åŠ¡ï¼ˆå†·çƒ­åŒå†™ + æ°´ä½çº¿ï¼‰"""
    try:
        return asyncio.run(_crawl_seeds_incremental_impl(force_refresh=force_refresh))
    except Exception:  # pragma: no cover - Celery records full traceback
        logger.exception("âŒ å¢é‡æŠ“å–å¤±è´¥")
        raise


async def _crawl_low_quality_communities_impl() -> dict[str, Any]:
    """ç²¾å‡†è¡¥æŠ“ä½è´¨é‡ç¤¾åŒºï¼ˆT1.8ï¼‰

    æŸ¥è¯¢æ¡ä»¶ï¼š
    - last_crawled_at > 8hï¼ˆè¶…è¿‡ 8 å°æ—¶æœªæŠ“å–ï¼‰
    - avg_valid_posts < 50ï¼ˆå¹³å‡æœ‰æ•ˆå¸–å­æ•°ä½äº 50ï¼‰
    - is_active = Trueï¼ˆä»…æŠ“å–æ´»è·ƒç¤¾åŒºï¼‰

    å¤±è´¥å¤„ç†ï¼š
    - å¤±è´¥æ—¶å›å†™ empty_hit += 1
    """
    settings = get_settings()
    reddit_client = await _build_reddit_client(settings)

    async with reddit_client:
        async with SessionFactory() as db:
            # æŸ¥è¯¢ä½è´¨é‡ç¤¾åŒº
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=8)
            from sqlalchemy import select, and_

            result = await db.execute(
                select(CommunityCache.community_name)
                .where(
                    and_(
                        CommunityCache.last_crawled_at < cutoff_time,
                        CommunityCache.avg_valid_posts < 50,
                        CommunityCache.is_active == True,
                    )
                )
                .order_by(CommunityCache.last_crawled_at.asc())
                .limit(50)  # æ¯æ¬¡æœ€å¤šè¡¥æŠ“ 50 ä¸ªç¤¾åŒº
            )
            low_quality_communities = result.scalars().all()

            if not low_quality_communities:
                logger.info("âœ… æ²¡æœ‰éœ€è¦è¡¥æŠ“çš„ä½è´¨é‡ç¤¾åŒº")
                return {
                    "status": "skipped",
                    "reason": "no_low_quality_communities",
                    "total": 0,
                }

            logger.info(f"ğŸ¯ å‘ç° {len(low_quality_communities)} ä¸ªä½è´¨é‡ç¤¾åŒºéœ€è¦è¡¥æŠ“")

            # åˆ›å»ºå¢é‡æŠ“å–å™¨
            results: List[dict[str, Any]] = []
            semaphore = asyncio.Semaphore(max(1, DEFAULT_MAX_CONCURRENCY))

            async def runner(community_name: str) -> dict[str, Any]:
                async with semaphore:
                    async with SessionFactory() as crawl_session:
                        await crawl_session.connection(
                            execution_options={"isolation_level": "AUTOCOMMIT"}
                        )
                        crawler = IncrementalCrawler(
                            db=crawl_session,
                            reddit_client=reddit_client,
                            hot_cache_ttl_hours=DEFAULT_HOT_CACHE_TTL_HOURS,
                        )
                        return await crawler.crawl_community_incremental(
                            community_name,
                            limit=DEFAULT_POST_LIMIT,
                            time_filter=DEFAULT_TIME_FILTER,
                            sort=DEFAULT_SORT,
                        )

            # åˆ†æ‰¹æŠ“å–
            community_list: list[str] = list(low_quality_communities)
            for batch in _chunked(community_list, DEFAULT_BATCH_SIZE):
                batch_results = await asyncio.gather(
                    *[runner(name) for name in batch],
                    return_exceptions=True,
                )
                for community_name, outcome in zip(batch, batch_results):
                    if isinstance(outcome, Exception):
                        logger.warning("âŒ %s: è¡¥æŠ“å¤±è´¥ - %s", community_name, outcome)
                        # å¤±è´¥æ—¶å›å†™ empty_hit += 1
                        try:
                            await _mark_empty_hit(community_name)
                        except Exception:
                            logger.exception("å›å†™ empty_hit å¤±è´¥ï¼š%s", community_name)
                        results.append(
                            {
                                "community": community_name,
                                "status": "failed",
                                "error": str(outcome),
                            }
                        )
                    else:
                        results.append(cast(dict[str, Any], outcome))

            # ç»Ÿè®¡ç»“æœ
            total_new = sum(r.get("new_posts", 0) for r in results)
            total_updated = sum(r.get("updated_posts", 0) for r in results)
            success_count = sum(1 for r in results if r.get("watermark_updated", False))
            failed_count = sum(1 for r in results if r.get("status") == "failed")
            empty_count = sum(
                1
                for r in results
                if r.get("status") != "failed" and r.get("new_posts", 0) == 0
            )

            logger.info(
                f"âœ… ä½è´¨é‡ç¤¾åŒºè¡¥æŠ“å®Œæˆ: æ€»æ•°={len(low_quality_communities)}, "
                f"æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, ç©ºç»“æœ={empty_count}, "
                f"æ–°å¢å¸–å­={total_new}"
            )

            return {
                "status": "completed",
                "mode": "low_quality_è¡¥æŠ“",
                "total": len(low_quality_communities),
                "succeeded": success_count,
                "failed": failed_count,
                "empty": empty_count,
                "total_new_posts": total_new,
                "total_updated_posts": total_updated,
                "communities": results,
            }


async def _mark_empty_hit(community_name: str) -> None:
    """æ ‡è®°ç¤¾åŒºç©ºç»“æœæ¬¡æ•°ï¼ˆç”¨äºè¡¥æŠ“å¤±è´¥ï¼‰ï¼Œä½¿ç”¨ AUTOCOMMIT é¿å…å¹¶å‘å†²çª"""
    now = datetime.now(timezone.utc)
    async with SessionFactory() as db:
        await db.connection(execution_options={"isolation_level": "AUTOCOMMIT"})
        await db.execute(
            pg_insert(CommunityCache)
            .values(community_name=community_name, last_crawled_at=now)
            .on_conflict_do_update(
                index_elements=["community_name"],
                set_={
                    "empty_hit": CommunityCache.empty_hit + 1,
                    "last_crawled_at": now,
                },
            )
        )


async def list_stale_caches(threshold_minutes: int = 90) -> List[Tuple[str, datetime]]:
    cutoff = datetime.now(timezone.utc) - timedelta(minutes=threshold_minutes)
    async with SessionFactory() as session:
        result = await session.execute(
            CommunityCache.__table__.select().where(
                CommunityCache.last_crawled_at < cutoff
            )
        )
        rows = result.fetchall()
        return [
            (row._mapping["community_name"], row._mapping["last_crawled_at"])
            for row in rows
            if row._mapping["last_crawled_at"] is not None
        ]
    # Fallback if session acquisition failed unexpectedly
    return []


@celery_app.task(name="tasks.crawler.crawl_low_quality_communities")  # type: ignore[misc]
def crawl_low_quality_communities() -> dict[str, Any]:
    """ç²¾å‡†è¡¥æŠ“ä½è´¨é‡ç¤¾åŒºä»»åŠ¡ï¼ˆT1.8ï¼‰"""
    try:
        return asyncio.run(_crawl_low_quality_communities_impl())
    except Exception:  # pragma: no cover - Celery records full traceback
        logger.exception("âŒ ä½è´¨é‡ç¤¾åŒºè¡¥æŠ“å¤±è´¥")
        raise


__all__ = [
    "crawl_community",
    "crawl_seed_communities",
    "crawl_seed_communities_incremental",
    "crawl_low_quality_communities",
    "list_stale_caches",
]
