"""å¯åŠ¨ 24 å°æ—¶ Warmup çˆ¬å–è®¡åˆ’ã€‚

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. åŠ è½½ç§å­ç¤¾åŒºåˆ°æ•°æ®åº“
2. å¯åŠ¨ Warmup Crawler ä»»åŠ¡
3. é…ç½® Celery Beat å®šæ—¶ä»»åŠ¡ï¼ˆæ¯ 2 å°æ—¶çˆ¬å–ä¸€æ¬¡ï¼‰
4. ç›‘æ§çˆ¬å–è¿›åº¦

Usage:
    cd backend
    export $(cat .env | grep -v '^#' | xargs)
    python scripts/start_24h_warmup.py
"""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path
from datetime import datetime, timezone, timedelta

# Ensure the backend package is importable
BACKEND_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BACKEND_DIR))

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import get_settings
from app.services.community_pool_loader import CommunityPoolLoader
from app.db.session import SessionFactory
from app.models.community_pool import CommunityPool
from app.models.community_cache import CommunityCache
from app.tasks.warmup_crawler import warmup_crawler_task


async def load_seed_communities() -> int:
    """åŠ è½½ç§å­ç¤¾åŒºåˆ°æ•°æ®åº“ã€‚"""
    print("=" * 60)
    print("ğŸ“‹ æ­¥éª¤ 1: åŠ è½½ç§å­ç¤¾åŒº")
    print("=" * 60)
    
    async with SessionFactory() as db:
        loader = CommunityPoolLoader(db=db)
        count = await loader.load_seed_communities()
        
        print(f"âœ… æˆåŠŸåŠ è½½ {count} ä¸ªç§å­ç¤¾åŒº")
        
        # ç»Ÿè®¡å„ä¼˜å…ˆçº§ç¤¾åŒºæ•°é‡
        result = await db.execute(
            select(
                CommunityPool.priority,
                func.count(CommunityPool.id).label("count")
            )
            .where(CommunityPool.is_active == True)
            .group_by(CommunityPool.priority)
            .order_by(CommunityPool.priority.desc())
        )
        
        stats = result.all()
        print("\nç¤¾åŒºä¼˜å…ˆçº§åˆ†å¸ƒï¼š")
        for priority, count in stats:
            print(f"  - ä¼˜å…ˆçº§ {priority}: {count} ä¸ªç¤¾åŒº")
        
        return count


async def check_existing_cache() -> dict:
    """æ£€æŸ¥ç°æœ‰ç¼“å­˜çŠ¶æ€ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ­¥éª¤ 2: æ£€æŸ¥ç°æœ‰ç¼“å­˜")
    print("=" * 60)
    
    async with SessionFactory() as db:
        # ç»Ÿè®¡ç¼“å­˜è®°å½•
        result = await db.execute(
            select(func.count(CommunityCache.id))
        )
        total_cached = result.scalar() or 0
        
        # ç»Ÿè®¡æœ€è¿‘ 24 å°æ—¶çš„ç¼“å­˜
        cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        result = await db.execute(
            select(func.count(CommunityCache.id))
            .where(CommunityCache.cached_at >= cutoff)
        )
        recent_cached = result.scalar() or 0
        
        # ç»Ÿè®¡æ€»å¸–å­æ•°
        result = await db.execute(
            select(func.sum(CommunityCache.post_count))
        )
        total_posts = result.scalar() or 0
        
        stats = {
            "total_cached_communities": total_cached,
            "recent_cached_communities": recent_cached,
            "total_posts": total_posts,
        }
        
        print(f"ç¼“å­˜ç»Ÿè®¡ï¼š")
        print(f"  - æ€»ç¼“å­˜ç¤¾åŒºæ•°: {total_cached}")
        print(f"  - æœ€è¿‘ 24 å°æ—¶ç¼“å­˜: {recent_cached}")
        print(f"  - æ€»å¸–å­æ•°: {total_posts}")
        
        if total_cached > 0:
            # æ˜¾ç¤ºæœ€è¿‘ç¼“å­˜çš„ç¤¾åŒº
            result = await db.execute(
                select(CommunityCache)
                .order_by(CommunityCache.cached_at.desc())
                .limit(5)
            )
            recent = result.scalars().all()
            
            print(f"\næœ€è¿‘ç¼“å­˜çš„ç¤¾åŒºï¼š")
            for cache in recent:
                age = datetime.now(timezone.utc) - cache.cached_at
                hours = age.total_seconds() / 3600
                print(f"  - r/{cache.community_name}: {cache.post_count} å¸–å­ ({hours:.1f} å°æ—¶å‰)")
        
        return stats


def start_warmup_crawler() -> str:
    """å¯åŠ¨ Warmup Crawler ä»»åŠ¡ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ­¥éª¤ 3: å¯åŠ¨ Warmup Crawler")
    print("=" * 60)
    
    try:
        # æäº¤ Celery ä»»åŠ¡
        result = warmup_crawler_task.delay()
        task_id = result.id
        
        print(f"âœ… Warmup Crawler ä»»åŠ¡å·²æäº¤")
        print(f"   ä»»åŠ¡ ID: {task_id}")
        print(f"   é¢„è®¡è€—æ—¶: 10-30 åˆ†é’Ÿï¼ˆå–å†³äºç¤¾åŒºæ•°é‡ï¼‰")
        print(f"\nç›‘æ§ä»»åŠ¡è¿›åº¦ï¼š")
        print(f"   tail -f /tmp/celery_worker.log | grep -E '(warmup|crawl)'")
        
        return task_id
    
    except Exception as e:
        print(f"âŒ å¯åŠ¨ Warmup Crawler å¤±è´¥: {e}")
        print(f"\nè¯·ç¡®ä¿ Celery Worker æ­£åœ¨è¿è¡Œï¼š")
        print(f"   ps aux | grep celery")
        raise


async def monitor_progress(task_id: str, duration_minutes: int = 30) -> None:
    """ç›‘æ§çˆ¬å–è¿›åº¦ã€‚"""
    print("\n" + "=" * 60)
    print(f"ğŸ“‹ æ­¥éª¤ 4: ç›‘æ§çˆ¬å–è¿›åº¦ï¼ˆ{duration_minutes} åˆ†é’Ÿï¼‰")
    print("=" * 60)
    
    start_time = datetime.now(timezone.utc)
    end_time = start_time + timedelta(minutes=duration_minutes)
    
    print(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    last_count = 0
    
    while datetime.now(timezone.utc) < end_time:
        async with SessionFactory() as db:
            # ç»Ÿè®¡ç¼“å­˜è®°å½•
            result = await db.execute(
                select(func.count(CommunityCache.id))
            )
            current_count = result.scalar() or 0
            
            # ç»Ÿè®¡æ€»å¸–å­æ•°
            result = await db.execute(
                select(func.sum(CommunityCache.post_count))
            )
            total_posts = result.scalar() or 0
            
            # è®¡ç®—è¿›åº¦
            elapsed = datetime.now(timezone.utc) - start_time
            elapsed_minutes = elapsed.total_seconds() / 60
            
            new_communities = current_count - last_count
            last_count = current_count
            
            print(f"[{elapsed_minutes:.1f} åˆ†é’Ÿ] å·²ç¼“å­˜ {current_count} ä¸ªç¤¾åŒºï¼Œ{total_posts} æ¡å¸–å­", end="")
            if new_communities > 0:
                print(f" (+{new_communities} æ–°å¢)", end="")
            print()
        
        # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡
        await asyncio.sleep(30)
    
    print(f"\nâœ… ç›‘æ§å®Œæˆ")


async def print_final_stats() -> None:
    """æ‰“å°æœ€ç»ˆç»Ÿè®¡ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("=" * 60)
    
    async with SessionFactory() as db:
        # æ€»ç¼“å­˜ç¤¾åŒºæ•°
        result = await db.execute(
            select(func.count(CommunityCache.id))
        )
        total_cached = result.scalar() or 0
        
        # æ€»å¸–å­æ•°
        result = await db.execute(
            select(func.sum(CommunityCache.post_count))
        )
        total_posts = result.scalar() or 0
        
        # å¹³å‡æ¯ä¸ªç¤¾åŒºçš„å¸–å­æ•°
        avg_posts = total_posts / total_cached if total_cached > 0 else 0
        
        print(f"æ€»ç¼“å­˜ç¤¾åŒºæ•°: {total_cached}")
        print(f"æ€»å¸–å­æ•°: {total_posts}")
        print(f"å¹³å‡æ¯ä¸ªç¤¾åŒºå¸–å­æ•°: {avg_posts:.1f}")
        
        # æ˜¾ç¤ºå‰ 10 ä¸ªç¤¾åŒº
        result = await db.execute(
            select(CommunityCache)
            .order_by(CommunityCache.post_count.desc())
            .limit(10)
        )
        top_communities = result.scalars().all()
        
        print(f"\nå‰ 10 ä¸ªç¤¾åŒºï¼ˆæŒ‰å¸–å­æ•°ï¼‰ï¼š")
        for i, cache in enumerate(top_communities, 1):
            print(f"  {i}. r/{cache.community_name}: {cache.post_count} å¸–å­")


async def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨ 24 å°æ—¶ Warmup çˆ¬å–è®¡åˆ’")
    print("=" * 60)
    print()
    
    try:
        # 1. åŠ è½½ç§å­ç¤¾åŒº
        count = await load_seed_communities()
        
        if count == 0:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ç§å­ç¤¾åŒºï¼Œé€€å‡º")
            sys.exit(1)
        
        # 2. æ£€æŸ¥ç°æœ‰ç¼“å­˜
        stats = await check_existing_cache()
        
        # 3. å¯åŠ¨ Warmup Crawler
        task_id = start_warmup_crawler()
        
        # 4. ç›‘æ§è¿›åº¦ï¼ˆå¯é€‰ï¼‰
        print("\næ˜¯å¦ç›‘æ§çˆ¬å–è¿›åº¦ï¼Ÿ(y/n): ", end="")
        choice = input().strip().lower()
        
        if choice == "y":
            print("ç›‘æ§æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼Œé»˜è®¤ 30ï¼‰: ", end="")
            duration_input = input().strip()
            duration = int(duration_input) if duration_input else 30
            
            await monitor_progress(task_id, duration)
            await print_final_stats()
        else:
            print("\nâœ… Warmup Crawler å·²åœ¨åå°è¿è¡Œ")
            print(f"\næŸ¥çœ‹è¿›åº¦ï¼š")
            print(f"   tail -f /tmp/celery_worker.log | grep -E '(warmup|crawl)'")
            print(f"\næŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡ï¼š")
            print(f"   psql -d reddit_scanner -c \"SELECT COUNT(*) FROM community_cache;\"")
        
        print("\n" + "=" * 60)
        print("âœ… 24 å°æ—¶ Warmup çˆ¬å–è®¡åˆ’å·²å¯åŠ¨ï¼")
        print("=" * 60)
        print()
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        print("   1. Warmup Crawler å°†åœ¨åå°æŒç»­è¿è¡Œ")
        print("   2. æ¯ 2 å°æ—¶è‡ªåŠ¨çˆ¬å–ä¸€æ¬¡ï¼ˆç”± Celery Beat è°ƒåº¦ï¼‰")
        print("   3. ç¼“å­˜æ•°æ®å°†ä¿å­˜åˆ° Redis å’Œ PostgreSQL")
        print("   4. ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨å‰ç«¯æäº¤çœŸå®çš„åˆ†æä»»åŠ¡")
        print()
        print("ğŸ” éªŒè¯ç¼“å­˜ï¼š")
        print("   psql -d reddit_scanner -c \"SELECT community_name, post_count, cached_at FROM community_cache ORDER BY cached_at DESC LIMIT 10;\"")
        print()
    
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

