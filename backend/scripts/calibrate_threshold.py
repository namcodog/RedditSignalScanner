#!/usr/bin/env python3
"""
é˜ˆå€¼æ ¡å‡†è„šæœ¬ - Phase 7 (User Story 5)

åŠŸèƒ½ï¼š
1. è¯»å–äººå·¥æ ‡æ³¨æ•°æ®
2. ä»æ•°æ®åº“è·å–å¸–å­çš„è¯„åˆ†æ•°æ®
3. è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ Precision@Kã€Recall@Kã€F1@K
4. ç½‘æ ¼æœç´¢æœ€ä¼˜é˜ˆå€¼
5. è¾“å‡ºæœ€ä¼˜é˜ˆå€¼é…ç½®

ç›®æ ‡ï¼šPrecision@50 â‰¥ 0.6

æ‰§è¡Œï¼špython scripts/calibrate_threshold.py --annotation-file ../data/annotations/real_annotated.csv
"""
import argparse
import asyncio
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import re

import numpy as np
import pandas as pd
import yaml
from sqlalchemy import select
from sklearn.linear_model import LogisticRegression

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
backend_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(backend_dir))

from app.db.session import get_session_context
from app.models.posts_storage import PostRaw

try:  # pragma: no cover - heuristic helper optional in tests
    from app.services.analysis.entity_matcher import EntityMatcher
except Exception:  # pragma: no cover - fallback when optional deps missing
    EntityMatcher = None  # type: ignore


ENTITY_MATCHER: EntityMatcher | None = None
if EntityMatcher is not None:  # pragma: no cover - best effort initialisation
    try:
        ENTITY_MATCHER = EntityMatcher()
    except Exception:
        ENTITY_MATCHER = None


INTENT_PHRASES = (
    "looking for",
    "any recommendations",
    "recommend",
    "how do you",
    "how are you",
    "what tool",
    "what do you use",
    "what software",
    "suggestions",
    "any advice",
    "need a",
    "need help",
    "is there a",
    "does anyone",
    "anyone using",
    "alternatives",
)

PAIN_KEYWORDS = (
    "pain",
    "painful",
    "problem",
    "issue",
    "struggle",
    "struggling",
    "frustrated",
    "frustrating",
    "blocked",
    "blocker",
    "manual",
    "time consuming",
    "takes forever",
    "hard",
    "difficult",
    "can't",
    "cannot",
)

ACTION_KEYWORDS = (
    "automation",
    "workflow",
    "process",
    "ops",
    "operations",
    "dashboard",
    "report",
    "reporting",
    "analytics",
    "insights",
    "crm",
    "pipeline",
    "integrations",
    "integration",
    "api",
    "tooling",
    "system",
)

BUYING_KEYWORDS = (
    "budget",
    "pricing",
    "cost",
    "pay",
    "paid",
    "subscription",
    "license",
    "roi",
)

SUBREDDIT_SIGNALS = {
    "r/startups": 0.12,
    "r/saas": 0.12,
    "r/entrepreneur": 0.08,
    "r/productmanagement": 0.08,
    "r/dataengineering": 0.07,
    "r/business": 0.06,
    "r/marketing": 0.05,
    "r/leanstartup": 0.1,
    "r/smallbusiness": 0.05,
    "r/projectmanagement": 0.08,
    "r/agile": 0.05,
    "r/growthhacking": 0.07,
}

SUBREDDIT_PENALTIES = {
    "r/rust": 0.12,
    "r/dotnet": 0.12,
    "r/java": 0.1,
    "r/progresspics": 0.2,
    "r/getdisciplined": 0.08,
    "r/selfimprovement": 0.05,
    "r/AskCulinary": 0.12,
    "r/budgetfood": 0.1,
    "r/Bitcoin": 0.1,
    "r/CryptoCurrency": 0.1,
    "r/wallstreetbets": 0.1,
}

PERSONAL_KEYWORDS = (
    "girlfriend",
    "boyfriend",
    "relationship",
    "parents",
    "school",
    "college",
    "university",
    "diet",
    "protein",
    "soup",
    "recipe",
    "cooking",
    "mental health",
    "anxiety",
    "depression",
    "weight loss",
    "workout",
)

QUESTION_PATTERN = re.compile(r"\?")


def _normalise(value: float, upper: float) -> float:
    if upper <= 0:
        return 0.0
    return max(0.0, min(value / upper, 1.0))


def _count_matches(text: str, keywords: tuple[str, ...]) -> int:
    return sum(1 for phrase in keywords if phrase in text)


def extract_features(post: PostRaw) -> dict[str, float]:
    title = (post.title or "").lower()
    body = (post.body or "").lower()
    combined = f"{title} {body}".strip()

    intent_hits = _count_matches(combined, INTENT_PHRASES)
    pain_hits = _count_matches(combined, PAIN_KEYWORDS)
    action_hits = _count_matches(combined, ACTION_KEYWORDS)
    buying_hits = _count_matches(combined, BUYING_KEYWORDS)
    personal_hits = _count_matches(combined, PERSONAL_KEYWORDS)

    question_flag = 1.0 if QUESTION_PATTERN.search(title) or QUESTION_PATTERN.search(body) else 0.0

    popularity = _normalise(float(post.score or 0), 400.0)
    comment_engagement = _normalise(float(post.num_comments or 0), 60.0)

    entity_brand = 0.0
    entity_feature = 0.0
    entity_pain = 0.0
    if ENTITY_MATCHER is not None:
        matches = ENTITY_MATCHER.match_text(combined)
        entity_brand = _normalise(float(len(matches.get("brands", []))), 2.0)
        entity_feature = _normalise(float(len(matches.get("features", []))), 3.0)
        entity_pain = _normalise(float(len(matches.get("pain_points", []))), 3.0)

    subreddit = (post.subreddit or "").lower()

    return {
        "intent_score": _normalise(intent_hits, 3.0),
        "pain_score": _normalise(pain_hits, 4.0),
        "action_score": _normalise(action_hits, 4.0),
        "buying_score": _normalise(buying_hits, 2.0),
        "question_flag": question_flag,
        "popularity": popularity,
        "comment_engagement": comment_engagement,
        "entity_brand": entity_brand,
        "entity_feature": entity_feature,
        "entity_pain": entity_pain,
        "personal_penalty": _normalise(personal_hits, 3.0),
        "subreddit_signal": SUBREDDIT_SIGNALS.get(subreddit, 0.0),
        "subreddit_penalty": SUBREDDIT_PENALTIES.get(subreddit, 0.0),
    }


def compute_opportunity_score(features: dict[str, float]) -> float:
    """Combine engineered features into a heuristic score."""

    engagement = 0.5 * features["popularity"] + 0.5 * features["comment_engagement"]

    raw_score = (
        0.35 * features["intent_score"]
        + 0.25 * features["pain_score"]
        + 0.15 * features["action_score"]
        + 0.05 * features["buying_score"]
        + 0.04 * features["question_flag"]
        + 0.13 * engagement
        + 0.06 * features["entity_brand"]
        + 0.05 * features["entity_feature"]
        + 0.04 * features["entity_pain"]
        + features["subreddit_signal"]
        - 0.08 * features["personal_penalty"]
        - features["subreddit_penalty"]
    )

    return float(max(0.0, min(raw_score, 1.0)))


def calculate_precision_at_k(
    predictions: List[Tuple[str, float]], ground_truth: Dict[str, str], k: int
) -> float:
    """
    è®¡ç®— Precision@K
    
    Args:
        predictions: [(post_id, score), ...] æŒ‰ score é™åºæ’åˆ—
        ground_truth: {post_id: label, ...}
        k: Top-K
    
    Returns:
        Precision@K å€¼
    """
    if k <= 0 or len(predictions) == 0:
        return 0.0
    
    top_k = predictions[:k]
    relevant_count = sum(
        1 for post_id, _ in top_k if ground_truth.get(post_id) == "opportunity"
    )
    
    return relevant_count / k


def calculate_recall_at_k(
    predictions: List[Tuple[str, float]], ground_truth: Dict[str, str], k: int
) -> float:
    """
    è®¡ç®— Recall@K
    
    Args:
        predictions: [(post_id, score), ...] æŒ‰ score é™åºæ’åˆ—
        ground_truth: {post_id: label, ...}
        k: Top-K
    
    Returns:
        Recall@K å€¼
    """
    total_relevant = sum(1 for label in ground_truth.values() if label == "opportunity")
    
    if total_relevant == 0 or k <= 0 or len(predictions) == 0:
        return 0.0
    
    top_k = predictions[:k]
    relevant_count = sum(
        1 for post_id, _ in top_k if ground_truth.get(post_id) == "opportunity"
    )
    
    return relevant_count / total_relevant


def calculate_f1_at_k(
    predictions: List[Tuple[str, float]], ground_truth: Dict[str, str], k: int
) -> float:
    """
    è®¡ç®— F1@K
    
    Args:
        predictions: [(post_id, score), ...] æŒ‰ score é™åºæ’åˆ—
        ground_truth: {post_id: label, ...}
        k: Top-K
    
    Returns:
        F1@K å€¼
    """
    precision = calculate_precision_at_k(predictions, ground_truth, k)
    recall = calculate_recall_at_k(predictions, ground_truth, k)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * (precision * recall) / (precision + recall)


async def load_post_scores(post_ids: List[str]) -> tuple[Dict[str, float], Dict[str, Dict[str, float]]]:
    """ä»æ•°æ®åº“åŠ è½½å¸–å­å¹¶è¿”å›å¯å‘å¼åˆ†æ•°ä¸ç‰¹å¾ã€‚"""
    async with get_session_context() as session:
        stmt = select(PostRaw).where(
            PostRaw.source_post_id.in_(post_ids), PostRaw.is_current.is_(True)
        )
        result = await session.execute(stmt)
        posts = result.scalars().all()

        scores: Dict[str, float] = {}
        feature_map: Dict[str, Dict[str, float]] = {}
        for post in posts:
            features = extract_features(post)
            feature_map[post.source_post_id] = features
            scores[post.source_post_id] = compute_opportunity_score(features)

        return scores, feature_map


def grid_search_threshold(
    predictions: List[Tuple[str, float]],
    ground_truth: Dict[str, str],
    threshold_range: Tuple[float, float] = (0.0, 1.0),
    step: float = 0.05,
    k: int = 50,
) -> Tuple[float, Dict[str, float]]:
    """
    ç½‘æ ¼æœç´¢æœ€ä¼˜é˜ˆå€¼
    
    Args:
        predictions: [(post_id, score), ...] æŒ‰ score é™åºæ’åˆ—
        ground_truth: {post_id: label, ...}
        threshold_range: é˜ˆå€¼èŒƒå›´ (min, max)
        step: æ­¥é•¿
        k: Top-K
    
    Returns:
        (best_threshold, metrics)
    """
    # å½’ä¸€åŒ– scores åˆ° [0, 1]
    scores = [score for _, score in predictions]
    if len(scores) == 0:
        return 0.5, {}
    
    min_score = min(scores)
    max_score = max(scores)
    score_range = max_score - min_score
    
    if score_range == 0:
        normalized_predictions = [(post_id, 0.5) for post_id, _ in predictions]
    else:
        normalized_predictions = [
            (post_id, (score - min_score) / score_range) for post_id, score in predictions
        ]
    
    # ç½‘æ ¼æœç´¢
    best_threshold = 0.5
    best_precision = 0.0
    best_metrics = {}
    
    threshold = threshold_range[0]
    while threshold <= threshold_range[1]:
        # è¿‡æ»¤é«˜äºé˜ˆå€¼çš„å¸–å­
        filtered = [
            (post_id, score)
            for post_id, score in normalized_predictions
            if score >= threshold
        ]
        
        if len(filtered) == 0:
            threshold += step
            continue
        
        # è®¡ç®—æŒ‡æ ‡
        precision = calculate_precision_at_k(filtered, ground_truth, k)
        recall = calculate_recall_at_k(filtered, ground_truth, k)
        f1 = calculate_f1_at_k(filtered, ground_truth, k)
        
        # æ›´æ–°æœ€ä¼˜é˜ˆå€¼ï¼ˆä¼˜å…ˆ Precision@50ï¼‰
        if precision > best_precision:
            best_precision = precision
            best_threshold = threshold
            best_metrics = {
                "precision_at_50": precision,
                "recall_at_50": recall,
                "f1_at_50": f1,
                "threshold": threshold,
                "filtered_count": len(filtered),
            }
        
        threshold += step
    
    return best_threshold, best_metrics


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é˜ˆå€¼æ ¡å‡†è„šæœ¬")
    parser.add_argument(
        "--annotation-file",
        type=str,
        default="../data/annotations/real_annotated.csv",
        help="æ ‡æ³¨æ•°æ®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="../backend/config/scoring_rules.yaml",
        help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument("--k", type=int, default=50, help="Top-K")
    parser.add_argument("--step", type=float, default=0.05, help="ç½‘æ ¼æœç´¢æ­¥é•¿")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ¯ é˜ˆå€¼æ ¡å‡†è„šæœ¬")
    print("=" * 80)
    print()
    
    # 1. è¯»å–æ ‡æ³¨æ•°æ®
    print("ğŸ“‹ è¯»å–æ ‡æ³¨æ•°æ®...")
    annotation_file = Path(args.annotation_file)
    if not annotation_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {annotation_file}")
        return 1
    
    df = pd.read_csv(annotation_file)
    print(f"   âœ… è¯»å– {len(df)} æ¡æ ‡æ³¨æ•°æ®")
    print()
    
    # 2. æ„å»º ground truth
    ground_truth = dict(zip(df["post_id"], df["label"]))
    opportunity_count = sum(1 for label in ground_truth.values() if label == "opportunity")
    print(f"ğŸ“Š æ ‡æ³¨ç»Ÿè®¡:")
    print(f"   æ€»æ•°: {len(ground_truth)}")
    print(f"   Opportunity: {opportunity_count} ({opportunity_count/len(ground_truth)*100:.1f}%)")
    print()
    
    # 3. åŠ è½½å¸–å­è¯„åˆ†ä¸ç‰¹å¾
    print("ğŸ“¥ åŠ è½½å¸–å­è¯„åˆ†...")
    post_ids = df["post_id"].tolist()
    scores, feature_map = await load_post_scores(post_ids)
    print(f"   âœ… åŠ è½½ {len(scores)} æ¡è¯„åˆ†æ•°æ®")
    print()

    # 4. åˆ©ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒè½»é‡çº§é€»è¾‘å›å½’è¿›è¡Œå†æ ¡å‡†
    feature_columns = [
        "intent_score",
        "pain_score",
        "action_score",
        "buying_score",
        "question_flag",
        "popularity",
        "comment_engagement",
        "entity_brand",
        "entity_feature",
        "entity_pain",
        "personal_penalty",
        "subreddit_signal",
        "subreddit_penalty",
    ]

    feature_matrix = []
    for post_id in post_ids:
        feats = feature_map.get(post_id, {})
        feature_matrix.append([float(feats.get(name, 0.0)) for name in feature_columns])

    X = np.asarray(feature_matrix, dtype=float)
    y = np.asarray([1 if ground_truth.get(pid) == "opportunity" else 0 for pid in post_ids])

    logistic_scores: np.ndarray | None = None
    if X.size > 0 and np.any(y == 1) and np.any(y == 0):
        try:
            model = LogisticRegression(
                max_iter=1000,
                class_weight="balanced",
                solver="lbfgs",
            )
            model.fit(X, y)
            logistic_scores = model.predict_proba(X)[:, 1]
        except Exception as exc:  # pragma: no cover - best effort fallback
            print(f"âš ï¸  é€»è¾‘å›å½’è®­ç»ƒå¤±è´¥ï¼Œæ”¹ç”¨å¯å‘å¼å¾—åˆ†: {exc}")
            logistic_scores = None

    combined_scores: Dict[str, float] = {}
    for idx, post_id in enumerate(post_ids):
        heuristic = scores.get(post_id, 0.0)
        if logistic_scores is not None:
            logistic = float(logistic_scores[idx])
            combined_scores[post_id] = 0.6 * logistic + 0.4 * heuristic
        else:
            combined_scores[post_id] = heuristic

    # 5. æ„å»ºé¢„æµ‹åˆ—è¡¨ï¼ˆæŒ‰å¾—åˆ†é™åºï¼‰
    predictions = [(post_id, combined_scores.get(post_id, 0.0)) for post_id in post_ids]
    predictions.sort(key=lambda x: x[1], reverse=True)

    # 6. ç½‘æ ¼æœç´¢æœ€ä¼˜é˜ˆå€¼
    print("ğŸ” ç½‘æ ¼æœç´¢æœ€ä¼˜é˜ˆå€¼...")
    print(f"   é˜ˆå€¼èŒƒå›´: [0.0, 1.0]")
    print(f"   æ­¥é•¿: {args.step}")
    print(f"   ç›®æ ‡: Precision@{args.k} â‰¥ 0.6")
    print()

    best_threshold, best_metrics = grid_search_threshold(
        predictions, ground_truth, threshold_range=(0.0, 1.0), step=args.step, k=args.k
    )

    # 7. è¾“å‡ºç»“æœ
    print("=" * 80)
    print("âœ… æ ¡å‡†å®Œæˆ")
    print("=" * 80)
    print()
    print(f"ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}")
    print()
    print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡ (Top-{args.k}):")
    print(f"   Precision@{args.k}: {best_metrics['precision_at_50']:.3f}")
    print(f"   Recall@{args.k}:    {best_metrics['recall_at_50']:.3f}")
    print(f"   F1@{args.k}:        {best_metrics['f1_at_50']:.3f}")
    print(f"   è¿‡æ»¤åæ•°é‡:         {best_metrics['filtered_count']}")
    print()
    
    # 8. ä¿å­˜é…ç½®
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    config = {
        "threshold": float(best_threshold),
        "metrics": {
            f"precision_at_{args.k}": float(best_metrics["precision_at_50"]),
            f"recall_at_{args.k}": float(best_metrics["recall_at_50"]),
            f"f1_at_{args.k}": float(best_metrics["f1_at_50"]),
        },
        "calibration_info": {
            "annotation_file": str(annotation_file),
            "total_samples": len(ground_truth),
            "opportunity_count": opportunity_count,
            "k": args.k,
            "step": args.step,
        },
    }
    
    with open(output_path, "w") as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    
    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
    print()
    
    # 9. éªŒæ”¶æ£€æŸ¥
    if best_metrics["precision_at_50"] >= 0.6:
        print("âœ… éªŒæ”¶é€šè¿‡: Precision@50 â‰¥ 0.6")
    else:
        print(f"âŒ éªŒæ”¶å¤±è´¥: Precision@50 = {best_metrics['precision_at_50']:.3f} < 0.6")
        return 1
    
    print()
    print("=" * 80)
    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
