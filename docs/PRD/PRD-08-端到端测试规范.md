# PRD-08: ç«¯åˆ°ç«¯æµ‹è¯•è§„èŒƒ

## 1. é—®é¢˜é™ˆè¿°

### 1.1 èƒŒæ™¯
åŸºäºå¯¹PRD 1-7çš„å®¡æŸ¥ï¼Œå‘ç°äº†å…³é”®çš„ç«¯åˆ°ç«¯æµ‹è¯•é—æ¼ã€‚è™½ç„¶æ¯ä¸ªç»„ä»¶éƒ½æœ‰å•å…ƒæµ‹è¯•ï¼Œä½†ç¼ºå°‘å®Œæ•´çš„ç”¨æˆ·æ—…ç¨‹éªŒè¯å’Œç³»ç»Ÿçº§å¤±è´¥æ¢å¤æµ‹è¯•ã€‚ç‰¹åˆ«æ˜¯**"5åˆ†é’Ÿæ‰¿è¯º"åœ¨æç«¯æƒ…å†µä¸‹æ˜¯å¦èƒ½å…‘ç°**è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œå®Œå…¨æ²¡æœ‰E2Eæµ‹è¯•è¦†ç›–ã€‚

**å…³é”®é£é™©**ï¼š
- ä»»åŠ¡å¤±è´¥åçš„ç”¨æˆ·ä½“éªŒå®Œå…¨æœªæµ‹è¯•
- ç¼“å­˜å¤±æ•ˆæ—¶çš„æ€§èƒ½ä¿è¯æ— éªŒè¯
- å¤šç§Ÿæˆ·éš”ç¦»åœ¨é«˜å¹¶å‘ä¸‹å¯èƒ½å¤±æ•ˆ
- SSEé™çº§é“¾æ¡çš„çŠ¶æ€ä¸€è‡´æ€§æœªéªŒè¯
- JWTè¿‡æœŸå¯¹é•¿æ—¶é—´ä»»åŠ¡çš„å½±å“æœªçŸ¥

### 1.2 ç›®æ ‡
å»ºç«‹å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•ä½“ç³»ï¼ŒéªŒè¯ç³»ç»Ÿåœ¨å„ç§æç«¯æƒ…å†µä¸‹çš„è¡Œä¸ºï¼š
- **ç”¨æˆ·æ—…ç¨‹å®Œæ•´æ€§**ï¼šä»æ³¨å†Œåˆ°æŠ¥å‘Šçš„å…¨æµç¨‹éªŒè¯
- **æ€§èƒ½ä¿è¯å¯é æ€§**ï¼š5åˆ†é’Ÿæ‰¿è¯ºåœ¨æ•…éšœæ¡ä»¶ä¸‹çš„å…‘ç°
- **å®‰å…¨éš”ç¦»æœ‰æ•ˆæ€§**ï¼šå¤šç§Ÿæˆ·æ•°æ®é›¶æ³„éœ²ä¿è¯
- **é™çº§é“¾æ¡æ­£ç¡®æ€§**ï¼šæ¯ä¸ªé™çº§æ–¹æ¡ˆçš„å®é™…éªŒè¯
- **æ•…éšœæ¢å¤èƒ½åŠ›**ï¼šç³»ç»Ÿåœ¨ç»„ä»¶å¤±æ•ˆåçš„è‡ªæ„ˆèƒ½åŠ›

### 1.3 éç›®æ ‡
- **ä¸æµ‹è¯•**å•ä¸ªå‡½æ•°çš„é€»è¾‘æ­£ç¡®æ€§ï¼ˆå•å…ƒæµ‹è¯•è´Ÿè´£ï¼‰
- **ä¸æµ‹è¯•**UIçš„è§†è§‰æ•ˆæœï¼ˆè§†è§‰å›å½’æµ‹è¯•è´Ÿè´£ï¼‰
- **ä¸æµ‹è¯•**ç¬¬ä¸‰æ–¹æœåŠ¡çš„å¯é æ€§ï¼ˆMockå’Œåˆçº¦æµ‹è¯•è´Ÿè´£ï¼‰
- **ä¸æ¨¡æ‹Ÿ**è¶…å‡ºç³»ç»Ÿè®¾è®¡èŒƒå›´çš„æç«¯åœºæ™¯

## 2. è§£å†³æ–¹æ¡ˆ

### 2.1 æ ¸å¿ƒè®¾è®¡ï¼šå››å±‚æµ‹è¯•é‡‘å­—å¡”

åŸºäºæµ‹è¯•é‡‘å­—å¡”åŸç†ï¼Œå»ºç«‹ç«¯åˆ°ç«¯æµ‹è¯•çš„å±‚çº§ç»“æ„ï¼š

```
ç«¯åˆ°ç«¯æµ‹è¯•é‡‘å­—å¡”
â”œâ”€â”€ L4: ä¸šåŠ¡å…³é”®è·¯å¾„æµ‹è¯• (5ä¸ªæ ¸å¿ƒåœºæ™¯)
â”œâ”€â”€ L3: ç»„ä»¶é›†æˆæµ‹è¯• (20ä¸ªäº¤äº’åœºæ™¯)  
â”œâ”€â”€ L2: æ•…éšœæ³¨å…¥æµ‹è¯• (15ä¸ªå¤±è´¥åœºæ™¯)
â””â”€â”€ L1: æ€§èƒ½è¾¹ç•Œæµ‹è¯• (10ä¸ªæé™åœºæ™¯)
```

**æµ‹è¯•å“²å­¦**ï¼š
- **è¯šå®æµ‹è¯•**ï¼šæµ‹è¯•çœŸå®ç”¨æˆ·åœºæ™¯ï¼Œä¸æ˜¯ç†æƒ³åŒ–çš„happy path
- **æ•…éšœä¼˜å…ˆ**ï¼šé‡ç‚¹æµ‹è¯•ç³»ç»Ÿåœ¨æ•…éšœæ¡ä»¶ä¸‹çš„è¡Œä¸º
- **æ•°æ®é©±åŠ¨**ï¼šæ‰€æœ‰æµ‹è¯•å¿…é¡»æœ‰é‡åŒ–çš„æˆåŠŸæ ‡å‡†
- **å¯é‡ç°**ï¼šæ¯ä¸ªæµ‹è¯•éƒ½èƒ½åœ¨ä»»ä½•ç¯å¢ƒä¸­ç¨³å®šé‡ç°

### 2.2 å…³é”®æµ‹è¯•åœºæ™¯

#### åœºæ™¯1ï¼šå®Œæ•´ç”¨æˆ·æ—…ç¨‹ (Happy Path)
```python
@pytest.mark.critical
async def test_complete_user_journey():
    """ä»æ³¨å†Œåˆ°è·å¾—æŠ¥å‘Šçš„å®Œæ•´ç”¨æˆ·ä½“éªŒ"""
    # 1. ç”¨æˆ·æ³¨å†Œï¼ˆ30ç§’å†…å®Œæˆï¼‰
    user = await register_new_user("test@example.com", "password123")
    assert user.created_at is not None
    
    # 2. ç™»å½•è·å–JWTï¼ˆ1ç§’å†…å®Œæˆï¼‰  
    token = await login_user("test@example.com", "password123")
    assert jwt.decode(token)["user_id"] == user.id
    
    # 3. æäº¤åˆ†æä»»åŠ¡ï¼ˆ200mså†…è¿”å›task_idï¼‰
    start_time = time.time()
    task = await submit_analysis(
        token, 
        "ä¸€æ¬¾AIé©±åŠ¨çš„é¡¹ç›®ç®¡ç†å·¥å…·ï¼Œå¸®åŠ©å›¢é˜Ÿæé«˜åä½œæ•ˆç‡"
    )
    response_time = time.time() - start_time
    assert response_time < 0.2  # 200msæ‰¿è¯º
    assert task.status == "pending"
    
    # 4. ç­‰å¾…åˆ†æå®Œæˆï¼ˆ5åˆ†é’Ÿå†…å®Œæˆï¼‰
    start_analysis = time.time()
    final_status = await wait_for_completion(task.id, timeout=360)
    analysis_time = time.time() - start_analysis
    assert final_status == "completed"
    assert analysis_time < 300  # 5åˆ†é’Ÿæ‰¿è¯º
    
    # 5. è·å–æŠ¥å‘Šï¼ˆ2ç§’å†…åŠ è½½ï¼‰
    start_report = time.time()
    report = await get_report(token, task.id)
    report_time = time.time() - start_report
    assert report_time < 2  # 2ç§’æ‰¿è¯º
    assert "pain_points" in report.data
    assert "competitors" in report.data
    assert "opportunities" in report.data
```

#### åœºæ™¯2ï¼šç¼“å­˜å¤±æ•ˆä¸‹çš„5åˆ†é’Ÿæ‰¿è¯º
```python
@pytest.mark.critical
async def test_5_minute_guarantee_with_cache_failure():
    """Rediså®Œå…¨å®•æœºæ—¶èƒ½å¦å…‘ç°5åˆ†é’Ÿæ‰¿è¯º"""
    # 1. ç ´åç¼“å­˜ç³»ç»Ÿ
    await redis_client.flushall()  # æ¸…ç©ºç¼“å­˜
    await redis_client.connection_pool.disconnect()  # æ–­å¼€è¿æ¥
    
    # 2. æäº¤ä»»åŠ¡ï¼ˆåº”è¯¥è‡ªåŠ¨é™çº§åˆ°APIæ¨¡å¼ï¼‰
    user_token = await get_test_user_token()
    task = await submit_analysis(
        user_token,
        "ç¤¾äº¤åª’ä½“ç®¡ç†å·¥å…·ï¼Œå¸®åŠ©å°ä¼ä¸šç®¡ç†å¤šä¸ªå¹³å°"
    )
    
    # 3. éªŒè¯é™çº§æç¤º
    status = await get_task_status(user_token, task.id)
    assert "é™çº§æ¨¡å¼" in status.message or "APIç›´è¿" in status.message
    
    # 4. ç­‰å¾…å®Œæˆï¼ˆåº”è¯¥åœ¨10åˆ†é’Ÿå†…å®Œæˆï¼Œæ‰¿è®¤é™çº§æˆæœ¬ï¼‰
    start_time = time.time()
    final_status = await wait_for_completion(task.id, timeout=600)
    actual_time = time.time() - start_time
    
    # è¯šå®çš„æœŸæœ›ï¼šé™çº§æ¨¡å¼ä¸‹10åˆ†é’Ÿæ˜¯å¯æ¥å—çš„
    assert final_status == "completed"
    assert actual_time < 600  # é™çº§æ¨¡å¼10åˆ†é’Ÿæ‰¿è¯º
    
    # 5. éªŒè¯ç»“æœè´¨é‡ï¼ˆé™çº§æ¨¡å¼ä¸‹è´¨é‡å¯èƒ½ä¸‹é™ï¼Œä½†å¿…é¡»å¯ç”¨ï¼‰
    report = await get_report(user_token, task.id)
    assert len(report.data["pain_points"]) >= 3  # è‡³å°‘3ä¸ªç—›ç‚¹
    assert report.confidence_score >= 0.6  # é™çº§æ¨¡å¼æœ€ä½è´¨é‡
```

#### åœºæ™¯3ï¼šå¤šç§Ÿæˆ·å¹¶å‘éš”ç¦»
```python
@pytest.mark.critical  
async def test_concurrent_multi_tenant_isolation():
    """100ä¸ªç”¨æˆ·åŒæ—¶ä½¿ç”¨æ—¶çš„æ•°æ®éš”ç¦»"""
    # 1. åˆ›å»º100ä¸ªæµ‹è¯•ç”¨æˆ·
    users = []
    for i in range(100):
        user = await register_new_user(f"user{i}@test.com", f"pass{i}")
        users.append(user)
    
    # 2. æ‰€æœ‰ç”¨æˆ·åŒæ—¶æäº¤ä¸åŒçš„åˆ†æä»»åŠ¡
    tasks = []
    async with asyncio.TaskGroup() as group:
        for i, user in enumerate(users):
            token = await login_user(user.email, f"pass{i}")
            task = group.create_task(submit_analysis(
                token, 
                f"äº§å“{i}ï¼š{generate_unique_product_description(i)}"
            ))
            tasks.append((user.id, task))
    
    # 3. ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    completed_tasks = []
    for user_id, task in tasks:
        result = await task
        completed_tasks.append((user_id, result.id))
        await wait_for_completion(result.id)
    
    # 4. éªŒè¯æ•°æ®éš”ç¦»ï¼šæ¯ä¸ªç”¨æˆ·åªèƒ½çœ‹åˆ°è‡ªå·±çš„ä»»åŠ¡
    for user_id, task_id in completed_tasks:
        # æ­£ç¡®ç”¨æˆ·å¯ä»¥è®¿é—®
        user_token = await get_user_token(user_id)
        own_report = await get_report(user_token, task_id)
        assert own_report is not None
        
        # éšæœºå…¶ä»–ç”¨æˆ·ä¸èƒ½è®¿é—®ï¼ˆåº”è¯¥è¿”å›404ï¼‰
        other_user_id = random.choice([uid for uid, _ in completed_tasks if uid != user_id])
        other_token = await get_user_token(other_user_id)
        
        with pytest.raises(HTTPException) as exc:
            await get_report(other_token, task_id)
        assert exc.value.status_code == 404  # ä¸æ˜¯403ï¼Œé¿å…ä¿¡æ¯æ³„éœ²
```

### 2.3 æ•…éšœæ³¨å…¥æµ‹è¯•

#### ä»»åŠ¡å¤±è´¥æ¢å¤æµ‹è¯•
```python
@pytest.mark.failure_injection
async def test_task_failure_user_experience():
    """åˆ†æå¼•æ“å´©æºƒæ—¶çš„ç”¨æˆ·ä½“éªŒ"""
    # 1. æ­£å¸¸æäº¤ä»»åŠ¡
    user_token = await get_test_user_token()
    task = await submit_analysis(user_token, "æµ‹è¯•äº§å“æè¿°")
    
    # 2. ç­‰å¾…ä»»åŠ¡å¼€å§‹å¤„ç†
    await wait_for_status(task.id, "processing")
    
    # 3. æ•…æ„æ€æ­»å¤„ç†è¯¥ä»»åŠ¡çš„Worker
    worker_pid = await get_task_worker_pid(task.id)
    os.kill(worker_pid, signal.SIGKILL)
    
    # 4. éªŒè¯ç³»ç»Ÿè‡ªåŠ¨é‡è¯•ï¼ˆ3æ¬¡é‡è¯•æœºåˆ¶ï¼‰
    retry_count = 0
    while retry_count < 4:  # åŸå§‹ + 3æ¬¡é‡è¯•
        status = await get_task_status(user_token, task.id)
        if status.status == "processing":
            retry_count += 1
            await asyncio.sleep(60)  # ç­‰å¾…é‡è¯•é—´éš”
        elif status.status == "completed":
            break
        elif status.status == "failed":
            assert retry_count == 3  # ç¡®è®¤é‡è¯•äº†3æ¬¡
            break
    
    # 5. å¦‚æœæœ€ç»ˆå¤±è´¥ï¼ŒéªŒè¯ç”¨æˆ·å¾—åˆ°æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
    if status.status == "failed":
        assert "ç³»ç»Ÿç¹å¿™" in status.error_message
        assert "è¯·é‡æ–°æäº¤" in status.error_message
        assert status.error_message != "Internal Server Error"  # ä¸è¦æŠ€æœ¯æ€§é”™è¯¯
```

#### SSEé‡è¿çŠ¶æ€ä¸€è‡´æ€§
```python
@pytest.mark.failure_injection
async def test_sse_reconnection_consistency():
    """SSEæ–­å¼€é‡è¿åçš„çŠ¶æ€ä¸€è‡´æ€§"""
    # 1. å»ºç«‹SSEè¿æ¥å¹¶æäº¤ä»»åŠ¡
    user_token = await get_test_user_token()
    task = await submit_analysis(user_token, "SSEæµ‹è¯•äº§å“")
    
    sse_events = []
    async with sse_client(f"/api/analyze/stream/{task.id}") as sse:
        # 2. æ¥æ”¶å‡ ä¸ªçŠ¶æ€æ›´æ–°
        for i in range(3):
            event = await sse.receive()
            sse_events.append(event.data)
            if "processing" in event.data:
                break
    
    # 3. å¼ºåˆ¶æ–­å¼€SSEè¿æ¥ï¼ˆæ¨¡æ‹Ÿç½‘ç»œé—®é¢˜ï¼‰
    await sse.close()
    
    # 4. ç«‹å³é™çº§åˆ°è½®è¯¢æ¨¡å¼
    polling_status = await get_task_status(user_token, task.id)
    
    # 5. é‡æ–°å»ºç«‹SSEè¿æ¥
    async with sse_client(f"/api/analyze/stream/{task.id}") as sse2:
        first_event = await sse2.receive()
        reconnected_status = json.loads(first_event.data)
    
    # 6. éªŒè¯çŠ¶æ€ä¸€è‡´æ€§
    assert polling_status.status == reconnected_status["status"]
    assert polling_status.started_at == reconnected_status["started_at"]
    # è¿›åº¦åªèƒ½å‰è¿›ï¼Œä¸èƒ½åé€€
    if polling_status.progress and reconnected_status.get("progress"):
        assert reconnected_status["progress"] >= polling_status.progress
```

### 2.4 æ€§èƒ½è¾¹ç•Œæµ‹è¯•

#### æé™å¹¶å‘æµ‹è¯•
```python
@pytest.mark.performance
async def test_system_under_extreme_load():
    """ç³»ç»Ÿåœ¨æé™è´Ÿè½½ä¸‹çš„è¡¨ç°"""
    # é…ç½®ï¼š100å¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ·æäº¤1ä¸ªä»»åŠ¡
    CONCURRENT_USERS = 100
    MAX_RESPONSE_TIME = 5.0  # æœ€å¤§å¯æ¥å—å“åº”æ—¶é—´
    
    # 1. å‡†å¤‡100ä¸ªç”¨æˆ·è´¦æˆ·
    users = await create_test_users(CONCURRENT_USERS)
    
    # 2. ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨
    system_monitor = SystemResourceMonitor()
    system_monitor.start()
    
    # 3. å¹¶å‘æäº¤ä»»åŠ¡
    submit_times = []
    tasks = []
    
    async def submit_single_task(user, user_index):
        start_time = time.time()
        try:
            token = await login_user(user.email, f"pass{user_index}")
            task = await submit_analysis(token, f"é«˜å¹¶å‘æµ‹è¯•äº§å“{user_index}")
            response_time = time.time() - start_time
            submit_times.append(response_time)
            return task
        except Exception as e:
            submit_times.append(float('inf'))  # è®°å½•å¤±è´¥
            raise
    
    # 4. å¹¶å‘æ‰§è¡Œ
    async with asyncio.TaskGroup() as group:
        for i, user in enumerate(users):
            task = group.create_task(submit_single_task(user, i))
            tasks.append(task)
    
    # 5. åˆ†æç»“æœ
    successful_submits = [t for t in submit_times if t != float('inf')]
    failed_submits = len(submit_times) - len(successful_submits)
    
    # 6. æ€§èƒ½æ–­è¨€
    assert failed_submits <= CONCURRENT_USERS * 0.05  # æœ€å¤š5%å¤±è´¥ç‡
    assert statistics.median(successful_submits) <= MAX_RESPONSE_TIME
    assert statistics.percentile(successful_submits, 95) <= MAX_RESPONSE_TIME * 2
    
    # 7. ç³»ç»Ÿèµ„æºæ–­è¨€
    max_memory = system_monitor.get_max_memory_usage()
    max_cpu = system_monitor.get_max_cpu_usage() 
    assert max_memory < 2048  # æœ€å¤§2GBå†…å­˜
    assert max_cpu < 90  # æœ€å¤§90% CPU
    
    system_monitor.stop()
```

## 3. æŠ€æœ¯è§„èŒƒ

### 3.1 æµ‹è¯•ç¯å¢ƒé…ç½®

```python
# conftest.py - æµ‹è¯•ç¯å¢ƒé…ç½®
import pytest
import docker
import asyncio
from testcontainers import compose

@pytest.fixture(scope="session")
async def test_environment():
    """å®Œæ•´çš„æµ‹è¯•ç¯å¢ƒï¼ˆDocker Composeï¼‰"""
    compose_file = """
    version: '3.8'
    services:
      redis:
        image: redis:7-alpine
        ports: ["6379:6379"]
      
      postgres:
        image: postgres:15-alpine
        environment:
          POSTGRES_DB: reddit_scanner_test
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        ports: ["5432:5432"]
      
      app:
        build: .
        environment:
          DATABASE_URL: postgresql://test:test@postgres:5432/reddit_scanner_test
          REDIS_URL: redis://redis:6379
          JWT_SECRET_KEY: test-secret-key-for-testing
          CELERY_WORKER_COUNT: 2
        ports: ["8000:8000"]
        depends_on: [redis, postgres]
      
      worker:
        build: .
        command: celery -A app.celery worker --loglevel=info
        environment:
          DATABASE_URL: postgresql://test:test@postgres:5432/redis_scanner_test
          REDIS_URL: redis://redis:6379
        depends_on: [redis, postgres, app]
    """
    
    with compose.DockerCompose(".", compose_file_content=compose_file) as env:
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        await asyncio.sleep(10)
        
        # è¿è¡Œæ•°æ®åº“è¿ç§»
        env.exec_in_container("app", ["alembic", "upgrade", "head"])
        
        yield {
            "app_url": f"http://localhost:{env.get_service_port('app', 8000)}",
            "redis_url": f"redis://localhost:{env.get_service_port('redis', 6379)}",
            "db_url": f"postgresql://test:test@localhost:{env.get_service_port('postgres', 5432)}/reddit_scanner_test"
        }

@pytest.fixture
async def clean_database(test_environment):
    """æ¯ä¸ªæµ‹è¯•å‰æ¸…ç†æ•°æ®åº“"""
    async with get_test_db() as db:
        await db.execute("TRUNCATE users, task, analysis, report CASCADE")
    yield
```

### 3.2 æ•…éšœæ³¨å…¥å·¥å…·

```python
# tests/utils/fault_injection.py
class FaultInjectionManager:
    """æ•…éšœæ³¨å…¥ç®¡ç†å™¨"""
    
    async def kill_redis(self):
        """æ¨¡æ‹ŸRediså®•æœº"""
        container = docker.from_env().containers.get("redis")
        container.kill()
        return container
    
    async def simulate_network_partition(self, duration=30):
        """æ¨¡æ‹Ÿç½‘ç»œåˆ†åŒº"""
        # ä½¿ç”¨iptablesé˜»æ–­ç‰¹å®šç«¯å£
        subprocess.run([
            "sudo", "iptables", "-A", "INPUT", 
            "-p", "tcp", "--dport", "6379", "-j", "DROP"
        ])
        await asyncio.sleep(duration)
        subprocess.run([
            "sudo", "iptables", "-D", "INPUT",
            "-p", "tcp", "--dport", "6379", "-j", "DROP"
        ])
    
    async def exhaust_db_connections(self):
        """è€—å°½æ•°æ®åº“è¿æ¥æ± """
        connections = []
        try:
            # åˆ›å»ºè¶…è¿‡è¿æ¥æ± é™åˆ¶çš„è¿æ¥
            for _ in range(50):  # å‡è®¾è¿æ¥æ± æœ€å¤§20
                conn = await asyncpg.connect(TEST_DB_URL)
                connections.append(conn)
        finally:
            for conn in connections:
                await conn.close()
    
    async def fill_redis_memory(self):
        """å¡«æ»¡Rediså†…å­˜"""
        redis_client = redis.Redis.from_url(TEST_REDIS_URL)
        # å†™å…¥å¤§é‡æ•°æ®ç›´åˆ°å†…å­˜æ»¡
        for i in range(100000):
            large_value = "x" * 10000  # 10KB per key
            redis_client.set(f"memory_filler_{i}", large_value)
```

### 3.3 æµ‹è¯•æ•°æ®ç”Ÿæˆ

```python
# tests/utils/data_generators.py
class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    def generate_product_descriptions(self, count=100):
        """ç”Ÿæˆå¤šæ ·åŒ–çš„äº§å“æè¿°"""
        industries = ["AI", "SaaS", "ç”µå•†", "æ•™è‚²", "åŒ»ç–—", "é‡‘è", "æ¸¸æˆ"]
        features = ["è‡ªåŠ¨åŒ–", "æ™ºèƒ½", "åä½œ", "ç®¡ç†", "åˆ†æ", "ä¼˜åŒ–", "å®‰å…¨"]
        targets = ["ä¼ä¸š", "ä¸ªäºº", "å›¢é˜Ÿ", "å¼€å‘è€…", "å­¦ç”Ÿ", "è€å¸ˆ", "åŒ»ç”Ÿ"]
        
        descriptions = []
        for i in range(count):
            industry = random.choice(industries)
            feature = random.choice(features)  
            target = random.choice(targets)
            
            desc = f"ä¸€æ¬¾é¢å‘{target}çš„{industry}{feature}å·¥å…·ï¼Œäº§å“ç¼–å·{i}"
            descriptions.append(desc)
        
        return descriptions
    
    async def create_test_users(self, count=10):
        """åˆ›å»ºæµ‹è¯•ç”¨æˆ·"""
        users = []
        for i in range(count):
            user_data = {
                "email": f"test_user_{i}@example.com",
                "password": f"testpass{i}123"
            }
            user = await register_user(user_data)
            users.append(user)
        return users
    
    def generate_realistic_load_pattern(self):
        """ç”ŸæˆçœŸå®è´Ÿè½½æ¨¡å¼"""
        # æ¨¡æ‹Ÿä¸€å¤©ä¸­çš„ç”¨æˆ·æ´»åŠ¨æ¨¡å¼
        hourly_multipliers = [
            0.1, 0.1, 0.1, 0.1, 0.2, 0.3,  # æ·±å¤œåˆ°æ¸…æ™¨
            0.5, 0.8, 1.0, 1.2, 1.5, 1.8,  # ä¸Šåˆé«˜å³°
            1.5, 1.0, 0.8, 1.0, 1.3, 1.6,  # ä¸‹åˆ
            1.4, 1.0, 0.8, 0.6, 0.4, 0.2   # æ™šä¸Š
        ]
        
        current_hour = datetime.now().hour
        base_load = 10  # åŸºç¡€æ¯å°æ—¶ä»»åŠ¡æ•°
        return int(base_load * hourly_multipliers[current_hour])
```

## 4. éªŒæ”¶æ ‡å‡†

### 4.1 åŠŸèƒ½è¦æ±‚

**P0çº§åˆ«æµ‹è¯•ï¼ˆå¿…é¡»100%é€šè¿‡ï¼‰**ï¼š
- âœ… å®Œæ•´ç”¨æˆ·æ—…ç¨‹æµ‹è¯•ï¼šæ³¨å†Œâ†’ç™»å½•â†’åˆ†æâ†’æŠ¥å‘Š
- âœ… 5åˆ†é’Ÿæ‰¿è¯ºåœ¨æ­£å¸¸æƒ…å†µä¸‹å…‘ç°ç‡â‰¥95%  
- âœ… å¤šç§Ÿæˆ·æ•°æ®éš”ç¦»ï¼š0æ³„éœ²å®¹å¿åº¦
- âœ… ä»»åŠ¡å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼š3æ¬¡é‡è¯•æœºåˆ¶éªŒè¯
- âœ… SSEé™çº§åˆ°è½®è¯¢ï¼šçŠ¶æ€å®Œå…¨ä¸€è‡´

**P1çº§åˆ«æµ‹è¯•ï¼ˆå¿…é¡»90%é€šè¿‡ï¼‰**ï¼š
- âœ… ç¼“å­˜å¤±æ•ˆæ—¶çš„é™çº§å¤„ç†ï¼š10åˆ†é’Ÿå†…å®Œæˆ
- âœ… JWTè¿‡æœŸæ—¶çš„ä»»åŠ¡çŠ¶æ€ä¿æŒ
- âœ… é…ç½®çƒ­æ›´æ–°å¯¹è¿è¡Œä»»åŠ¡çš„å½±å“
- âœ… 100å¹¶å‘ç”¨æˆ·æµ‹è¯•ï¼š95%æˆåŠŸç‡
- âœ… ç³»ç»Ÿèµ„æºä½¿ç”¨ç›‘æ§ï¼šå†…å­˜<2GBï¼ŒCPU<90%

**P2çº§åˆ«æµ‹è¯•ï¼ˆå¿…é¡»80%é€šè¿‡ï¼‰**ï¼š
- âœ… çº§è”æ•…éšœæ¢å¤èƒ½åŠ›
- âœ… æé™è´Ÿè½½æµ‹è¯•
- âœ… é»‘è‰²æ˜ŸæœŸäº”åœºæ™¯æ¨¡æ‹Ÿ

### 4.2 æ€§èƒ½æŒ‡æ ‡

| æµ‹è¯•åœºæ™¯ | ç›®æ ‡æŒ‡æ ‡ | æµ‹é‡æ–¹æ³• | é€šè¿‡æ ‡å‡† |
|---------|---------|----------|----------|
| ä»»åŠ¡æäº¤å“åº” | <200ms | HTTPå“åº”æ—¶é—´ | 95%è¾¾æ ‡ |
| 5åˆ†é’Ÿåˆ†ææ‰¿è¯º | <300ç§’ | ä»»åŠ¡å®Œæˆæ—¶é—´ | 95%è¾¾æ ‡ |
| é™çº§æ¨¡å¼åˆ†æ | <600ç§’ | ç¼“å­˜å¤±æ•ˆä¸‹å®Œæˆæ—¶é—´ | 80%è¾¾æ ‡ |
| æŠ¥å‘ŠåŠ è½½æ—¶é—´ | <2ç§’ | å‰ç«¯åŠ è½½æµ‹é‡ | 99%è¾¾æ ‡ |
| SSEé‡è¿æ—¶é—´ | <5ç§’ | è¿æ¥é‡å»ºæ—¶é—´ | 90%è¾¾æ ‡ |
| å¹¶å‘å¤„ç†èƒ½åŠ› | 100ç”¨æˆ· | åŒæ—¶åœ¨çº¿ç”¨æˆ·æ•° | ç³»ç»Ÿç¨³å®š |

### 4.3 æµ‹è¯•è¦†ç›–è¦æ±‚

```python
# tests/coverage_requirements.py
COVERAGE_REQUIREMENTS = {
    "critical_paths": {
        "target": 100,  # å…³é”®è·¯å¾„å¿…é¡»100%è¦†ç›–
        "paths": [
            "user_registration_flow",
            "analysis_submission_flow", 
            "result_retrieval_flow",
            "multi_tenant_isolation"
        ]
    },
    
    "failure_scenarios": {
        "target": 80,   # å¤±è´¥åœºæ™¯80%è¦†ç›–
        "scenarios": [
            "task_failure_recovery",
            "cache_unavailable_fallback",
            "database_connection_lost",
            "worker_process_crash",
            "sse_connection_drop"
        ]
    },
    
    "performance_limits": {
        "target": 70,   # æ€§èƒ½è¾¹ç•Œ70%è¦†ç›–
        "limits": [
            "concurrent_user_load",
            "memory_usage_limits",
            "queue_depth_limits",
            "api_rate_limiting"
        ]
    }
}

def validate_coverage(test_results):
    """éªŒè¯æµ‹è¯•è¦†ç›–ç‡æ˜¯å¦è¾¾æ ‡"""
    for category, requirements in COVERAGE_REQUIREMENTS.items():
        actual_coverage = calculate_coverage(test_results, category)
        assert actual_coverage >= requirements["target"], \
            f"{category} coverage {actual_coverage}% below target {requirements['target']}%"
```

## 5. é£é™©ç®¡ç†

### 5.1 æµ‹è¯•ç¯å¢ƒé£é™©

**é£é™©1ï¼šæµ‹è¯•ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒå·®å¼‚**
- **å½±å“**ï¼šæµ‹è¯•é€šè¿‡ä½†ç”Ÿäº§å¤±è´¥
- **ç¼“è§£**ï¼šä½¿ç”¨Dockerä¿è¯ç¯å¢ƒä¸€è‡´æ€§ï¼Œç”Ÿäº§é…ç½®å‚æ•°åŒ–
- **é™çº§æ–¹æ¡ˆ**ï¼šåœ¨stagingç¯å¢ƒè¿›è¡Œé¢„ç”Ÿäº§éªŒè¯

**é£é™©2ï¼šæµ‹è¯•æ•°æ®æ±¡æŸ“**
- **å½±å“**ï¼šæµ‹è¯•ä¹‹é—´ç›¸äº’å½±å“ï¼Œç»“æœä¸å¯ä¿¡
- **ç¼“è§£**ï¼šæ¯ä¸ªæµ‹è¯•ä½¿ç”¨ç‹¬ç«‹æ•°æ®åº“ï¼Œæµ‹è¯•åè‡ªåŠ¨æ¸…ç†
- **é™çº§æ–¹æ¡ˆ**ï¼šæ•°æ®åº“å¿«ç…§å›æ»šæœºåˆ¶

**é£é™©3ï¼šå¤–éƒ¨æœåŠ¡ä¾èµ–**
- **å½±å“**ï¼šReddit APIé™åˆ¶å½±å“æµ‹è¯•æ‰§è¡Œ
- **ç¼“è§£**ï¼šä½¿ç”¨MockæœåŠ¡ï¼Œè®°å½•çœŸå®APIå“åº”
- **é™çº§æ–¹æ¡ˆ**ï¼šç¦»çº¿æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨é¢„å½•åˆ¶çš„æ•°æ®

### 5.2 æµ‹è¯•æ‰§è¡Œé£é™©

**é£é™©1ï¼šæµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿**
- **å½±å“**ï¼šå¼€å‘åé¦ˆå‘¨æœŸå»¶é•¿
- **ç¼“è§£**ï¼šå¹¶è¡Œæ‰§è¡Œæµ‹è¯•ï¼Œåˆ†çº§æµ‹è¯•ç­–ç•¥
- **ç›®æ ‡**ï¼šP0æµ‹è¯•<5åˆ†é’Ÿï¼Œå®Œæ•´å¥—ä»¶<30åˆ†é’Ÿ

**é£é™©2ï¼šæµ‹è¯•ç»“æœä¸ç¨³å®š**
- **å½±å“**ï¼šå‡é˜³æ€§/å‡é˜´æ€§å½±å“å¼€å‘ä¿¡å¿ƒ
- **ç¼“è§£**ï¼šé‡è¯•æœºåˆ¶ï¼Œç»Ÿè®¡å­¦éªŒè¯
- **æ ‡å‡†**ï¼šç›¸åŒæµ‹è¯•è¿ç»­æ‰§è¡Œ10æ¬¡ï¼Œè‡³å°‘9æ¬¡ç›¸åŒç»“æœ

### 5.3 é™çº§æ–¹æ¡ˆ

**å®Œå…¨é™çº§ï¼šæ‰‹åŠ¨éªŒè¯**
```bash
#!/bin/bash
# scripts/manual_verification.sh
echo "ğŸ” æ‰‹åŠ¨éªŒè¯å…³é”®åŠŸèƒ½..."

echo "1. ç”¨æˆ·æ³¨å†Œæµ‹è¯•:"
curl -X POST localhost:8000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email":"manual@test.com","password":"test123"}'

echo "2. ä»»åŠ¡æäº¤æµ‹è¯•:"
TOKEN=$(curl -s -X POST localhost:8000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"manual@test.com","password":"test123"}' \
  | jq -r '.access_token')

curl -X POST localhost:8000/api/analyze \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"product_description":"æ‰‹åŠ¨æµ‹è¯•äº§å“"}'

echo "3. å¤šç§Ÿæˆ·éš”ç¦»æµ‹è¯•:"
# åˆ›å»ºç¬¬äºŒä¸ªç”¨æˆ·ï¼Œå°è¯•è®¿é—®ç¬¬ä¸€ä¸ªç”¨æˆ·çš„æ•°æ®
```

**éƒ¨åˆ†é™çº§ï¼šæ ¸å¿ƒæµ‹è¯•ä¼˜å…ˆ**
```python
# å½“æµ‹è¯•ç¯å¢ƒä¸ç¨³å®šæ—¶ï¼Œåªè¿è¡Œæœ€å…³é”®çš„æµ‹è¯•
@pytest.mark.smoke
class SmokeTests:
    """çƒŸé›¾æµ‹è¯• - ç³»ç»ŸåŸºæœ¬å¯ç”¨æ€§"""
    
    def test_system_alive(self):
        response = requests.get("http://localhost:8000/health")
        assert response.status_code == 200
    
    def test_user_can_register(self):
        # æœ€åŸºæœ¬çš„ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½
        pass
        
    def test_analysis_can_submit(self):
        # æœ€åŸºæœ¬çš„ä»»åŠ¡æäº¤åŠŸèƒ½
        pass
```

---

## æ€»ç»“

è¿™ä¸ªç«¯åˆ°ç«¯æµ‹è¯•è§„èŒƒ**ç›´é¢äº†PRDä½“ç³»çš„æµ‹è¯•ç›²ç‚¹**ï¼š

1. **è¯šå®éªŒè¯**ï¼šä¸å†å‡è®¾"ç†è®ºä¸Šåº”è¯¥å·¥ä½œ"ï¼Œè€Œæ˜¯ç”¨çœŸå®åœºæ™¯éªŒè¯
2. **æ•…éšœä¼˜å…ˆ**ï¼šé‡ç‚¹æµ‹è¯•ç³»ç»Ÿåœ¨æ•…éšœæ¡ä»¶ä¸‹çš„è¡Œä¸ºï¼Œè€Œä¸åªæ˜¯happy path
3. **é‡åŒ–æ ‡å‡†**ï¼šæ¯ä¸ªæ‰¿è¯ºéƒ½æœ‰å¯æµ‹é‡çš„æˆåŠŸæ ‡å‡†
4. **åˆ†çº§ç­–ç•¥**ï¼šP0/P1/P2åˆ†çº§ï¼Œç¡®ä¿å…³é”®åŠŸèƒ½ä¼˜å…ˆè¦†ç›–

**æœ€å…³é”®çš„çªç ´**ï¼šæˆ‘ä»¬ä¸å†å›é¿"5åˆ†é’Ÿæ‰¿è¯ºåœ¨Rediså®•æœºæ—¶èƒ½å¦å…‘ç°"è¿™æ ·çš„å°–é”é—®é¢˜ï¼Œè€Œæ˜¯ç”¨E2Eæµ‹è¯•ç»™å‡ºæ˜ç¡®ç­”æ¡ˆã€‚

æ­£å¦‚Linusæ‰€è¯´ï¼š"Theory and practice sometimes clash. Theory loses." 

è¿™å¥—æµ‹è¯•è§„èŒƒç¡®ä¿æˆ‘ä»¬çš„ç³»ç»Ÿæ‰¿è¯ºä¸æ˜¯çº¸ä¸Šè°ˆå…µï¼Œè€Œæ˜¯ç»è¿‡çœŸå®éªŒè¯çš„å¯é ä¿è¯ã€‚