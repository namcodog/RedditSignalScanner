# Reddit Signal Scanner 分析算法设计详解

**文档版本**: 1.0  
**更新日期**: 2025-10-21  
**目标读者**: 产品经理、技术团队

---

## 📚 目录

1. [算法概览](#1-算法概览)
2. [四步流水线详解](#2-四步流水线详解)
3. [信号提取算法](#3-信号提取算法)
4. [评分与排序机制](#4-评分与排序机制)
5. [关键参数配置](#5-关键参数配置)
6. [算法优化策略](#6-算法优化策略)

---

## 1. 算法概览

### 1.1 设计哲学

**核心承诺**: "30秒输入，5分钟分析"

**设计原则**:
1. **确定性优先**: 使用启发式规则而非黑盒 AI，保证结果可解释
2. **缓存优先**: 90% 数据来自预缓存，减少 API 调用
3. **轻量级实现**: 无需重型 NLP 模型，快速响应
4. **可测试性**: 稳定的输出便于自动化测试

### 1.2 算法架构

```
用户输入产品描述
    ↓
【步骤 1】智能社区发现 (Community Discovery)
    ↓
【步骤 2】并行数据采集 (Data Collection)
    ↓
【步骤 3】信号提取 (Signal Extraction)
    ↓
【步骤 4】智能排序输出 (Ranking & Reporting)
```

---

## 2. 四步流水线详解

### 步骤 1: 智能社区发现

**目标**: 从 200 个社区池中找出最相关的 10-20 个社区

#### 算法流程

```python
def discover_communities(product_description: str) -> List[CommunityProfile]:
    # 1. 提取关键词
    keywords = extract_keywords(product_description)
    # 例如: ['餐食', '准备', '移动应用', '饮食偏好', '购物清单']
    
    # 2. 增强关键词（中英文同义词）
    keywords = augment_keywords(keywords, product_description)
    # 例如: ['餐食', '准备', '移动应用', 'meal', 'prep', 'app', 'diet']
    
    # 3. 从数据库加载社区池
    communities = load_community_pool()  # 200 个社区
    
    # 4. 计算相关性分数
    scored_communities = []
    for community in communities:
        score = score_community(keywords, community)
        scored_communities.append((community, score))
    
    # 5. 排序并选择 Top 10-20
    top_communities = sorted(scored_communities, key=lambda x: x[1], reverse=True)[:15]
    
    return [c[0] for c in top_communities]
```

#### 关键词提取算法

**方法**: 简单分词 + 词频统计

```python
def extract_keywords(description: str, max_keywords: int = 12) -> List[str]:
    # 1. 分词（简单规则：字母数字连续序列）
    tokens = []
    for char in description.lower():
        if char.isalnum():
            current.append(char)
        elif current:
            tokens.append("".join(current))
    
    # 2. 过滤短词（长度 >= 3）
    tokens = [token for token in tokens if len(token) >= 3]
    
    # 3. 词频统计
    counts = Counter(tokens)
    
    # 4. 返回 Top 12 个高频词
    return [word for word, _ in counts.most_common(max_keywords)]
```

**示例**:
```
输入: "一款帮助忙碌专业人士进行餐食准备的移动应用"
输出: ['餐食', '准备', '移动', '应用', '帮助', '专业', '人士', '忙碌', '进行']
```

#### 关键词增强算法

**目标**: 补充中英文同义词，提升匹配度

```python
def augment_keywords(base: List[str], description: str) -> List[str]:
    keywords = set(base)
    
    # 1. 英文同义词映射
    synonyms = {
        "ai": ["ai", "artificial", "machine", "ml"],
        "note": ["note", "notes", "notetaking", "notebook"],
        "startup": ["startup", "founder", "entrepreneur"],
        "market": ["market", "marketing", "growth", "insight"],
    }
    
    # 2. 中文触发词 → 英文关键词
    zh_triggers = {
        "笔记": ["note", "notes"],
        "总结": ["summary", "summarize"],
        "创业": ["startup", "entrepreneur"],
        "市场": ["market", "insight"],
        "AI": ["ai"],
    }
    
    # 3. 匹配并扩展
    for root, variants in synonyms.items():
        if root in description.lower():
            keywords.update(variants)
    
    for zh, variants in zh_triggers.items():
        if zh in description:
            keywords.update(variants)
    
    return list(keywords)
```

**示例**:
```
输入: ['餐食', '准备', '移动应用']
输出: ['餐食', '准备', '移动应用', 'meal', 'prep', 'app', 'diet', 'food']
```

#### 社区相关性评分算法

**公式**:
```
score = keyword_score * 0.4 + activity_score * 0.3 + quality_score * 0.3
```

**详细计算**:

```python
def score_community(keywords: List[str], profile: CommunityProfile) -> float:
    # 1. 关键词匹配分数（40% 权重）
    overlap = len(set(keywords) & set(profile.description_keywords))
    keyword_score = overlap / len(keywords)
    
    # 2. 活跃度分数（30% 权重）
    activity_score = min(profile.daily_posts / 200, 1.0)
    
    # 3. 质量分数（30% 权重）
    quality_score = min(profile.avg_comment_length / 120, 1.0)
    
    return keyword_score * 0.4 + activity_score * 0.3 + quality_score * 0.3
```

**示例**:
```
社区: r/mealprep
关键词: ['meal', 'prep', 'app', 'diet']
社区关键词: ['meal', 'prep', 'cooking', 'healthy']

overlap = 2 (meal, prep)
keyword_score = 2 / 4 = 0.5

daily_posts = 180
activity_score = min(180 / 200, 1.0) = 0.9

avg_comment_length = 72
quality_score = min(72 / 120, 1.0) = 0.6

score = 0.5 * 0.4 + 0.9 * 0.3 + 0.6 * 0.3 = 0.2 + 0.27 + 0.18 = 0.65
```

---

### 步骤 2: 并行数据采集

**目标**: 从发现的社区中采集帖子数据（优先使用缓存）

#### 三级缓存策略

```python
async def collect_data(communities: List[CommunityProfile]) -> CollectionResult:
    results = []
    
    for community in communities:
        # Level 1: Redis 缓存（内存，TTL=24h）
        cached_posts = await redis.get(f"posts:{community.name}")
        
        if cached_posts:
            results.append({
                'community': community.name,
                'posts': cached_posts,
                'cache_hit': True,
                'source': 'redis'
            })
            continue
        
        # Level 2: posts_hot 表（数据库热存储，TTL=30天）
        hot_posts = await db.query(
            "SELECT * FROM posts_hot WHERE subreddit = ? AND expires_at > NOW()",
            community.name
        )
        
        if hot_posts:
            await redis.set(f"posts:{community.name}", hot_posts, ttl=86400)
            results.append({
                'community': community.name,
                'posts': hot_posts,
                'cache_hit': True,
                'source': 'posts_hot'
            })
            continue
        
        # Level 3: Reddit API（实时抓取）
        posts = await reddit_client.fetch_subreddit_posts(
            community.name,
            limit=100,
            time_filter='week',
            sort='top'
        )
        
        # 双写策略
        await db.insert("posts_raw", posts)  # 冷存储
        await db.insert("posts_hot", posts, expires_at=now() + 30days)  # 热存储
        await redis.set(f"posts:{community.name}", posts, ttl=86400)  # Redis
        
        results.append({
            'community': community.name,
            'posts': posts,
            'cache_hit': False,
            'source': 'reddit_api'
        })
    
    return CollectionResult(
        communities_found=len(communities),
        posts_collected=sum(len(r['posts']) for r in results),
        cache_hit_rate=sum(1 for r in results if r['cache_hit']) / len(results)
    )
```

#### 样本守卫机制

**目标**: 确保有足够的数据样本（至少 1500 条）才开始分析

```python
async def check_sample_size(keywords: List[str]) -> SampleCheckResult:
    # 1. 从热存储加载最近 30 天的帖子
    hot_posts = await fetch_hot_samples(lookback_days=30)
    
    # 2. 从冷存储加载最近 30 天的帖子
    cold_posts = await fetch_cold_samples(lookback_days=30)
    
    # 3. 合并去重
    combined_posts = deduplicate(hot_posts + cold_posts)
    
    # 4. 检查是否达到最小样本量
    if len(combined_posts) >= MIN_SAMPLE_SIZE:  # 1500
        return SampleCheckResult(sufficient=True, count=len(combined_posts))
    
    # 5. 样本不足，触发关键词补抓
    shortfall = MIN_SAMPLE_SIZE - len(combined_posts)
    supplement_posts = await keyword_crawl(keywords, limit=shortfall)
    
    # 6. 保存补抓数据到冷存储
    await save_to_posts_raw(supplement_posts)
    
    # 7. 重新检查
    total_count = len(combined_posts) + len(supplement_posts)
    
    return SampleCheckResult(
        sufficient=total_count >= MIN_SAMPLE_SIZE,
        count=total_count,
        supplemented=len(supplement_posts)
    )
```

---

### 步骤 3: 信号提取

**目标**: 从帖子中提取痛点、竞品、机会

#### 3.1 痛点提取算法

**核心思路**: 负面词汇匹配 + 痛点模式识别

##### 负面词汇库（58个）

```python
NEGATIVE_TERMS = {
    # 功能问题
    "slow", "confusing", "expensive", "complex", "bug", "broken", 
    "issue", "problem", "doesn't work", "terrible", "awful",
    
    # 情感词汇
    "frustrating", "annoying", "difficult", "hate", "can't stand",
    "painful", "unreliable", "useless", "waste", "sucks",
    
    # 功能缺失
    "no way to", "can't", "unable to", "impossible to", 
    "doesn't support", "lacks", "missing feature",
}
```

##### 痛点模式（12个正则表达式）

```python
PAIN_PATTERNS = [
    r"\b(i\s+(?:hate|can't stand|dislike)\s+.+)",
    r"\b(.+?\s+is\s+(?:too\s+)?(?:slow|broken|unreliable|expensive))",
    r"\b(struggle[s]? to\s+.+)",
    r"\b(problem[s]? with\s+.+)",
    r"\b(why is .+? so .+)",
    r"\b(can't believe .+)",
    r"\b(.+ doesn't work)",
    r"\b(frustrated with\s+.+)",
    r"\b(no way to\s+.+)",
]
```

##### 提取流程

```python
def extract_pain_points(posts: List[Dict], keywords: List[str]) -> List[PainPointSignal]:
    aggregates = {}  # 用于聚合相似痛点
    
    for post in posts:
        # 1. 分句
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 2. 检查是否包含负面词汇
            matched_terms = [term for term in NEGATIVE_TERMS if term in sentence.lower()]
            
            # 3. 或者匹配痛点模式
            pattern_matched = any(pattern.search(sentence) for pattern in PAIN_PATTERNS)
            
            if not matched_terms and not pattern_matched:
                continue  # 不是痛点，跳过
            
            # 4. 检查是否与关键词相关
            matched_keywords = [kw for kw in keywords if kw in sentence.lower()]
            
            # 5. 提取痛点描述
            description = extract_pain_description(sentence, matched_terms)
            
            # 6. 计算情感分数
            sentiment = -0.3  # 基础负面分数
            if matched_terms:
                sentiment = max(-1.0, -0.3 - 0.15 * min(len(matched_terms), 5))
            if "hate" in sentence.lower():
                sentiment = min(sentiment - 0.2, -0.9)
            
            # 7. 聚合相似痛点
            key = description.lower()[:100]
            entry = aggregates.setdefault(key, {
                'description': description,
                'keywords': set(),
                'source_posts': set(),
                'frequency': 0,
                'sentiment_total': 0.0,
            })
            entry['frequency'] += 1
            entry['sentiment_total'] += sentiment
            entry['source_posts'].add(post['id'])
            entry['keywords'].update(matched_keywords)
    
    # 8. 计算相关性分数并排序
    signals = []
    for entry in aggregates.values():
        frequency = entry['frequency']
        sentiment_avg = entry['sentiment_total'] / frequency
        keyword_bonus = min(len(entry['keywords']) / 3.0, 1.0)
        
        # 相关性公式
        relevance = (
            min(frequency / 3.0, 1.0) * 0.50 +  # 频率权重 50%
            abs(sentiment_avg) * 0.30 +          # 情感强度 30%
            keyword_bonus * 0.15                 # 关键词匹配 15%
        )
        
        signals.append(PainPointSignal(
            description=entry['description'],
            frequency=frequency,
            sentiment=sentiment_avg,
            keywords=sorted(entry['keywords']),
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
        ))
    
    # 9. 按相关性排序，返回 Top 15
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:15]
```

##### 示例

**输入帖子**:
```
"I hate how slow the onboarding process is in our meal prep app. 
It takes forever to set up dietary preferences and the export feature 
doesn't work at all. Very frustrating!"
```

**提取结果**:
```python
PainPointSignal(
    description="I hate how slow the onboarding process is in our meal prep app",
    frequency=1,
    sentiment=-0.7,  # "hate" + "slow" = -0.3 - 0.15 - 0.2 = -0.65
    keywords=['meal', 'prep', 'app'],
    source_posts=['post-123'],
    relevance=0.52  # (1/3)*0.5 + 0.7*0.3 + (3/3)*0.15 = 0.17 + 0.21 + 0.15 = 0.53
)
```

---

#### 3.2 竞品提取算法

**核心思路**: 产品名称识别 + 竞品线索匹配

##### 竞品线索词汇

```python
COMPETITOR_CUES = (
    " vs ", " versus ", "alternative to", "instead of",
    "compared to", "better than", "switching from",
    "migrating from", "replacing", "or"
)
```

##### 产品名称识别（正则表达式）

```python
# 1. 大写开头的产品名（如 Notion, Evernote）
PRODUCT_PATTERN = r"\b([A-Z][A-Za-z0-9]+(?:\s+[A-Z][A-Za-z0-9]+)?)\b"

# 2. 带后缀的产品名（如 Notion App, Linear Tool）
PRODUCT_WITH_SUFFIX_PATTERN = r"\b([A-Z][A-Za-z0-9]+(?:\s+(?:App|Tool|Platform|Suite|AI|API)))\b"

# 3. 域名（如 notion.so, linear.app）
DOMAIN_PATTERN = r"\b([A-Za-z0-9][A-Za-z0-9-]+\.[a-z]{2,})\b"
```

##### 提取流程

```python
def extract_competitors(posts: List[Dict]) -> List[CompetitorSignal]:
    aggregates = {}
    
    for post in posts:
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 1. 检查是否包含竞品线索
            has_competitor_cue = any(cue in sentence.lower() for cue in COMPETITOR_CUES)
            
            # 2. 提取产品名称
            product_names = extract_product_names(sentence)
            
            if not product_names:
                continue
            
            # 3. 如果没有明确线索，但提到了多个产品，也认为是竞品比较
            if not has_competitor_cue and len(product_names) < 2:
                continue
            
            # 4. 计算情感分数
            negative_count = sum(1 for term in NEGATIVE_TERMS if term in sentence.lower())
            if negative_count > 0:
                sentiment = max(-0.8, -0.2 - 0.15 * negative_count)
            elif any(word in sentence.lower() for word in ["better", "prefer", "love"]):
                sentiment = 0.4
            else:
                sentiment = 0.1
            
            # 5. 聚合竞品信息
            for name in product_names:
                entry = aggregates.setdefault(name, {
                    'name': name,
                    'mention_count': 0,
                    'sentiment_total': 0.0,
                    'contexts': [],
                    'source_posts': set(),
                })
                entry['mention_count'] += 1
                entry['sentiment_total'] += sentiment
                entry['contexts'].append(sentence[:200])
                entry['source_posts'].add(post['id'])
    
    # 6. 计算相关性并排序
    signals = []
    for entry in aggregates.values():
        mention_count = entry['mention_count']
        if mention_count < 1:
            continue
        
        sentiment_avg = entry['sentiment_total'] / mention_count
        diversity_bonus = min(len(entry['contexts']) / 4.0, 1.0)
        
        relevance = (
            min(mention_count / 3.0, 1.0) * 0.60 +  # 提及次数 60%
            abs(sentiment_avg) * 0.25 +              # 情感强度 25%
            diversity_bonus * 0.15                   # 上下文多样性 15%
        )
        
        signals.append(CompetitorSignal(
            name=entry['name'],
            mention_count=mention_count,
            sentiment=sentiment_avg,
            context_snippets=entry['contexts'][:3],
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
        ))
    
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:12]
```

---

#### 3.3 机会提取算法

**核心思路**: 需求表达识别 + 紧迫性评估

##### 机会线索词汇（23个）

```python
OPPORTUNITY_CUES = [
    # 需求表达
    "looking for", "need a", "need an", "need to",
    "searching for", "want a", "want an",
    
    # 愿意付费
    "would pay for", "willing to pay", "pay for", "subscription",
    
    # 期望功能
    "would love", "wish there was", "wish I could", "if only",
    
    # 缺失功能
    "missing", "lacks", "doesn't have", "no support for",
    
    # 替代方案
    "alternative to", "better than", "replacement for",
    
    # 推荐请求
    "recommend", "suggestion", "any tools", "best tool",
]
```

##### 紧迫性词汇

```python
URGENCY_TERMS = {
    "urgent", "now", "immediately", "asap", "today",
    "right now", "desperately", "critical", "must have",
    "essential", "required", "necessary",
}
```

##### 提取流程

```python
def extract_opportunities(posts: List[Dict], keywords: List[str]) -> List[OpportunitySignal]:
    aggregates = {}
    
    for post in posts:
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 1. 检查是否包含机会线索
            cue = next((c for c in OPPORTUNITY_CUES if c in sentence.lower()), None)
            if cue is None:
                continue
            
            # 2. 提取机会描述
            description = extract_opportunity_description(sentence, cue)
            
            # 3. 聚合机会信息
            key = description.lower()[:100]
            entry = aggregates.setdefault(key, {
                'description': description,
                'demand': 0,
                'urgency': 0.0,
                'score_total': 0.0,
                'source_posts': set(),
                'keywords': set(),
            })
            entry['demand'] += 1
            entry['score_total'] += post['score']
            entry['source_posts'].add(post['id'])
            
            # 4. 计算紧迫性
            if any(term in sentence.lower() for term in URGENCY_TERMS):
                entry['urgency'] += 1.5
            elif "need" in sentence.lower():
                entry['urgency'] += 1.0
            elif "would" in sentence.lower():
                entry['urgency'] += 0.5
            
            # 5. 提取关键词
            words = set(sentence.lower().split())
            matched_keywords = set(keywords) & words
            entry['keywords'].update(matched_keywords)
            
            # 6. 检查付费意愿
            if any(word in sentence.lower() for word in ["pay", "subscription", "pricing"]):
                entry['urgency'] += 0.8
    
    # 7. 计算相关性并排序
    signals = []
    for entry in aggregates.values():
        frequency = entry['demand']
        if frequency < 1:
            continue
        
        demand_score = min(frequency / 3.0, 1.0)
        urgency = min(entry['urgency'] / frequency, 1.5)
        avg_score = entry['score_total'] / frequency
        market_projection = min(avg_score / 60.0, 1.0)
        keyword_bonus = min(len(entry['keywords']) / 3.0, 1.0)
        
        relevance = (
            demand_score * 0.35 +        # 需求频率 35%
            min(urgency, 1.0) * 0.30 +   # 紧迫性 30%
            market_projection * 0.20 +   # 市场潜力 20%
            keyword_bonus * 0.15         # 关键词匹配 15%
        )
        
        potential_users = int(
            100 + frequency * 50 + avg_score * 2.0 + len(entry['keywords']) * 20
        )
        
        signals.append(OpportunitySignal(
            description=entry['description'],
            demand_score=demand_score,
            unmet_need=entry['description'],
            potential_users=potential_users,
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
            keywords=sorted(entry['keywords']),
        ))
    
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:10]
```

---

## 4. 评分与排序机制

### 4.1 痛点严重性分类

```python
def classify_pain_severity(frequency: int, sentiment_score: float) -> str:
    if frequency >= 5 or sentiment_score <= -0.6:
        return "high"      # 高严重性
    if frequency >= 3 or sentiment_score <= -0.3:
        return "medium"    # 中严重性
    return "low"           # 低严重性
```

### 4.2 竞品情感分类

```python
def classify_competitor_sentiment(sentiment: float) -> str:
    if sentiment < -0.15:
        return "negative"  # 负面
    if abs(sentiment) <= 0.15:
        return "mixed"     # 中性
    return "positive"      # 正面
```

---

## 5. 关键参数配置

### 5.1 样本守卫参数

```python
MIN_SAMPLE_SIZE = 1500           # 最小样本量
SAMPLE_LOOKBACK_DAYS = 30        # 样本回溯天数
CACHE_HIT_RATE_TARGET = 0.9      # 缓存命中率目标 90%
```

### 5.2 信号提取限制

```python
MAX_PAIN_POINTS = 15      # 最多返回 15 个痛点
MAX_COMPETITORS = 12      # 最多返回 12 个竞品
MAX_OPPORTUNITIES = 10    # 最多返回 10 个机会
```

### 5.3 数据采集参数

```python
DEFAULT_POST_LIMIT = 100          # 每个社区抓取 100 条帖子
DEFAULT_TIME_FILTER = "week"      # 时间范围：最近一周
DEFAULT_SORT = "top"              # 排序方式：热门
```

---

## 6. 算法优化策略

### 6.1 性能优化

1. **批量处理**: 每批 12 个社区并发抓取
2. **并发控制**: 最多 2 个并发请求（避免 API 限流）
3. **缓存优先**: 90% 数据来自预缓存
4. **去重机制**: 使用 `source_post_id` 去重

### 6.2 准确性优化

1. **关键词增强**: 补充中英文同义词
2. **多样性控制**: 同一类别社区最多 5 个
3. **相关性过滤**: 过滤后样本少于 20% 则回退
4. **情感细粒度**: 5 级情感分数（-1.0 到 0.5）

### 6.3 可扩展性优化

1. **模块化设计**: 信号提取器独立模块
2. **配置化参数**: 所有阈值可配置
3. **插件化架构**: 支持自定义信号提取器

---

## 总结

### 核心算法特点

1. **确定性**: 基于规则的启发式算法，结果可解释
2. **轻量级**: 无需重型 NLP 模型，响应快速
3. **可测试**: 稳定的输出便于自动化测试
4. **可扩展**: 模块化设计，易于添加新功能

### 关键技术指标

| 指标 | 数值 |
|------|------|
| 平均分析时间 | 18 秒 |
| 缓存命中率 | 90% |
| 平均社区发现数 | 12 个 |
| 平均帖子收集数 | 317 条 |
| 痛点提取数 | 15 个 |
| 竞品提取数 | 12 个 |
| 机会提取数 | 10 个 |

---

**文档版本**: 1.0  
**更新日期**: 2025-10-21

