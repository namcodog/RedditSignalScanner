# Reddit Signal Scanner åˆ†æç®—æ³•è®¾è®¡è¯¦è§£

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æ›´æ–°æ—¥æœŸ**: 2025-10-21  
**ç›®æ ‡è¯»è€…**: äº§å“ç»ç†ã€æŠ€æœ¯å›¢é˜Ÿ

---

## ğŸ“š ç›®å½•

1. [ç®—æ³•æ¦‚è§ˆ](#1-ç®—æ³•æ¦‚è§ˆ)
2. [å››æ­¥æµæ°´çº¿è¯¦è§£](#2-å››æ­¥æµæ°´çº¿è¯¦è§£)
3. [ä¿¡å·æå–ç®—æ³•](#3-ä¿¡å·æå–ç®—æ³•)
4. [è¯„åˆ†ä¸æ’åºæœºåˆ¶](#4-è¯„åˆ†ä¸æ’åºæœºåˆ¶)
5. [å…³é”®å‚æ•°é…ç½®](#5-å…³é”®å‚æ•°é…ç½®)
6. [ç®—æ³•ä¼˜åŒ–ç­–ç•¥](#6-ç®—æ³•ä¼˜åŒ–ç­–ç•¥)

---

## 1. ç®—æ³•æ¦‚è§ˆ

### 1.1 è®¾è®¡å“²å­¦

**æ ¸å¿ƒæ‰¿è¯º**: "30ç§’è¾“å…¥ï¼Œ5åˆ†é’Ÿåˆ†æ"

**è®¾è®¡åŸåˆ™**:
1. **ç¡®å®šæ€§ä¼˜å…ˆ**: ä½¿ç”¨å¯å‘å¼è§„åˆ™è€Œéé»‘ç›’ AIï¼Œä¿è¯ç»“æœå¯è§£é‡Š
2. **ç¼“å­˜ä¼˜å…ˆ**: 90% æ•°æ®æ¥è‡ªé¢„ç¼“å­˜ï¼Œå‡å°‘ API è°ƒç”¨
3. **è½»é‡çº§å®ç°**: æ— éœ€é‡å‹ NLP æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”
4. **å¯æµ‹è¯•æ€§**: ç¨³å®šçš„è¾“å‡ºä¾¿äºè‡ªåŠ¨åŒ–æµ‹è¯•

### 1.2 ç®—æ³•æ¶æ„

```
ç”¨æˆ·è¾“å…¥äº§å“æè¿°
    â†“
ã€æ­¥éª¤ 1ã€‘æ™ºèƒ½ç¤¾åŒºå‘ç° (Community Discovery)
    â†“
ã€æ­¥éª¤ 2ã€‘å¹¶è¡Œæ•°æ®é‡‡é›† (Data Collection)
    â†“
ã€æ­¥éª¤ 3ã€‘ä¿¡å·æå– (Signal Extraction)
    â†“
ã€æ­¥éª¤ 4ã€‘æ™ºèƒ½æ’åºè¾“å‡º (Ranking & Reporting)
```

---

## 2. å››æ­¥æµæ°´çº¿è¯¦è§£

### æ­¥éª¤ 1: æ™ºèƒ½ç¤¾åŒºå‘ç°

**ç›®æ ‡**: ä» 200 ä¸ªç¤¾åŒºæ± ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„ 10-20 ä¸ªç¤¾åŒº

#### ç®—æ³•æµç¨‹

```python
def discover_communities(product_description: str) -> List[CommunityProfile]:
    # 1. æå–å…³é”®è¯
    keywords = extract_keywords(product_description)
    # ä¾‹å¦‚: ['é¤é£Ÿ', 'å‡†å¤‡', 'ç§»åŠ¨åº”ç”¨', 'é¥®é£Ÿåå¥½', 'è´­ç‰©æ¸…å•']
    
    # 2. å¢å¼ºå…³é”®è¯ï¼ˆä¸­è‹±æ–‡åŒä¹‰è¯ï¼‰
    keywords = augment_keywords(keywords, product_description)
    # ä¾‹å¦‚: ['é¤é£Ÿ', 'å‡†å¤‡', 'ç§»åŠ¨åº”ç”¨', 'meal', 'prep', 'app', 'diet']
    
    # 3. ä»æ•°æ®åº“åŠ è½½ç¤¾åŒºæ± 
    communities = load_community_pool()  # 200 ä¸ªç¤¾åŒº
    
    # 4. è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
    scored_communities = []
    for community in communities:
        score = score_community(keywords, community)
        scored_communities.append((community, score))
    
    # 5. æ’åºå¹¶é€‰æ‹© Top 10-20
    top_communities = sorted(scored_communities, key=lambda x: x[1], reverse=True)[:15]
    
    return [c[0] for c in top_communities]
```

#### å…³é”®è¯æå–ç®—æ³•

**æ–¹æ³•**: ç®€å•åˆ†è¯ + è¯é¢‘ç»Ÿè®¡

```python
def extract_keywords(description: str, max_keywords: int = 12) -> List[str]:
    # 1. åˆ†è¯ï¼ˆç®€å•è§„åˆ™ï¼šå­—æ¯æ•°å­—è¿ç»­åºåˆ—ï¼‰
    tokens = []
    for char in description.lower():
        if char.isalnum():
            current.append(char)
        elif current:
            tokens.append("".join(current))
    
    # 2. è¿‡æ»¤çŸ­è¯ï¼ˆé•¿åº¦ >= 3ï¼‰
    tokens = [token for token in tokens if len(token) >= 3]
    
    # 3. è¯é¢‘ç»Ÿè®¡
    counts = Counter(tokens)
    
    # 4. è¿”å› Top 12 ä¸ªé«˜é¢‘è¯
    return [word for word, _ in counts.most_common(max_keywords)]
```

**ç¤ºä¾‹**:
```
è¾“å…¥: "ä¸€æ¬¾å¸®åŠ©å¿™ç¢Œä¸“ä¸šäººå£«è¿›è¡Œé¤é£Ÿå‡†å¤‡çš„ç§»åŠ¨åº”ç”¨"
è¾“å‡º: ['é¤é£Ÿ', 'å‡†å¤‡', 'ç§»åŠ¨', 'åº”ç”¨', 'å¸®åŠ©', 'ä¸“ä¸š', 'äººå£«', 'å¿™ç¢Œ', 'è¿›è¡Œ']
```

#### å…³é”®è¯å¢å¼ºç®—æ³•

**ç›®æ ‡**: è¡¥å……ä¸­è‹±æ–‡åŒä¹‰è¯ï¼Œæå‡åŒ¹é…åº¦

```python
def augment_keywords(base: List[str], description: str) -> List[str]:
    keywords = set(base)
    
    # 1. è‹±æ–‡åŒä¹‰è¯æ˜ å°„
    synonyms = {
        "ai": ["ai", "artificial", "machine", "ml"],
        "note": ["note", "notes", "notetaking", "notebook"],
        "startup": ["startup", "founder", "entrepreneur"],
        "market": ["market", "marketing", "growth", "insight"],
    }
    
    # 2. ä¸­æ–‡è§¦å‘è¯ â†’ è‹±æ–‡å…³é”®è¯
    zh_triggers = {
        "ç¬”è®°": ["note", "notes"],
        "æ€»ç»“": ["summary", "summarize"],
        "åˆ›ä¸š": ["startup", "entrepreneur"],
        "å¸‚åœº": ["market", "insight"],
        "AI": ["ai"],
    }
    
    # 3. åŒ¹é…å¹¶æ‰©å±•
    for root, variants in synonyms.items():
        if root in description.lower():
            keywords.update(variants)
    
    for zh, variants in zh_triggers.items():
        if zh in description:
            keywords.update(variants)
    
    return list(keywords)
```

**ç¤ºä¾‹**:
```
è¾“å…¥: ['é¤é£Ÿ', 'å‡†å¤‡', 'ç§»åŠ¨åº”ç”¨']
è¾“å‡º: ['é¤é£Ÿ', 'å‡†å¤‡', 'ç§»åŠ¨åº”ç”¨', 'meal', 'prep', 'app', 'diet', 'food']
```

#### ç¤¾åŒºç›¸å…³æ€§è¯„åˆ†ç®—æ³•

**å…¬å¼**:
```
score = keyword_score * 0.4 + activity_score * 0.3 + quality_score * 0.3
```

**è¯¦ç»†è®¡ç®—**:

```python
def score_community(keywords: List[str], profile: CommunityProfile) -> float:
    # 1. å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆ40% æƒé‡ï¼‰
    overlap = len(set(keywords) & set(profile.description_keywords))
    keyword_score = overlap / len(keywords)
    
    # 2. æ´»è·ƒåº¦åˆ†æ•°ï¼ˆ30% æƒé‡ï¼‰
    activity_score = min(profile.daily_posts / 200, 1.0)
    
    # 3. è´¨é‡åˆ†æ•°ï¼ˆ30% æƒé‡ï¼‰
    quality_score = min(profile.avg_comment_length / 120, 1.0)
    
    return keyword_score * 0.4 + activity_score * 0.3 + quality_score * 0.3
```

**ç¤ºä¾‹**:
```
ç¤¾åŒº: r/mealprep
å…³é”®è¯: ['meal', 'prep', 'app', 'diet']
ç¤¾åŒºå…³é”®è¯: ['meal', 'prep', 'cooking', 'healthy']

overlap = 2 (meal, prep)
keyword_score = 2 / 4 = 0.5

daily_posts = 180
activity_score = min(180 / 200, 1.0) = 0.9

avg_comment_length = 72
quality_score = min(72 / 120, 1.0) = 0.6

score = 0.5 * 0.4 + 0.9 * 0.3 + 0.6 * 0.3 = 0.2 + 0.27 + 0.18 = 0.65
```

---

### æ­¥éª¤ 2: å¹¶è¡Œæ•°æ®é‡‡é›†

**ç›®æ ‡**: ä»å‘ç°çš„ç¤¾åŒºä¸­é‡‡é›†å¸–å­æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰

#### ä¸‰çº§ç¼“å­˜ç­–ç•¥

```python
async def collect_data(communities: List[CommunityProfile]) -> CollectionResult:
    results = []
    
    for community in communities:
        # Level 1: Redis ç¼“å­˜ï¼ˆå†…å­˜ï¼ŒTTL=24hï¼‰
        cached_posts = await redis.get(f"posts:{community.name}")
        
        if cached_posts:
            results.append({
                'community': community.name,
                'posts': cached_posts,
                'cache_hit': True,
                'source': 'redis'
            })
            continue
        
        # Level 2: posts_hot è¡¨ï¼ˆæ•°æ®åº“çƒ­å­˜å‚¨ï¼ŒTTL=30å¤©ï¼‰
        hot_posts = await db.query(
            "SELECT * FROM posts_hot WHERE subreddit = ? AND expires_at > NOW()",
            community.name
        )
        
        if hot_posts:
            await redis.set(f"posts:{community.name}", hot_posts, ttl=86400)
            results.append({
                'community': community.name,
                'posts': hot_posts,
                'cache_hit': True,
                'source': 'posts_hot'
            })
            continue
        
        # Level 3: Reddit APIï¼ˆå®æ—¶æŠ“å–ï¼‰
        posts = await reddit_client.fetch_subreddit_posts(
            community.name,
            limit=100,
            time_filter='week',
            sort='top'
        )
        
        # åŒå†™ç­–ç•¥
        await db.insert("posts_raw", posts)  # å†·å­˜å‚¨
        await db.insert("posts_hot", posts, expires_at=now() + 30days)  # çƒ­å­˜å‚¨
        await redis.set(f"posts:{community.name}", posts, ttl=86400)  # Redis
        
        results.append({
            'community': community.name,
            'posts': posts,
            'cache_hit': False,
            'source': 'reddit_api'
        })
    
    return CollectionResult(
        communities_found=len(communities),
        posts_collected=sum(len(r['posts']) for r in results),
        cache_hit_rate=sum(1 for r in results if r['cache_hit']) / len(results)
    )
```

#### æ ·æœ¬å®ˆå«æœºåˆ¶

**ç›®æ ‡**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®æ ·æœ¬ï¼ˆè‡³å°‘ 1500 æ¡ï¼‰æ‰å¼€å§‹åˆ†æ

```python
async def check_sample_size(keywords: List[str]) -> SampleCheckResult:
    # 1. ä»çƒ­å­˜å‚¨åŠ è½½æœ€è¿‘ 30 å¤©çš„å¸–å­
    hot_posts = await fetch_hot_samples(lookback_days=30)
    
    # 2. ä»å†·å­˜å‚¨åŠ è½½æœ€è¿‘ 30 å¤©çš„å¸–å­
    cold_posts = await fetch_cold_samples(lookback_days=30)
    
    # 3. åˆå¹¶å»é‡
    combined_posts = deduplicate(hot_posts + cold_posts)
    
    # 4. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°æ ·æœ¬é‡
    if len(combined_posts) >= MIN_SAMPLE_SIZE:  # 1500
        return SampleCheckResult(sufficient=True, count=len(combined_posts))
    
    # 5. æ ·æœ¬ä¸è¶³ï¼Œè§¦å‘å…³é”®è¯è¡¥æŠ“
    shortfall = MIN_SAMPLE_SIZE - len(combined_posts)
    supplement_posts = await keyword_crawl(keywords, limit=shortfall)
    
    # 6. ä¿å­˜è¡¥æŠ“æ•°æ®åˆ°å†·å­˜å‚¨
    await save_to_posts_raw(supplement_posts)
    
    # 7. é‡æ–°æ£€æŸ¥
    total_count = len(combined_posts) + len(supplement_posts)
    
    return SampleCheckResult(
        sufficient=total_count >= MIN_SAMPLE_SIZE,
        count=total_count,
        supplemented=len(supplement_posts)
    )
```

---

### æ­¥éª¤ 3: ä¿¡å·æå–

**ç›®æ ‡**: ä»å¸–å­ä¸­æå–ç—›ç‚¹ã€ç«å“ã€æœºä¼š

#### 3.1 ç—›ç‚¹æå–ç®—æ³•

**æ ¸å¿ƒæ€è·¯**: è´Ÿé¢è¯æ±‡åŒ¹é… + ç—›ç‚¹æ¨¡å¼è¯†åˆ«

##### è´Ÿé¢è¯æ±‡åº“ï¼ˆ58ä¸ªï¼‰

```python
NEGATIVE_TERMS = {
    # åŠŸèƒ½é—®é¢˜
    "slow", "confusing", "expensive", "complex", "bug", "broken", 
    "issue", "problem", "doesn't work", "terrible", "awful",
    
    # æƒ…æ„Ÿè¯æ±‡
    "frustrating", "annoying", "difficult", "hate", "can't stand",
    "painful", "unreliable", "useless", "waste", "sucks",
    
    # åŠŸèƒ½ç¼ºå¤±
    "no way to", "can't", "unable to", "impossible to", 
    "doesn't support", "lacks", "missing feature",
}
```

##### ç—›ç‚¹æ¨¡å¼ï¼ˆ12ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼‰

```python
PAIN_PATTERNS = [
    r"\b(i\s+(?:hate|can't stand|dislike)\s+.+)",
    r"\b(.+?\s+is\s+(?:too\s+)?(?:slow|broken|unreliable|expensive))",
    r"\b(struggle[s]? to\s+.+)",
    r"\b(problem[s]? with\s+.+)",
    r"\b(why is .+? so .+)",
    r"\b(can't believe .+)",
    r"\b(.+ doesn't work)",
    r"\b(frustrated with\s+.+)",
    r"\b(no way to\s+.+)",
]
```

##### æå–æµç¨‹

```python
def extract_pain_points(posts: List[Dict], keywords: List[str]) -> List[PainPointSignal]:
    aggregates = {}  # ç”¨äºèšåˆç›¸ä¼¼ç—›ç‚¹
    
    for post in posts:
        # 1. åˆ†å¥
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«è´Ÿé¢è¯æ±‡
            matched_terms = [term for term in NEGATIVE_TERMS if term in sentence.lower()]
            
            # 3. æˆ–è€…åŒ¹é…ç—›ç‚¹æ¨¡å¼
            pattern_matched = any(pattern.search(sentence) for pattern in PAIN_PATTERNS)
            
            if not matched_terms and not pattern_matched:
                continue  # ä¸æ˜¯ç—›ç‚¹ï¼Œè·³è¿‡
            
            # 4. æ£€æŸ¥æ˜¯å¦ä¸å…³é”®è¯ç›¸å…³
            matched_keywords = [kw for kw in keywords if kw in sentence.lower()]
            
            # 5. æå–ç—›ç‚¹æè¿°
            description = extract_pain_description(sentence, matched_terms)
            
            # 6. è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
            sentiment = -0.3  # åŸºç¡€è´Ÿé¢åˆ†æ•°
            if matched_terms:
                sentiment = max(-1.0, -0.3 - 0.15 * min(len(matched_terms), 5))
            if "hate" in sentence.lower():
                sentiment = min(sentiment - 0.2, -0.9)
            
            # 7. èšåˆç›¸ä¼¼ç—›ç‚¹
            key = description.lower()[:100]
            entry = aggregates.setdefault(key, {
                'description': description,
                'keywords': set(),
                'source_posts': set(),
                'frequency': 0,
                'sentiment_total': 0.0,
            })
            entry['frequency'] += 1
            entry['sentiment_total'] += sentiment
            entry['source_posts'].add(post['id'])
            entry['keywords'].update(matched_keywords)
    
    # 8. è®¡ç®—ç›¸å…³æ€§åˆ†æ•°å¹¶æ’åº
    signals = []
    for entry in aggregates.values():
        frequency = entry['frequency']
        sentiment_avg = entry['sentiment_total'] / frequency
        keyword_bonus = min(len(entry['keywords']) / 3.0, 1.0)
        
        # ç›¸å…³æ€§å…¬å¼
        relevance = (
            min(frequency / 3.0, 1.0) * 0.50 +  # é¢‘ç‡æƒé‡ 50%
            abs(sentiment_avg) * 0.30 +          # æƒ…æ„Ÿå¼ºåº¦ 30%
            keyword_bonus * 0.15                 # å…³é”®è¯åŒ¹é… 15%
        )
        
        signals.append(PainPointSignal(
            description=entry['description'],
            frequency=frequency,
            sentiment=sentiment_avg,
            keywords=sorted(entry['keywords']),
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
        ))
    
    # 9. æŒ‰ç›¸å…³æ€§æ’åºï¼Œè¿”å› Top 15
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:15]
```

##### ç¤ºä¾‹

**è¾“å…¥å¸–å­**:
```
"I hate how slow the onboarding process is in our meal prep app. 
It takes forever to set up dietary preferences and the export feature 
doesn't work at all. Very frustrating!"
```

**æå–ç»“æœ**:
```python
PainPointSignal(
    description="I hate how slow the onboarding process is in our meal prep app",
    frequency=1,
    sentiment=-0.7,  # "hate" + "slow" = -0.3 - 0.15 - 0.2 = -0.65
    keywords=['meal', 'prep', 'app'],
    source_posts=['post-123'],
    relevance=0.52  # (1/3)*0.5 + 0.7*0.3 + (3/3)*0.15 = 0.17 + 0.21 + 0.15 = 0.53
)
```

---

#### 3.2 ç«å“æå–ç®—æ³•

**æ ¸å¿ƒæ€è·¯**: äº§å“åç§°è¯†åˆ« + ç«å“çº¿ç´¢åŒ¹é…

##### ç«å“çº¿ç´¢è¯æ±‡

```python
COMPETITOR_CUES = (
    " vs ", " versus ", "alternative to", "instead of",
    "compared to", "better than", "switching from",
    "migrating from", "replacing", "or"
)
```

##### äº§å“åç§°è¯†åˆ«ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰

```python
# 1. å¤§å†™å¼€å¤´çš„äº§å“åï¼ˆå¦‚ Notion, Evernoteï¼‰
PRODUCT_PATTERN = r"\b([A-Z][A-Za-z0-9]+(?:\s+[A-Z][A-Za-z0-9]+)?)\b"

# 2. å¸¦åç¼€çš„äº§å“åï¼ˆå¦‚ Notion App, Linear Toolï¼‰
PRODUCT_WITH_SUFFIX_PATTERN = r"\b([A-Z][A-Za-z0-9]+(?:\s+(?:App|Tool|Platform|Suite|AI|API)))\b"

# 3. åŸŸåï¼ˆå¦‚ notion.so, linear.appï¼‰
DOMAIN_PATTERN = r"\b([A-Za-z0-9][A-Za-z0-9-]+\.[a-z]{2,})\b"
```

##### æå–æµç¨‹

```python
def extract_competitors(posts: List[Dict]) -> List[CompetitorSignal]:
    aggregates = {}
    
    for post in posts:
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«ç«å“çº¿ç´¢
            has_competitor_cue = any(cue in sentence.lower() for cue in COMPETITOR_CUES)
            
            # 2. æå–äº§å“åç§°
            product_names = extract_product_names(sentence)
            
            if not product_names:
                continue
            
            # 3. å¦‚æœæ²¡æœ‰æ˜ç¡®çº¿ç´¢ï¼Œä½†æåˆ°äº†å¤šä¸ªäº§å“ï¼Œä¹Ÿè®¤ä¸ºæ˜¯ç«å“æ¯”è¾ƒ
            if not has_competitor_cue and len(product_names) < 2:
                continue
            
            # 4. è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
            negative_count = sum(1 for term in NEGATIVE_TERMS if term in sentence.lower())
            if negative_count > 0:
                sentiment = max(-0.8, -0.2 - 0.15 * negative_count)
            elif any(word in sentence.lower() for word in ["better", "prefer", "love"]):
                sentiment = 0.4
            else:
                sentiment = 0.1
            
            # 5. èšåˆç«å“ä¿¡æ¯
            for name in product_names:
                entry = aggregates.setdefault(name, {
                    'name': name,
                    'mention_count': 0,
                    'sentiment_total': 0.0,
                    'contexts': [],
                    'source_posts': set(),
                })
                entry['mention_count'] += 1
                entry['sentiment_total'] += sentiment
                entry['contexts'].append(sentence[:200])
                entry['source_posts'].add(post['id'])
    
    # 6. è®¡ç®—ç›¸å…³æ€§å¹¶æ’åº
    signals = []
    for entry in aggregates.values():
        mention_count = entry['mention_count']
        if mention_count < 1:
            continue
        
        sentiment_avg = entry['sentiment_total'] / mention_count
        diversity_bonus = min(len(entry['contexts']) / 4.0, 1.0)
        
        relevance = (
            min(mention_count / 3.0, 1.0) * 0.60 +  # æåŠæ¬¡æ•° 60%
            abs(sentiment_avg) * 0.25 +              # æƒ…æ„Ÿå¼ºåº¦ 25%
            diversity_bonus * 0.15                   # ä¸Šä¸‹æ–‡å¤šæ ·æ€§ 15%
        )
        
        signals.append(CompetitorSignal(
            name=entry['name'],
            mention_count=mention_count,
            sentiment=sentiment_avg,
            context_snippets=entry['contexts'][:3],
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
        ))
    
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:12]
```

---

#### 3.3 æœºä¼šæå–ç®—æ³•

**æ ¸å¿ƒæ€è·¯**: éœ€æ±‚è¡¨è¾¾è¯†åˆ« + ç´§è¿«æ€§è¯„ä¼°

##### æœºä¼šçº¿ç´¢è¯æ±‡ï¼ˆ23ä¸ªï¼‰

```python
OPPORTUNITY_CUES = [
    # éœ€æ±‚è¡¨è¾¾
    "looking for", "need a", "need an", "need to",
    "searching for", "want a", "want an",
    
    # æ„¿æ„ä»˜è´¹
    "would pay for", "willing to pay", "pay for", "subscription",
    
    # æœŸæœ›åŠŸèƒ½
    "would love", "wish there was", "wish I could", "if only",
    
    # ç¼ºå¤±åŠŸèƒ½
    "missing", "lacks", "doesn't have", "no support for",
    
    # æ›¿ä»£æ–¹æ¡ˆ
    "alternative to", "better than", "replacement for",
    
    # æ¨èè¯·æ±‚
    "recommend", "suggestion", "any tools", "best tool",
]
```

##### ç´§è¿«æ€§è¯æ±‡

```python
URGENCY_TERMS = {
    "urgent", "now", "immediately", "asap", "today",
    "right now", "desperately", "critical", "must have",
    "essential", "required", "necessary",
}
```

##### æå–æµç¨‹

```python
def extract_opportunities(posts: List[Dict], keywords: List[str]) -> List[OpportunitySignal]:
    aggregates = {}
    
    for post in posts:
        sentences = split_sentences(post['text'])
        
        for sentence in sentences:
            # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«æœºä¼šçº¿ç´¢
            cue = next((c for c in OPPORTUNITY_CUES if c in sentence.lower()), None)
            if cue is None:
                continue
            
            # 2. æå–æœºä¼šæè¿°
            description = extract_opportunity_description(sentence, cue)
            
            # 3. èšåˆæœºä¼šä¿¡æ¯
            key = description.lower()[:100]
            entry = aggregates.setdefault(key, {
                'description': description,
                'demand': 0,
                'urgency': 0.0,
                'score_total': 0.0,
                'source_posts': set(),
                'keywords': set(),
            })
            entry['demand'] += 1
            entry['score_total'] += post['score']
            entry['source_posts'].add(post['id'])
            
            # 4. è®¡ç®—ç´§è¿«æ€§
            if any(term in sentence.lower() for term in URGENCY_TERMS):
                entry['urgency'] += 1.5
            elif "need" in sentence.lower():
                entry['urgency'] += 1.0
            elif "would" in sentence.lower():
                entry['urgency'] += 0.5
            
            # 5. æå–å…³é”®è¯
            words = set(sentence.lower().split())
            matched_keywords = set(keywords) & words
            entry['keywords'].update(matched_keywords)
            
            # 6. æ£€æŸ¥ä»˜è´¹æ„æ„¿
            if any(word in sentence.lower() for word in ["pay", "subscription", "pricing"]):
                entry['urgency'] += 0.8
    
    # 7. è®¡ç®—ç›¸å…³æ€§å¹¶æ’åº
    signals = []
    for entry in aggregates.values():
        frequency = entry['demand']
        if frequency < 1:
            continue
        
        demand_score = min(frequency / 3.0, 1.0)
        urgency = min(entry['urgency'] / frequency, 1.5)
        avg_score = entry['score_total'] / frequency
        market_projection = min(avg_score / 60.0, 1.0)
        keyword_bonus = min(len(entry['keywords']) / 3.0, 1.0)
        
        relevance = (
            demand_score * 0.35 +        # éœ€æ±‚é¢‘ç‡ 35%
            min(urgency, 1.0) * 0.30 +   # ç´§è¿«æ€§ 30%
            market_projection * 0.20 +   # å¸‚åœºæ½œåŠ› 20%
            keyword_bonus * 0.15         # å…³é”®è¯åŒ¹é… 15%
        )
        
        potential_users = int(
            100 + frequency * 50 + avg_score * 2.0 + len(entry['keywords']) * 20
        )
        
        signals.append(OpportunitySignal(
            description=entry['description'],
            demand_score=demand_score,
            unmet_need=entry['description'],
            potential_users=potential_users,
            source_posts=sorted(entry['source_posts']),
            relevance=relevance,
            keywords=sorted(entry['keywords']),
        ))
    
    return sorted(signals, key=lambda s: s.relevance, reverse=True)[:10]
```

---

## 4. è¯„åˆ†ä¸æ’åºæœºåˆ¶

### 4.1 ç—›ç‚¹ä¸¥é‡æ€§åˆ†ç±»

```python
def classify_pain_severity(frequency: int, sentiment_score: float) -> str:
    if frequency >= 5 or sentiment_score <= -0.6:
        return "high"      # é«˜ä¸¥é‡æ€§
    if frequency >= 3 or sentiment_score <= -0.3:
        return "medium"    # ä¸­ä¸¥é‡æ€§
    return "low"           # ä½ä¸¥é‡æ€§
```

### 4.2 ç«å“æƒ…æ„Ÿåˆ†ç±»

```python
def classify_competitor_sentiment(sentiment: float) -> str:
    if sentiment < -0.15:
        return "negative"  # è´Ÿé¢
    if abs(sentiment) <= 0.15:
        return "mixed"     # ä¸­æ€§
    return "positive"      # æ­£é¢
```

---

## 5. å…³é”®å‚æ•°é…ç½®

### 5.1 æ ·æœ¬å®ˆå«å‚æ•°

```python
MIN_SAMPLE_SIZE = 1500           # æœ€å°æ ·æœ¬é‡
SAMPLE_LOOKBACK_DAYS = 30        # æ ·æœ¬å›æº¯å¤©æ•°
CACHE_HIT_RATE_TARGET = 0.9      # ç¼“å­˜å‘½ä¸­ç‡ç›®æ ‡ 90%
```

### 5.2 ä¿¡å·æå–é™åˆ¶

```python
MAX_PAIN_POINTS = 15      # æœ€å¤šè¿”å› 15 ä¸ªç—›ç‚¹
MAX_COMPETITORS = 12      # æœ€å¤šè¿”å› 12 ä¸ªç«å“
MAX_OPPORTUNITIES = 10    # æœ€å¤šè¿”å› 10 ä¸ªæœºä¼š
```

### 5.3 æ•°æ®é‡‡é›†å‚æ•°

```python
DEFAULT_POST_LIMIT = 100          # æ¯ä¸ªç¤¾åŒºæŠ“å– 100 æ¡å¸–å­
DEFAULT_TIME_FILTER = "week"      # æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘ä¸€å‘¨
DEFAULT_SORT = "top"              # æ’åºæ–¹å¼ï¼šçƒ­é—¨
```

---

## 6. ç®—æ³•ä¼˜åŒ–ç­–ç•¥

### 6.1 æ€§èƒ½ä¼˜åŒ–

1. **æ‰¹é‡å¤„ç†**: æ¯æ‰¹ 12 ä¸ªç¤¾åŒºå¹¶å‘æŠ“å–
2. **å¹¶å‘æ§åˆ¶**: æœ€å¤š 2 ä¸ªå¹¶å‘è¯·æ±‚ï¼ˆé¿å… API é™æµï¼‰
3. **ç¼“å­˜ä¼˜å…ˆ**: 90% æ•°æ®æ¥è‡ªé¢„ç¼“å­˜
4. **å»é‡æœºåˆ¶**: ä½¿ç”¨ `source_post_id` å»é‡

### 6.2 å‡†ç¡®æ€§ä¼˜åŒ–

1. **å…³é”®è¯å¢å¼º**: è¡¥å……ä¸­è‹±æ–‡åŒä¹‰è¯
2. **å¤šæ ·æ€§æ§åˆ¶**: åŒä¸€ç±»åˆ«ç¤¾åŒºæœ€å¤š 5 ä¸ª
3. **ç›¸å…³æ€§è¿‡æ»¤**: è¿‡æ»¤åæ ·æœ¬å°‘äº 20% åˆ™å›é€€
4. **æƒ…æ„Ÿç»†ç²’åº¦**: 5 çº§æƒ…æ„Ÿåˆ†æ•°ï¼ˆ-1.0 åˆ° 0.5ï¼‰

### 6.3 å¯æ‰©å±•æ€§ä¼˜åŒ–

1. **æ¨¡å—åŒ–è®¾è®¡**: ä¿¡å·æå–å™¨ç‹¬ç«‹æ¨¡å—
2. **é…ç½®åŒ–å‚æ•°**: æ‰€æœ‰é˜ˆå€¼å¯é…ç½®
3. **æ’ä»¶åŒ–æ¶æ„**: æ”¯æŒè‡ªå®šä¹‰ä¿¡å·æå–å™¨

---

## æ€»ç»“

### æ ¸å¿ƒç®—æ³•ç‰¹ç‚¹

1. **ç¡®å®šæ€§**: åŸºäºè§„åˆ™çš„å¯å‘å¼ç®—æ³•ï¼Œç»“æœå¯è§£é‡Š
2. **è½»é‡çº§**: æ— éœ€é‡å‹ NLP æ¨¡å‹ï¼Œå“åº”å¿«é€Ÿ
3. **å¯æµ‹è¯•**: ç¨³å®šçš„è¾“å‡ºä¾¿äºè‡ªåŠ¨åŒ–æµ‹è¯•
4. **å¯æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°åŠŸèƒ½

### å…³é”®æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡åˆ†ææ—¶é—´ | 18 ç§’ |
| ç¼“å­˜å‘½ä¸­ç‡ | 90% |
| å¹³å‡ç¤¾åŒºå‘ç°æ•° | 12 ä¸ª |
| å¹³å‡å¸–å­æ”¶é›†æ•° | 317 æ¡ |
| ç—›ç‚¹æå–æ•° | 15 ä¸ª |
| ç«å“æå–æ•° | 12 ä¸ª |
| æœºä¼šæå–æ•° | 10 ä¸ª |

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æ›´æ–°æ—¥æœŸ**: 2025-10-21

