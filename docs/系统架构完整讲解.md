# Reddit Signal Scanner 系统架构完整讲解

**文档版本**: 1.0  
**更新日期**: 2025-10-21  
**目标读者**: 产品经理、技术团队

---

## 📚 目录

1. [系统概览](#1-系统概览)
2. [数据库设计](#2-数据库设计)
3. [用户提交需求后的完整流程](#3-用户提交需求后的完整流程)
4. [自动抓取数据系统](#4-自动抓取数据系统)
5. [数据存储规则](#5-数据存储规则)
6. [关键技术组件](#6-关键技术组件)
7. [完整数据流图](#7-完整数据流图)

---

## 1. 系统概览

### 1.1 核心承诺

**"30秒输入，5分钟分析"**

- 用户输入产品描述（30秒）
- 系统自动分析 Reddit 数据（5分钟）
- 生成完整的市场洞察报告

### 1.2 技术栈

**后端**:
- FastAPI（HTTP API）
- Celery（异步任务队列）
- Redis（消息队列 + 缓存）
- PostgreSQL（数据持久化）
- SQLAlchemy（ORM）

**前端**:
- React + TypeScript
- Vite（构建工具）
- SSE（Server-Sent Events，实时通信）

**外部服务**:
- Reddit API（数据源）

### 1.3 架构设计哲学

**四表架构** - 极简而强大：
```
Users (用户) → Tasks (任务) → Analyses (分析) → Reports (报告)
                                    ↓
                          CommunityCache (社区缓存)
```

**缓存优先架构** - 90% 数据来自预缓存：
- 后台持续爬取热门社区数据
- 用户请求时优先使用缓存
- 仅在必要时调用 Reddit API

---

## 2. 数据库设计

### 2.1 核心表结构

#### 表 1: `users` - 用户账户

**用途**: 多租户隔离，每个用户的数据完全独立

```sql
CREATE TABLE users (
    id UUID PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    membership_level VARCHAR(20) DEFAULT 'free',  -- free/pro/enterprise
    created_at TIMESTAMP WITH TIME ZONE,
    updated_at TIMESTAMP WITH TIME ZONE
);
```

**关键字段**:
- `id`: UUID，全局唯一标识
- `email`: 用户邮箱，唯一索引
- `membership_level`: 会员等级（未来扩展）

---

#### 表 2: `tasks` - 分析任务

**用途**: 存储用户提交的产品描述和任务状态

```sql
CREATE TABLE tasks (
    id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    product_description TEXT NOT NULL,  -- 10-2000 字
    status VARCHAR(20) NOT NULL,        -- pending/processing/completed/failed
    error_message TEXT,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE,
    updated_at TIMESTAMP WITH TIME ZONE
);
```

**状态流转**:
```
pending → processing → completed
                    ↘ failed (可重试)
```

**关键约束**:
- `product_description`: 10-2000 字
- `completed_at >= created_at`
- `status = 'failed'` 时必须有 `error_message`

---

#### 表 3: `analyses` - 分析结果

**用途**: 存储结构化的分析数据（JSON 格式）

```sql
CREATE TABLE analyses (
    id UUID PRIMARY KEY,
    task_id UUID UNIQUE REFERENCES tasks(id) ON DELETE CASCADE,
    insights JSONB NOT NULL,           -- 痛点、竞品、机会
    sources JSONB NOT NULL,            -- 数据来源统计
    confidence_score NUMERIC(3,2),     -- 0.00-1.00
    analysis_version INTEGER DEFAULT 1,
    created_at TIMESTAMP WITH TIME ZONE
);
```

**insights 结构**:
```json
{
  "pain_points": [
    {
      "title": "用户痛点标题",
      "severity": "high",
      "mention_count": 15,
      "example_quote": "用户原话引用"
    }
  ],
  "competitors": [...],
  "opportunities": [...]
}
```

**sources 结构**:
```json
{
  "communities_found": 10,
  "posts_collected": 317,
  "cache_hit_rate": 0.90,
  "api_calls_made": 32
}
```

---

#### 表 4: `reports` - 渲染报告

**用途**: 存储预渲染的 HTML 报告

```sql
CREATE TABLE reports (
    id UUID PRIMARY KEY,
    analysis_id UUID UNIQUE REFERENCES analyses(id) ON DELETE CASCADE,
    html_content TEXT NOT NULL,        -- 完整 HTML
    metadata JSONB,                    -- 报告元数据
    created_at TIMESTAMP WITH TIME ZONE
);
```

**为什么预渲染 HTML?**
- 加快报告加载速度
- 支持离线查看
- 便于导出 PDF

---

### 2.2 辅助表结构

#### 表 5: `community_pool` - 社区池

**用途**: 存储预先筛选的高质量 Reddit 社区

```sql
CREATE TABLE community_pool (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,  -- 如 'r/startups'
    tier VARCHAR(20) NOT NULL,          -- high/medium/low
    categories JSON NOT NULL,           -- ['business', 'tech']
    description_keywords JSON NOT NULL, -- ['startup', 'founder']
    daily_posts INTEGER DEFAULT 0,
    avg_comment_length INTEGER DEFAULT 0,
    quality_score NUMERIC(3,2) DEFAULT 0.50,
    priority VARCHAR(20) DEFAULT 'medium',
    is_active BOOLEAN DEFAULT TRUE,
    is_blacklisted BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE,
    updated_at TIMESTAMP WITH TIME ZONE
);
```

**当前状态**: 200 个活跃社区

**分层策略**:
- **High tier** (高优先级): 每小时爬取一次
- **Medium tier** (中优先级): 每 6 小时爬取一次
- **Low tier** (低优先级): 每 24 小时爬取一次

---

#### 表 6: `posts_raw` - 原始帖子存储（冷存储）

**用途**: 存储所有爬取的原始帖子数据

```sql
CREATE TABLE posts_raw (
    id SERIAL PRIMARY KEY,
    source_post_id VARCHAR(50) UNIQUE NOT NULL,  -- Reddit 帖子 ID
    subreddit VARCHAR(100) NOT NULL,
    title TEXT,
    selftext TEXT,
    author VARCHAR(100),
    score INTEGER,
    num_comments INTEGER,
    created_utc TIMESTAMP WITH TIME ZONE,
    url TEXT,
    permalink TEXT,
    raw_data JSONB,                    -- 完整原始数据
    created_at TIMESTAMP WITH TIME ZONE
);
```

**当前数据量**: 18,260 条

---

#### 表 7: `posts_hot` - 热门帖子缓存（热存储）

**用途**: 存储最近 30 天的热门帖子，用于快速分析

```sql
CREATE TABLE posts_hot (
    id SERIAL PRIMARY KEY,
    source_post_id VARCHAR(50) REFERENCES posts_raw(source_post_id),
    subreddit VARCHAR(100) NOT NULL,
    title TEXT,
    selftext TEXT,
    score INTEGER,
    num_comments INTEGER,
    created_utc TIMESTAMP WITH TIME ZONE,
    cached_at TIMESTAMP WITH TIME ZONE,
    expires_at TIMESTAMP WITH TIME ZONE,  -- 30 天后过期
    created_at TIMESTAMP WITH TIME ZONE
);
```

**当前数据量**: 17,977 条

**自动清理**: 每天凌晨删除过期数据

---

#### 表 8: `community_cache` - 社区缓存状态

**用途**: 记录每个社区的缓存元数据

```sql
CREATE TABLE community_cache (
    id SERIAL PRIMARY KEY,
    community_name VARCHAR(100) UNIQUE NOT NULL,
    posts_cached INTEGER DEFAULT 0,
    last_crawled_at TIMESTAMP WITH TIME ZONE,
    cache_expires_at TIMESTAMP WITH TIME ZONE,
    quality_score NUMERIC(3,2),
    created_at TIMESTAMP WITH TIME ZONE,
    updated_at TIMESTAMP WITH TIME ZONE
);
```

**用途**:
- 记录上次爬取时间
- 判断缓存是否过期
- 计算社区质量分数

---

## 3. 用户提交需求后的完整流程

### 3.1 流程概览

```
用户输入产品描述
    ↓
前端提交 POST /api/analyze
    ↓
后端创建 Task 记录（status: pending）
    ↓
Celery 异步任务启动（status: processing）
    ↓
【步骤 1】智能社区发现
    ↓
【步骤 2】并行数据采集
    ↓
【步骤 3】信号提取与分析
    ↓
【步骤 4】报告生成
    ↓
更新 Task 状态（status: completed）
    ↓
前端通过 SSE 实时接收进度
    ↓
用户查看完整报告
```

### 3.2 详细步骤拆解

#### 步骤 0: 用户提交（前端）

**用户操作**:
1. 在输入框填写产品描述（83 字）
2. 点击"开始 5 分钟分析"按钮

**前端代码**:
```typescript
// frontend/src/pages/InputPage.tsx
const response = await api.post('/api/analyze', {
  product_description: description
});
const taskId = response.data.task_id;
navigate(`/progress/${taskId}`);
```

**API 请求**:
```http
POST /api/analyze HTTP/1.1
Authorization: Bearer <JWT_TOKEN>
Content-Type: application/json

{
  "product_description": "一款帮助忙碌专业人士进行餐食准备的移动应用..."
}
```

---

#### 步骤 1: 创建任务（后端 API）

**后端代码**:
```python
# backend/app/api/v1/analyze.py
@router.post("/analyze")
async def create_analysis_task(
    request: TaskCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    # 1. 创建 Task 记录
    task = Task(
        id=uuid.uuid4(),
        user_id=current_user.id,
        product_description=request.product_description,
        status=TaskStatus.PENDING
    )
    db.add(task)
    await db.commit()
    
    # 2. 发送到 Celery 队列
    run_analysis_task.apply_async(
        args=[str(task.id)],
        queue='analysis_queue'
    )
    
    return {"task_id": str(task.id)}
```

**数据库变化**:
```sql
INSERT INTO tasks (id, user_id, product_description, status, created_at)
VALUES ('626e668e-...', 'user-uuid', '产品描述...', 'pending', NOW());
```

---

#### 步骤 2: Celery 任务启动

**Celery Worker 接收任务**:
```python
# backend/app/tasks/analysis_task.py
@celery_app.task(name="run_analysis_task")
def run_analysis_task(task_id: str):
    asyncio.run(_run_analysis_async(task_id))

async def _run_analysis_async(task_id: str):
    # 1. 更新状态为 processing
    await update_task_status(task_id, TaskStatus.PROCESSING)
    
    # 2. 调用分析引擎
    result = await AnalysisEngine.run(task_id)
    
    # 3. 保存结果
    await save_analysis_result(task_id, result)
    
    # 4. 更新状态为 completed
    await update_task_status(task_id, TaskStatus.COMPLETED)
```

---

#### 步骤 3: 分析引擎执行（核心逻辑）

**3.1 智能社区发现**

**目标**: 从 200 个社区池中找出最相关的 10-20 个社区

**算法**:
```python
# backend/app/services/analysis_engine.py
async def discover_communities(product_description: str) -> List[CommunityProfile]:
    # 1. 提取产品关键词
    keywords = extract_keywords(product_description)
    # 例如: ['餐食', '准备', '移动应用', '饮食偏好', '购物清单']
    
    # 2. 从社区池加载所有活跃社区
    communities = await load_community_pool()
    # 200 个社区
    
    # 3. 计算每个社区的相关性分数
    scored_communities = []
    for community in communities:
        score = calculate_relevance_score(
            keywords=keywords,
            community_categories=community.categories,
            community_keywords=community.description_keywords
        )
        scored_communities.append((community, score))
    
    # 4. 按分数排序，取前 10-20 个
    top_communities = sorted(scored_communities, key=lambda x: x[1], reverse=True)[:15]
    
    return [c[0] for c in top_communities]
```

**输出示例**:
```
发现 12 个相关社区:
- r/mealprep (分数: 0.95)
- r/EatCheapAndHealthy (分数: 0.88)
- r/cookingforbeginners (分数: 0.82)
...
```

---

**3.2 并行数据采集**

**目标**: 从发现的社区中采集帖子数据（优先使用缓存）

**流程**:
```python
async def collect_data(communities: List[CommunityProfile]) -> CollectionResult:
    results = []
    
    for community in communities:
        # 1. 检查缓存
        cached_posts = await cache_manager.get_cached_posts(community.name)
        
        if cached_posts and not is_expired(cached_posts):
            # 缓存命中
            results.append({
                'community': community.name,
                'posts': cached_posts,
                'cache_hit': True
            })
        else:
            # 缓存未命中，调用 Reddit API
            posts = await reddit_client.fetch_subreddit_posts(
                community.name,
                limit=100,
                time_filter='week',
                sort='top'
            )
            
            # 更新缓存
            await cache_manager.set_cached_posts(community.name, posts)
            
            results.append({
                'community': community.name,
                'posts': posts,
                'cache_hit': False
            })
    
    return CollectionResult(
        communities_found=len(communities),
        posts_collected=sum(len(r['posts']) for r in results),
        cache_hit_rate=calculate_cache_hit_rate(results)
    )
```

**输出示例**:
```
数据采集完成:
- 社区数量: 12
- 帖子总数: 234
- 缓存命中率: 90% (10/12 个社区使用缓存)
- API 调用次数: 2
```

---

**3.3 信号提取与分析**

**目标**: 从帖子中提取痛点、竞品、机会

**算法**:
```python
async def extract_signals(posts: List[Dict]) -> Dict[str, List]:
    pain_points = []
    competitors = []
    opportunities = []
    
    for post in posts:
        text = post['title'] + ' ' + post['selftext']
        
        # 1. 痛点检测（关键词匹配 + 情感分析）
        if contains_pain_keywords(text):
            pain_points.append({
                'title': extract_pain_title(text),
                'severity': calculate_severity(text),
                'mention_count': 1,
                'example_quote': extract_quote(text),
                'subreddit': post['subreddit']
            })
        
        # 2. 竞品检测（品牌名称识别）
        competitors_found = extract_competitor_mentions(text)
        competitors.extend(competitors_found)
        
        # 3. 机会检测（需求表达识别）
        opportunities_found = extract_opportunities(text)
        opportunities.extend(opportunities_found)
    
    # 4. 去重和聚合
    pain_points = deduplicate_and_aggregate(pain_points)
    competitors = deduplicate_and_aggregate(competitors)
    opportunities = deduplicate_and_aggregate(opportunities)
    
    return {
        'pain_points': pain_points,
        'competitors': competitors,
        'opportunities': opportunities
    }
```

**输出示例**:
```
信号提取完成:
- 痛点: 15 条
- 竞品: 8 个
- 机会: 10 个
```

---

**3.4 报告生成**

**目标**: 生成 HTML 格式的完整报告

```python
async def generate_report(insights: Dict, sources: Dict) -> str:
    html = f"""
    <html>
    <head><title>市场洞察报告</title></head>
    <body>
        <h1>市场洞察报告</h1>
        <section>
            <h2>数据概览</h2>
            <p>分析了 {sources['communities_found']} 个社区</p>
            <p>收集了 {sources['posts_collected']} 条帖子</p>
        </section>
        
        <section>
            <h2>用户痛点</h2>
            {render_pain_points(insights['pain_points'])}
        </section>
        
        <section>
            <h2>竞品分析</h2>
            {render_competitors(insights['competitors'])}
        </section>
        
        <section>
            <h2>商业机会</h2>
            {render_opportunities(insights['opportunities'])}
        </section>
    </body>
    </html>
    """
    return html
```

---

#### 步骤 4: 保存结果到数据库

```python
async def save_analysis_result(task_id: str, result: AnalysisResult):
    async with SessionFactory() as db:
        # 1. 创建 Analysis 记录
        analysis = Analysis(
            id=uuid.uuid4(),
            task_id=task_id,
            insights=result.insights,
            sources=result.sources,
            confidence_score=0.85
        )
        db.add(analysis)
        
        # 2. 创建 Report 记录
        report = Report(
            id=uuid.uuid4(),
            analysis_id=analysis.id,
            html_content=result.report_html
        )
        db.add(report)
        
        # 3. 更新 Task 状态
        task = await db.get(Task, task_id)
        task.status = TaskStatus.COMPLETED
        task.completed_at = datetime.now(timezone.utc)
        
        await db.commit()
```

---

#### 步骤 5: SSE 实时推送进度

**后端 SSE 服务**:
```python
# backend/app/core/sse.py
async def send_progress_update(task_id: str, stage: int, total_stages: int):
    event = {
        'event': 'progress',
        'task_id': task_id,
        'stage': stage,
        'total_stages': total_stages,
        'percentage': (stage / total_stages) * 100
    }
    await sse_manager.broadcast(task_id, event)
```

**前端 SSE 客户端**:
```typescript
// frontend/src/api/sse.client.ts
const eventSource = new EventSource(`/api/stream/${taskId}`);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  
  if (data.event === 'progress') {
    setProgress(data.percentage);
    setStage(data.stage);
  }
  
  if (data.event === 'completed') {
    navigate(`/report/${taskId}`);
  }
};
```

---

## 4. 自动抓取数据系统

### 4.1 系统概述

**目标**: 24/7 持续爬取社区数据，保持缓存新鲜度

**架构**:
```
Celery Beat (定时调度器)
    ↓
Celery Worker (爬虫任务)
    ↓
Reddit API (数据源)
    ↓
PostgreSQL (posts_raw, posts_hot)
    ↓
Redis (缓存)
```

### 4.2 定时任务配置

```python
# backend/app/core/celery_app.py
celery_app.conf.beat_schedule = {
    'crawl-high-priority-communities': {
        'task': 'crawl_seed_communities',
        'schedule': crontab(minute='*/60'),  # 每小时
        'args': ('high',)
    },
    'crawl-medium-priority-communities': {
        'task': 'crawl_seed_communities',
        'schedule': crontab(minute='*/360'),  # 每 6 小时
        'args': ('medium',)
    },
    'crawl-low-priority-communities': {
        'task': 'crawl_seed_communities',
        'schedule': crontab(hour='*/24'),  # 每 24 小时
        'args': ('low',)
    }
}
```

### 4.3 增量爬虫逻辑

```python
# backend/app/tasks/crawler_task.py
@celery_app.task(name="crawl_seed_communities")
def crawl_seed_communities(tier: str = 'all'):
    asyncio.run(_crawl_async(tier))

async def _crawl_async(tier: str):
    # 1. 从社区池加载待爬取社区
    communities = await load_communities_by_tier(tier)
    # 例如: tier='high' 返回 50 个高优先级社区
    
    # 2. 批量爬取（并发控制：2）
    for batch in chunked(communities, batch_size=12):
        tasks = [
            crawl_single_community(community)
            for community in batch
        ]
        await asyncio.gather(*tasks, return_exceptions=True)
    
    # 3. 记录爬取指标
    await record_crawl_metrics(tier, len(communities))
```

### 4.4 单个社区爬取流程

```python
async def crawl_single_community(community: CommunityProfile):
    # 1. 调用 Reddit API
    posts = await reddit_client.fetch_subreddit_posts(
        community.name,
        limit=100,
        time_filter='week',
        sort='top'
    )
    
    # 2. 保存到 posts_raw（冷存储）
    await save_to_posts_raw(posts)
    
    # 3. 保存到 posts_hot（热存储）
    await save_to_posts_hot(posts, ttl_days=30)
    
    # 4. 更新 Redis 缓存
    await cache_manager.set_cached_posts(community.name, posts)
    
    # 5. 更新 community_cache 元数据
    await update_community_cache(
        community.name,
        posts_cached=len(posts),
        last_crawled_at=datetime.now(timezone.utc)
    )
```

---

## 5. 数据存储规则

### 5.1 冷热分离架构

**设计理念**: 分离"全量历史数据"和"热门近期数据"

```
posts_raw (冷存储)          posts_hot (热存储)
    ↓                           ↓
所有爬取的帖子              最近 30 天的热门帖子
永久保存                    自动过期删除
18,260 条                   17,977 条
```

**为什么需要冷热分离?**
- **性能**: 分析时只查询 posts_hot，速度快 10 倍
- **成本**: 冷数据可以迁移到廉价存储
- **维护**: 热数据自动清理，无需手动管理

### 5.2 数据写入规则

**规则 1: 双写策略**
```python
async def save_posts(posts: List[RedditPost]):
    # 1. 写入 posts_raw（冷存储）
    await insert_posts_raw(posts)
    
    # 2. 写入 posts_hot（热存储）
    await insert_posts_hot(posts, expires_at=now() + timedelta(days=30))
```

**规则 2: 去重策略**
```python
# 使用 source_post_id 作为唯一键
INSERT INTO posts_raw (source_post_id, ...)
VALUES ('abc123', ...)
ON CONFLICT (source_post_id) DO NOTHING;
```

**规则 3: 过期清理**
```python
# 每天凌晨 2 点执行
DELETE FROM posts_hot
WHERE expires_at < NOW();
```

### 5.3 缓存策略

**三级缓存架构**:
```
Level 1: Redis (内存缓存)
    ↓ 未命中
Level 2: posts_hot (数据库热存储)
    ↓ 未命中
Level 3: Reddit API (实时抓取)
```

**缓存 TTL**:
- Redis: 24 小时
- posts_hot: 30 天
- community_cache: 记录上次爬取时间

---

## 6. 关键技术组件

### 6.1 Celery 任务队列

**队列分类**:
```python
CELERY_TASK_ROUTES = {
    'run_analysis_task': {'queue': 'analysis_queue'},
    'crawl_community': {'queue': 'crawler_queue'},
    'monitor_cache_health': {'queue': 'monitoring_queue'},
    'cleanup_expired_data': {'queue': 'maintenance_queue'}
}
```

**并发控制**:
- 分析任务: 4 个 worker
- 爬虫任务: 2 个 worker（避免 API 限流）
- 监控任务: 1 个 worker

### 6.2 SSE 实时通信

**为什么选择 SSE 而不是 WebSocket?**
- 单向通信（服务器 → 客户端）
- 自动重连
- 更简单的实现

**SSE 事件类型**:
```typescript
type SSEEvent = 
  | { event: 'connected', task_id: string }
  | { event: 'progress', stage: number, total_stages: number }
  | { event: 'completed', task_id: string }
  | { event: 'error', message: string }
  | { event: 'close' };
```

### 6.3 JWT 认证

**Token 结构**:
```json
{
  "sub": "user-uuid",
  "email": "user@example.com",
  "exp": 1234567890
}
```

**认证流程**:
```
1. 用户登录 → 后端验证密码 → 返回 JWT token
2. 前端存储 token 到 localStorage
3. 每次请求携带 token: Authorization: Bearer <token>
4. 后端验证 token → 提取 user_id → 查询数据
```

---

## 7. 完整数据流图

```
┌─────────────────────────────────────────────────────────────────┐
│                         用户操作层                               │
│  用户输入产品描述 → 点击"开始分析" → 查看实时进度 → 查看报告    │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                         前端层 (React)                           │
│  InputPage → POST /api/analyze → ProgressPage (SSE) → ReportPage│
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                      后端 API 层 (FastAPI)                       │
│  创建 Task → 发送到 Celery → SSE 推送进度 → 返回报告            │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Celery 任务层 (异步处理)                      │
│  run_analysis_task → 调用分析引擎 → 保存结果 → 更新状态         │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                      分析引擎 (核心逻辑)                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ 社区发现     │→ │ 数据采集     │→ │ 信号提取     │          │
│  │ (200→12社区) │  │ (缓存优先)   │  │ (痛点/竞品)  │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
│                                           ↓                      │
│                                    ┌──────────────┐             │
│                                    │ 报告生成     │             │
│                                    │ (HTML)       │             │
│                                    └──────────────┘             │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                      数据层 (PostgreSQL + Redis)                 │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐        │
│  │ users    │  │ tasks    │  │ analyses │  │ reports  │        │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘        │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                      │
│  │posts_raw │  │posts_hot │  │community │                      │
│  │(18,260)  │  │(17,977)  │  │_pool(200)│                      │
│  └──────────┘  └──────────┘  └──────────┘                      │
│                                                                  │
│  Redis: 缓存 + 消息队列                                          │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                    后台爬虫系统 (24/7 运行)                      │
│  Celery Beat → 定时触发 → 爬取社区 → 保存数据 → 更新缓存        │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │ High (1h)    │  │ Medium (6h)  │  │ Low (24h)    │          │
│  │ 50 社区      │  │ 100 社区     │  │ 50 社区      │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│                      外部服务 (Reddit API)                       │
│  GET /r/{subreddit}/top → 返回帖子列表                           │
└─────────────────────────────────────────────────────────────────┘
```

---

## 总结

### 核心设计原则

1. **极简架构**: 四表核心 + 辅助表
2. **缓存优先**: 90% 数据来自预缓存
3. **异步处理**: Celery 解耦 HTTP 和分析
4. **实时反馈**: SSE 推送进度
5. **多租户隔离**: 每个用户数据完全独立

### 关键数据指标

- **社区池**: 200 个活跃社区
- **冷存储**: 18,260 条帖子
- **热存储**: 17,977 条帖子
- **缓存命中率**: 90%
- **分析时间**: 5 分钟内完成

### 技术亮点

- **冷热分离**: 性能提升 10 倍
- **增量爬虫**: 24/7 持续更新
- **智能调度**: 分层优先级管理
- **去重机制**: 避免重复数据
- **自动清理**: 30 天过期删除

---

**文档结束**

