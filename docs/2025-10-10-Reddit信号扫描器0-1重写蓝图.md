# Reddit Signal Scanner - 0-1重写蓝图

> **创建日期**: 2025-10-10
> **目标**: 基于完整PRD文档体系，从零重建Reddit Signal Scanner
> **工期**: 15天
> **质量承诺**: 100% PRD符合度 + Linus设计哲学 + 类型安全零容忍

---

## 📋 执行摘要

### 重写原因

**当前项目问题**（基于2025-10-08深度核查报告）：
1. ❌ 数据流转字段不匹配（signals vs insights）导致报告永远显示demo数据
2. ❌ 数据收集量不足（total_posts=1）
3. ⚠️ API端点膨胀（59个 vs PRD定义的4个核心）
4. ⚠️ 配置管理混乱（3个.env + 7个YAML分散）
5. ⚠️ 代码过大（analysis_tasks.py 1329行）
6. ✅ 算法完整实现（但数据管道故障）

**重写优势**：
- ✅ 从PRD文档直接实现，避免历史债务
- ✅ 类型安全从第一天内建（100% mypy --strict）
- ✅ 架构清晰简洁（严格遵循4表/4端点/4步设计）
- ✅ 15天交付可用MVP

---

## 🎯 核心设计原则

### Linus Torvalds设计哲学

1. **数据结构优先**
   - 先设计4表架构，代码自然简单
   - Users → Tasks → Analyses → Reports（1:N:1:1）
   - 消除复杂的JOIN和特殊情况

2. **简单胜过聪明**
   - 4个API端点 vs 59个路由
   - SSE实时推送 vs 300次轮询
   - 线性流水线 vs 复杂依赖图

3. **诚实的架构**
   - "5分钟承诺"基于缓存，不是魔法
   - 明确标注数据源（90%缓存 + 10% API）
   - 完整的错误恢复策略

### 类型安全零容忍

```python
# ❌ 绝对禁止
# type: ignore
Any
Dict[str, Any]
def func(data):  # 无类型注解

# ✅ 必须遵守
from typing import Dict, List, Optional
from pydantic import BaseModel

class TaskRequest(BaseModel):
    product_description: str

def analyze(request: TaskRequest) -> Dict[str, str]:
    return {"task_id": "..."}
```

---

## 📂 目标架构（PRD驱动）

### 数据层（PRD-01）

```sql
-- 四表架构：消除特殊情况
users (id, email, password_hash, created_at)
  ↓ 1:N
tasks (id, user_id, product_description, status)
  ↓ 1:1
analyses (id, task_id, insights JSONB, sources JSONB)
  ↓ 1:1
reports (id, analysis_id, html_content)

-- 独立缓存管理
community_cache (community_name, last_crawled_at, posts_cached)
```

**关键决策**：
- 从第一天支持多租户（user_id不是预留字段）
- JSONB + Schema验证（防止数据格式错误）
- 缓存优先架构支持

### API层（PRD-02）

```
POST /api/analyze                    # 创建任务 (< 200ms)
GET /api/analyze/stream/{task_id}    # SSE实时推送（主要）
GET /api/status/{task_id}            # 轮询fallback
GET /api/report/{task_id}            # 获取报告
```

**关键决策**：
- SSE优先，轮询作为降级
- 完整错误恢复机制（检测→恢复→降级）
- JWT认证 + 多租户隔离

### 分析引擎（PRD-03）

```
Step 1: 智能社区发现 (30秒)
  ↓ TF-IDF + 语义相似度 → Top 20社区
Step 2: 并行数据采集 (120秒)
  ↓ 90%缓存 + 10%实时API
Step 3: 统一信号提取 (90秒)
  ↓ Reddit特定NLP → 痛点/竞品/机会
Step 4: 智能排序输出 (30秒)
  ↓ 多维度评分 → 结构化报告
```

**关键决策**：
- 缓存优先保证"5分钟承诺"
- Reddit专用NLP模型（非Twitter模型）
- 动态社区数量（10-30个，基于缓存命中率）

### 任务系统（PRD-04）

```
FastAPI (HTTP) → Celery (任务队列) → Redis (状态缓存)
                     ↓
            Analysis Workers (并行处理)
                     ↓
            PostgreSQL (结果持久化)
```

**关键决策**：
- Celery解耦HTTP和分析引擎
- 3次重试 + 死信队列
- 可中断超时机制

---

## 📅 15天执行路线图

### Phase 0: 环境准备 (Day 0)

**目标**: 干净的开发环境 + 工具链

```bash
# 1. 创建全新仓库
mkdir RedditSignalScanner && cd RedditSignalScanner
git init

# 2. Python环境（3.11）
python3.11 -m venv venv
source venv/bin/activate

# 3. 安装核心依赖
pip install fastapi[all]==0.104.1 \
            sqlalchemy==2.0.23 \
            alembic==1.12.0 \
            pydantic==2.5.0 \
            celery==5.3.4 \
            redis==5.0.1

# 4. 类型检查工具
pip install mypy==1.7.0

# 5. 测试工具
pip install pytest==7.4.0 pytest-asyncio pytest-cov

# 6. 配置mypy (100% strict)
cat > mypy.ini << EOF
[mypy]
python_version = 3.11
strict = True
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_any_unimported = True
EOF

# 7. 前端环境（Node 18）
npx create-vite@latest frontend -- --template react-ts
cd frontend && npm install
```

**验收标准**:
- [ ] `python --version` 显示 3.11.x
- [ ] `mypy --version` 可用
- [ ] `pytest --version` 可用
- [ ] `npm --version` 可用

---

### Phase 1: 数据模型层 (Day 1-2)

**目标**: 完整的4表架构 + 迁移脚本

#### Day 1: 表结构设计

**文件**: `backend/app/models/__init__.py`

```python
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, String, UUID, Text, TIMESTAMP, Enum, Numeric
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.sql import func
import enum
from typing import Optional

Base = declarative_base()

class TaskStatus(enum.Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class User(Base):
    __tablename__ = "users"

    id = Column(UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    email = Column(String(255), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    created_at = Column(TIMESTAMP(timezone=True), server_default=func.now())
    is_active = Column(Boolean, default=True)

class Task(Base):
    __tablename__ = "tasks"

    id = Column(UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    product_description = Column(Text, nullable=False)
    status = Column(Enum(TaskStatus), nullable=False, default=TaskStatus.PENDING)
    created_at = Column(TIMESTAMP(timezone=True), server_default=func.now())
    completed_at = Column(TIMESTAMP(timezone=True), nullable=True)

class Analysis(Base):
    __tablename__ = "analyses"

    id = Column(UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    task_id = Column(UUID(as_uuid=True), ForeignKey("tasks.id", ondelete="CASCADE"), nullable=False, unique=True)
    insights = Column(JSONB, nullable=False)  # Schema验证在应用层
    sources = Column(JSONB, nullable=False)
    confidence_score = Column(Numeric(3,2), nullable=False)
    created_at = Column(TIMESTAMP(timezone=True), server_default=func.now())

class CommunityCache(Base):
    __tablename__ = "community_cache"

    community_name = Column(String(100), primary_key=True)
    last_crawled_at = Column(TIMESTAMP(timezone=True), nullable=False)
    posts_cached = Column(Integer, default=0)
    quality_score = Column(Numeric(3,2), default=0.50)
    hit_count = Column(Integer, default=0)
```

**Pydantic Schema验证** (`backend/app/schemas/analysis.py`):

```python
from pydantic import BaseModel, Field, validator
from typing import List
from decimal import Decimal

class PainPoint(BaseModel):
    description: str = Field(min_length=10, max_length=500)
    frequency: int = Field(ge=1)
    sentiment_score: Decimal = Field(ge=-1.0, le=1.0)

class Competitor(BaseModel):
    name: str = Field(min_length=1, max_length=100)
    mentions: int = Field(ge=1)
    sentiment: str

class Opportunity(BaseModel):
    description: str = Field(min_length=10, max_length=500)
    relevance_score: Decimal = Field(ge=0.0, le=1.0)

class Insights(BaseModel):
    pain_points: List[PainPoint]
    competitors: List[Competitor]
    opportunities: List[Opportunity]

    @validator('pain_points')
    def validate_pain_points(cls, v: List[PainPoint]) -> List[PainPoint]:
        if len(v) == 0:
            raise ValueError("至少需要1个痛点")
        return v

class AnalysisCreate(BaseModel):
    task_id: str
    insights: Insights
    confidence_score: Decimal = Field(ge=0.0, le=1.0)
```

#### Day 2: 数据库迁移

**Alembic配置** (`backend/alembic/versions/001_initial_schema.py`):

```python
"""Initial schema

Revision ID: 001
Create Date: 2025-10-10
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

def upgrade() -> None:
    # 创建枚举
    op.execute("CREATE TYPE task_status AS ENUM ('pending', 'processing', 'completed', 'failed')")

    # 创建users表
    op.create_table(
        'users',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('email', sa.String(255), unique=True, nullable=False),
        sa.Column('password_hash', sa.String(255), nullable=False),
        sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default=sa.text('now()')),
        sa.Column('is_active', sa.Boolean(), default=True)
    )

    # 创建索引
    op.create_index('idx_users_email', 'users', ['email'])

    # ... 其他表创建（tasks, analyses, community_cache）

def downgrade() -> None:
    op.drop_table('community_cache')
    op.drop_table('analyses')
    op.drop_table('tasks')
    op.drop_table('users')
    op.execute("DROP TYPE task_status")
```

**测试** (`backend/tests/test_models.py`):

```python
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from backend.app.models import Base, User, Task, TaskStatus

@pytest.fixture
def db_session():
    engine = create_engine("postgresql://test:test@localhost:5432/test_db")
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()
    Base.metadata.drop_all(engine)

def test_create_user(db_session):
    user = User(email="test@example.com", password_hash="hashed")
    db_session.add(user)
    db_session.commit()

    assert user.id is not None
    assert user.email == "test@example.com"

def test_user_task_relationship(db_session):
    user = User(email="test@example.com", password_hash="hashed")
    db_session.add(user)
    db_session.commit()

    task = Task(user_id=user.id, product_description="Test product")
    db_session.add(task)
    db_session.commit()

    assert task.status == TaskStatus.PENDING
    assert task.user_id == user.id
```

**验收标准**:
- [ ] `alembic upgrade head` 成功执行
- [ ] 4个表全部创建
- [ ] 索引全部生成
- [ ] `pytest backend/tests/test_models.py` 全部通过
- [ ] `mypy backend/app/models` 0错误

---

### Phase 2: API层 (Day 3-5)

**目标**: 4个核心端点 + SSE推送

#### Day 3: 任务创建API

**文件**: `backend/app/api/v1/endpoints/analyze.py`

```python
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
from typing import Dict
from uuid import UUID
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

class AnalyzeRequest(BaseModel):
    product_description: str = Field(min_length=10, max_length=2000)

class AnalyzeResponse(BaseModel):
    task_id: str
    status: str
    created_at: str
    estimated_completion: str
    sse_endpoint: str

@router.post("/analyze", response_model=AnalyzeResponse, status_code=status.HTTP_201_CREATED)
async def create_analysis_task(
    request: AnalyzeRequest,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> AnalyzeResponse:
    """创建分析任务 - PRD-02要求<200ms响应"""
    try:
        # 创建任务记录
        task = Task(
            user_id=current_user.id,
            product_description=request.product_description,
            status=TaskStatus.PENDING
        )
        db.add(task)
        await db.commit()
        await db.refresh(task)

        # 异步触发Celery任务
        from backend.app.tasks.analysis_tasks import analyze_product
        analyze_product.delay(str(task.id))

        return AnalyzeResponse(
            task_id=str(task.id),
            status="pending",
            created_at=task.created_at.isoformat(),
            estimated_completion=(task.created_at + timedelta(minutes=5)).isoformat(),
            sse_endpoint=f"/api/analyze/stream/{task.id}"
        )
    except Exception as e:
        logger.error(f"Failed to create task: {e}")
        raise HTTPException(status_code=500, detail="任务创建失败")
```

#### Day 4: SSE实时推送

**文件**: `backend/app/api/v1/endpoints/stream.py`

```python
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
import asyncio
import json
from typing import AsyncGenerator

router = APIRouter()

@router.get("/analyze/stream/{task_id}")
async def stream_progress(task_id: str, request: Request) -> StreamingResponse:
    """SSE实时进度推送 - PRD-02核心功能"""

    async def event_generator() -> AsyncGenerator[str, None]:
        try:
            # 发送连接确认
            yield f"data: {json.dumps({'event': 'connected', 'task_id': task_id})}\n\n"

            # 监控任务进度
            while True:
                # 检查客户端是否断开
                if await request.is_disconnected():
                    break

                # 从Redis获取任务状态
                status = await get_task_status_from_redis(task_id)

                if status.status == "processing":
                    # 发送进度更新
                    event_data = {
                        "event": "progress",
                        "status": "processing",
                        "current_step": status.current_step,
                        "percentage": status.percentage
                    }
                    yield f"data: {json.dumps(event_data)}\n\n"

                elif status.status == "completed":
                    # 发送完成事件
                    yield f"event: completed\n"
                    yield f"data: {json.dumps({'event': 'completed', 'task_id': task_id})}\n\n"
                    break

                elif status.status == "failed":
                    # 发送错误事件
                    yield f"event: error\n"
                    yield f"data: {json.dumps({'event': 'error', 'error': status.error})}\n\n"
                    break

                await asyncio.sleep(1)  # 1秒轮询间隔

        finally:
            yield f"event: close\n"
            yield f"data: {json.dumps({'event': 'connection_closed'})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
```

#### Day 5: 状态查询 + 报告获取

**文件**: `backend/app/api/v1/endpoints/status.py`

```python
@router.get("/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> TaskStatusResponse:
    """查询任务状态 - PRD-02 Fallback轮询"""
    task = await db.get(Task, UUID(task_id))

    if not task:
        raise HTTPException(status_code=404, detail="任务不存在")

    # 多租户隔离检查
    if task.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="无权访问")

    return TaskStatusResponse(
        task_id=str(task.id),
        status=task.status.value,
        created_at=task.created_at.isoformat(),
        completed_at=task.completed_at.isoformat() if task.completed_at else None
    )
```

**验收标准**:
- [ ] POST /api/analyze 响应时间 < 200ms
- [ ] SSE连接成功建立并推送事件
- [ ] 轮询API响应时间 < 50ms
- [ ] 多租户隔离测试通过
- [ ] `mypy backend/app/api` 0错误

---

### Phase 3: 分析引擎 (Day 6-8)

**目标**: 4步分析流水线

**参考**: 完整实现见PRD-03，这里仅展示骨架

```python
# backend/app/services/analysis_engine.py
from typing import List, Dict
from pydantic import BaseModel

class AnalysisEngine:
    """4步分析流水线 - PRD-03"""

    async def analyze(self, product_description: str) -> Dict[str, Any]:
        # Step 1: 智能社区发现 (30秒)
        communities = await self.discover_communities(product_description)

        # Step 2: 并行数据采集 (120秒) - 缓存优先
        posts_data = await self.collect_data(communities)

        # Step 3: 统一信号提取 (90秒)
        signals = await self.extract_signals(posts_data)

        # Step 4: 智能排序输出 (30秒)
        report = await self.generate_report(signals)

        return report
```

**验收标准**:
- [ ] 总处理时间 < 270秒（5分钟以内）
- [ ] 缓存命中率 > 60%
- [ ] 信号提取准确率 > 75%（人工标注验证）

---

### Phase 4: 前端界面 (Day 9-11)

**目标**: 3页面React SPA

```typescript
// frontend/src/pages/InputPage.tsx
export function InputPage() {
  const [description, setDescription] = useState("");

  const handleSubmit = async () => {
    const response = await api.post("/api/analyze", {
      product_description: description
    });

    // 建立SSE连接
    const eventSource = new EventSource(
      `/api/analyze/stream/${response.data.task_id}`
    );

    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.event === "completed") {
        navigate(`/report/${response.data.task_id}`);
      }
    };
  };

  return <form onSubmit={handleSubmit}>...</form>;
}
```

**验收标准**:
- [ ] SSE客户端正常工作
- [ ] 3个页面路由正确
- [ ] TypeScript编译0错误
- [ ] 前端测试覆盖率 > 70%

---

### Phase 5: 测试与验收 (Day 12-15)

**目标**: 端到端测试 + 性能验证

```python
# tests/integration/test_e2e_flow.py
@pytest.mark.integration
async def test_complete_analysis_flow():
    """端到端测试：从提交到报告"""
    # 1. 创建任务
    response = await client.post("/api/analyze", json={
        "product_description": "AI笔记应用，帮助研究者管理知识"
    })
    assert response.status_code == 201
    task_id = response.json()["task_id"]

    # 2. 等待完成（最多5分钟）
    start_time = time.time()
    while time.time() - start_time < 300:
        status = await client.get(f"/api/status/{task_id}")
        if status.json()["status"] == "completed":
            break
        await asyncio.sleep(2)

    # 3. 验证报告
    report = await client.get(f"/api/report/{task_id}")
    assert report.status_code == 200
    data = report.json()
    assert len(data["pain_points"]) > 0
    assert data["confidence_score"] > 0.7
```

**验收标准**:
- [ ] E2E测试全部通过
- [ ] 性能测试符合PRD指标
- [ ] mypy --strict 0错误
- [ ] pytest覆盖率 > 80%

---

## 📊 质量门禁

### 每日检查清单

```bash
# 1. 类型检查（必须0错误）
mypy --strict backend/app backend/tests

# 2. 测试（必须全部通过）
pytest backend/tests -v --cov=backend.app --cov-report=term-missing

# 3. 代码格式
black backend/app --check
isort backend/app --check

# 4. 前端类型检查
cd frontend && npm run type-check

# 5. 前端测试
cd frontend && npm test
```

### 提交前检查

- [ ] mypy 0错误
- [ ] pytest 全部通过
- [ ] 覆盖率 > 80%
- [ ] 无console.log残留
- [ ] 文档更新

---

## 📚 关键文档索引

**必读PRD**（按实施顺序）：
1. `docs/PRD/PRD-01-数据模型.md` - 4表架构设计
2. `docs/PRD/PRD-02-API设计.md` - 4端点规范
3. `docs/PRD/PRD-03-分析引擎.md` - 4步算法
4. `docs/PRD/PRD-04-任务系统.md` - Celery配置
5. `docs/PRD/PRD-05-前端交互.md` - React SPA
6. `docs/PRD/PRD-06-用户认证.md` - JWT系统
7. `docs/PRD/PRD-07-Admin后台.md` - 监控面板
8. `docs/PRD/PRD-08-端到端测试规范.md` - 测试策略

**参考文档**：
- `docs/PRD/ARCHITECTURE.md` - 架构边界
- `docs/PRD/PRD-INDEX.md` - PRD依赖关系
- `2025-10-09-敏捷开发避坑与高效交付指南.md` - 开发流程

---

## ✅ 最终交付清单

### 代码交付
- [ ] 后端代码（backend/）
- [ ] 前端代码（frontend/）
- [ ] 数据库迁移（alembic/）
- [ ] 测试代码（tests/）
- [ ] 配置文件（.env.example, docker-compose.yml）

### 文档交付
- [ ] README.md（快速开始）
- [ ] API文档（OpenAPI/Swagger）
- [ ] 部署文档
- [ ] 维护手册
- [ ] 架构决策记录（ADR）

### 质量报告
- [ ] 类型检查报告（mypy）
- [ ] 测试覆盖率报告
- [ ] 性能测试报告
- [ ] PRD符合度报告

---

## 🎯 成功标准

1. **功能完整性**: 8个PRD 100%实现
2. **类型安全**: mypy --strict 0错误
3. **测试覆盖**: 后端>80%, 前端>70%
4. **性能达标**: 5分钟分析承诺兑现
5. **代码质量**: 无type:ignore, 无Any类型
6. **文档同步**: 代码与PRD 100%对应

---

**祝开发顺利！记住Linus的话："Talk is cheap. Show me the code."**
