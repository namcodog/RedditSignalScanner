#!/usr/bin/env python3
"""
ç²¾å‡†è¡¥æŠ“ä»»åŠ¡

é’ˆå¯¹ä½è´¨é‡æˆ–é•¿æ—¶é—´æœªæ›´æ–°çš„ç¤¾åŒºè¿›è¡Œå®šå‘è¡¥æŠ“ï¼Œ
æå‡æ•´ä½“æ•°æ®è¦†ç›–åº¦å’Œæ–°é²œåº¦ã€‚

æŸ¥è¯¢æ¡ä»¶:
- last_crawled_at > 8 å°æ—¶ï¼ˆé•¿æ—¶é—´æœªæŠ“å–ï¼‰
- avg_valid_posts < 50ï¼ˆä½è´¨é‡æˆ–æ— æ•°æ®ï¼‰
- is_active = trueï¼ˆæ´»è·ƒç¤¾åŒºï¼‰
- is_blacklisted = falseï¼ˆéé»‘åå•ï¼‰

ä½¿ç”¨æ–¹æ³•:
    PYTHONPATH=backend python3 scripts/targeted_recrawl.py [--dry-run] [--hours HOURS] [--threshold THRESHOLD]
"""
import asyncio
import sys
from datetime import datetime, timedelta, timezone

from sqlalchemy import select, and_

from app.db.session import SessionFactory
from app.models.community_cache import CommunityCache
from app.models.community_pool import CommunityPool


async def find_recrawl_candidates(
    hours_threshold: int = 8,
    quality_threshold: int = 50,
    dry_run: bool = False
) -> list[str]:
    """æŸ¥æ‰¾éœ€è¦è¡¥æŠ“çš„ç¤¾åŒº"""
    now = datetime.now(timezone.utc)
    cutoff_time = now - timedelta(hours=hours_threshold)
    
    async with SessionFactory() as db:
        # æŸ¥è¯¢ç¬¦åˆæ¡ä»¶çš„ç¤¾åŒº
        query = (
            select(CommunityPool.name, CommunityCache.last_crawled_at, CommunityCache.avg_valid_posts)
            .join(
                CommunityCache,
                CommunityPool.name == CommunityCache.community_name,
                isouter=True
            )
            .where(
                and_(
                    CommunityPool.is_active == True,
                    CommunityPool.is_blacklisted == False,
                    # æ¡ä»¶1: é•¿æ—¶é—´æœªæŠ“å– OR ä»æœªæŠ“å–
                    (
                        (CommunityCache.last_crawled_at < cutoff_time) |
                        (CommunityCache.last_crawled_at == None)
                    ),
                    # æ¡ä»¶2: ä½è´¨é‡ OR æ— æ•°æ®
                    (
                        (CommunityCache.avg_valid_posts < quality_threshold) |
                        (CommunityCache.avg_valid_posts == None)
                    )
                )
            )
            .order_by(CommunityCache.last_crawled_at.asc().nullsfirst())
        )
        
        result = await db.execute(query)
        candidates = []
        
        print(f"ğŸ” æŸ¥æ‰¾è¡¥æŠ“å€™é€‰ç¤¾åŒº...")
        print(f"   - æ—¶é—´é˜ˆå€¼: {hours_threshold} å°æ—¶å‰")
        print(f"   - è´¨é‡é˜ˆå€¼: avg_valid_posts < {quality_threshold}")
        print(f"   - æˆªæ­¢æ—¶é—´: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        for name, last_crawled, avg_posts in result:
            candidates.append(name)
            
            if len(candidates) <= 20:  # åªæ‰“å°å‰ 20 ä¸ª
                last_crawled_str = last_crawled.strftime('%Y-%m-%d %H:%M:%S') if last_crawled else 'ä»æœªæŠ“å–'
                avg_posts_str = str(avg_posts) if avg_posts is not None else 'æ— æ•°æ®'
                print(f"  âœ“ {name}")
                print(f"      æœ€åæŠ“å–: {last_crawled_str}")
                print(f"      è´¨é‡åˆ†: {avg_posts_str}")
        
        if len(candidates) > 20:
            print(f"  ... è¿˜æœ‰ {len(candidates) - 20} ä¸ªç¤¾åŒº")
        
        return candidates


async def execute_recrawl(
    candidates: list[str],
    dry_run: bool = False
) -> None:
    """æ‰§è¡Œè¡¥æŠ“ä»»åŠ¡"""
    if not candidates:
        print("\nâš ï¸  æ— éœ€è¡¥æŠ“çš„ç¤¾åŒº")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¡¥æŠ“ç»Ÿè®¡:")
    print(f"   - å€™é€‰ç¤¾åŒºæ•°: {len(candidates)}")
    
    if dry_run:
        print(f"\nğŸ” DRY RUN æ¨¡å¼ - ä¸ä¼šå®é™…æŠ“å–")
        print(f"\nå»ºè®®æ‰§è¡Œå‘½ä»¤:")
        print(f"PYTHONPATH=backend \\")
        print(f"CRAWLER_SORT=top \\")
        print(f"CRAWLER_TIME_FILTER=month \\")
        print(f"CRAWLER_POST_LIMIT=100 \\")
        print(f"CRAWLER_BATCH_SIZE={min(len(candidates), 15)} \\")
        print(f"CRAWLER_MAX_CONCURRENCY=3 \\")
        print(f"python3 scripts/run-incremental-crawl.py")
        return
    
    # å®é™…æ‰§è¡Œè¡¥æŠ“
    print(f"\nğŸš€ å¼€å§‹è¡¥æŠ“...")
    print(f"   - ç­–ç•¥: sort=top, time_filter=month, limit=100")
    print(f"   - æ‰¹æ¬¡å¤§å°: {min(len(candidates), 15)}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    import os
    os.environ["CRAWLER_SORT"] = "top"
    os.environ["CRAWLER_TIME_FILTER"] = "month"
    os.environ["CRAWLER_POST_LIMIT"] = "100"
    os.environ["CRAWLER_BATCH_SIZE"] = str(min(len(candidates), 15))
    os.environ["CRAWLER_MAX_CONCURRENCY"] = "3"
    
    # è°ƒç”¨å¢é‡æŠ“å–
    # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥è°ƒç”¨ crawler_task
    print(f"\nğŸ’¡ æç¤º: è¯·æ‰‹åŠ¨æ‰§è¡Œä¸Šè¿°å‘½ä»¤è¿›è¡Œè¡¥æŠ“")


async def main() -> None:
    """ä¸»å‡½æ•°"""
    dry_run = "--dry-run" in sys.argv
    
    # è§£æå‚æ•°
    hours_threshold = 8
    quality_threshold = 50
    
    if "--hours" in sys.argv:
        hours_idx = sys.argv.index("--hours")
        if hours_idx + 1 < len(sys.argv):
            hours_threshold = int(sys.argv[hours_idx + 1])
    
    if "--threshold" in sys.argv:
        threshold_idx = sys.argv.index("--threshold")
        if threshold_idx + 1 < len(sys.argv):
            quality_threshold = int(sys.argv[threshold_idx + 1])
    
    if dry_run:
        print("ğŸ” DRY RUN æ¨¡å¼å¯ç”¨\n")
    
    print("=" * 60)
    print("ğŸ¯ ç²¾å‡†è¡¥æŠ“ä»»åŠ¡")
    print("=" * 60)
    print()
    
    # æŸ¥æ‰¾å€™é€‰ç¤¾åŒº
    candidates = await find_recrawl_candidates(
        hours_threshold=hours_threshold,
        quality_threshold=quality_threshold,
        dry_run=dry_run
    )
    
    # æ‰§è¡Œè¡¥æŠ“
    await execute_recrawl(candidates, dry_run=dry_run)
    
    print(f"\n{'='*60}")
    print("âœ… è¡¥æŠ“ä»»åŠ¡å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

