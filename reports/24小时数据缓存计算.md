# 24 小时数据缓存容量计算

生成时间：2025-10-16 19:35

---

## 📊 当前配置参数

### 抓取配置

| 参数 | 当前值 | 说明 |
|------|--------|------|
| 抓取频率 | 每 1 小时 | 每小时整点执行 |
| 社区数量 | 100 个 | 实际成功爬取的社区 |
| 每社区帖子数 | 100 条 | Reddit API 限制 |
| 时间范围 | 最近 1 周 | time_filter="week" |
| 排序方式 | 热门 | sort="top" |
| 缓存 TTL | 3600 秒 | 1 小时过期 |

### Reddit API 限制

| 限制类型 | 限制值 | 说明 |
|---------|--------|------|
| 速率限制 | 30 次/分钟 | 我们的配置（官方 60 次/分钟） |
| 并发限制 | 5 个请求 | 同时进行的请求数 |
| 单次搜索 | 最多 100 条 | Reddit API 限制 |
| 单个社区 | 最多 1000 条 | 我们限制 100 条 |

---

## 🧮 数据量计算

### 单次爬取数据量

**理论最大值：**
```
100 个社区 × 100 条帖子/社区 = 10,000 条帖子
```

**实际平均值（基于真实数据）：**
```
100 个社区 × 83.8 条帖子/社区 = 8,380 条帖子
```

**原因：**
- 部分小众社区活跃度低（如 `interiordesign` 只有 17 条）
- 部分社区在最近 1 周内帖子不足 100 条
- Reddit API 有时返回少于请求的数量

---

### 24 小时数据量

#### 方案 1：当前配置（每 1 小时刷新）

**问题：缓存会被覆盖！**

由于缓存 TTL 是 1 小时，每次爬取会**覆盖**上次的数据，而不是累积。

```
24 小时内的数据量 = 8,380 条帖子（最新的一次爬取）
```

**数据新鲜度：**
- ✅ 非常新鲜（<1 小时）
- ✅ 始终是最新的热门帖子
- ❌ 无法积累历史数据

---

#### 方案 2：如果要累积数据（需要修改）

**假设：不覆盖缓存，而是追加到数据库**

```
单次爬取：8,380 条帖子
24 小时爬取次数：24 次
理论总量：8,380 × 24 = 201,120 条帖子

去重后（估计 50% 重复）：
201,120 × 0.5 = 100,560 条独特帖子
```

**但这需要：**
1. 修改缓存策略（不覆盖，而是追加）
2. 增加数据库存储（存储所有历史帖子）
3. 去重逻辑（避免重复存储）

---

## 📈 不同时间范围的数据量对比

### 当前配置（time_filter="week"）

| 时间 | 数据量 | 说明 |
|------|--------|------|
| 1 小时 | 8,380 条 | 单次爬取 |
| 24 小时 | 8,380 条 | 缓存覆盖，不累积 |
| 7 天 | 8,380 条 | 缓存覆盖，不累积 |

**特点：**
- ✅ 数据始终新鲜
- ✅ 存储空间小
- ❌ 无法分析趋势

---

### 如果改为 time_filter="month"

| 时间 | 数据量 | 说明 |
|------|--------|------|
| 1 小时 | ~15,000 条 | 单次爬取（估计） |
| 24 小时 | ~15,000 条 | 缓存覆盖 |
| 30 天 | ~15,000 条 | 缓存覆盖 |

**特点：**
- ✅ 数据覆盖范围更广
- ✅ 更多历史帖子
- ⚠️ 数据不够新鲜
- ⚠️ 爬取时间更长

---

### 如果改为累积模式（存储到数据库）

| 时间 | 数据量 | 说明 |
|------|--------|------|
| 1 小时 | 8,380 条 | 单次爬取 |
| 24 小时 | ~100,000 条 | 去重后 |
| 7 天 | ~300,000 条 | 去重后 |
| 30 天 | ~800,000 条 | 去重后 |

**特点：**
- ✅ 可以分析趋势
- ✅ 数据量大
- ❌ 存储空间大（~500MB）
- ❌ 查询速度慢

---

## 🎯 推荐方案对比

### 方案 A：当前配置（缓存覆盖模式）

**配置：**
- 频率：每 1 小时
- 时间范围：最近 1 周
- 存储：Redis 缓存（覆盖）
- 数据量：8,380 条（固定）

**优点：**
- ✅ 数据始终新鲜
- ✅ 存储空间小（~50MB）
- ✅ 查询速度快
- ✅ 适合实时分析

**缺点：**
- ❌ 无法分析趋势
- ❌ 无法回溯历史

**适用场景：**
- 用户提交产品描述，立即分析
- 关注最新的市场动态
- 快速验证产品想法

---

### 方案 B：扩大时间范围（month）

**配置：**
- 频率：每 2 小时
- 时间范围：最近 1 个月
- 存储：Redis 缓存（覆盖）
- 数据量：~15,000 条（固定）

**优点：**
- ✅ 数据覆盖范围更广
- ✅ 更多历史帖子
- ✅ 存储空间适中（~80MB）

**缺点：**
- ⚠️ 数据不够新鲜
- ⚠️ 爬取时间更长（~8 分钟）

**适用场景：**
- 需要更多历史数据
- 分析长期趋势
- 竞品分析

---

### 方案 C：累积模式（数据库存储）

**配置：**
- 频率：每 6 小时
- 时间范围：最近 1 周
- 存储：PostgreSQL（累积）
- 数据量：24 小时 ~30,000 条，7 天 ~300,000 条

**优点：**
- ✅ 可以分析趋势
- ✅ 可以回溯历史
- ✅ 数据量大

**缺点：**
- ❌ 存储空间大（~500MB/月）
- ❌ 查询速度慢
- ❌ 需要去重逻辑
- ❌ 需要数据清理策略

**适用场景：**
- 需要趋势分析
- 需要历史回溯
- 数据科学研究

---

## 💡 优化建议

### 短期优化（1-2 周）

**建议 1：调整时间范围为 "month"**

```python
# backend/app/tasks/crawler_task.py
posts = await reddit_client.fetch_subreddit_posts(
    api_subreddit,
    limit=100,
    time_filter="month",  # 从 "week" 改为 "month"
    sort="top",
)
```

**效果：**
- 数据量：8,380 → ~15,000 条（+78%）
- 覆盖范围：7 天 → 30 天
- 爬取时间：5 分钟 → 8 分钟

---

**建议 2：降低爬取频率为 2 小时**

```python
# backend/app/core/celery_app.py
"auto-crawl-seed-communities": {
    "task": "tasks.crawler.crawl_seed_communities",
    "schedule": crontab(minute="0", hour="*/2"),  # 从每小时改为每 2 小时
},
```

**效果：**
- Reddit API 压力减少 50%
- 缓存命中率提高（TTL 可以延长到 2 小时）
- 数据新鲜度仍然足够

---

### 中期优化（1 个月）

**建议 3：增加数据库存储层**

```python
# 新增表：reddit_posts
CREATE TABLE reddit_posts (
    id VARCHAR(50) PRIMARY KEY,
    subreddit VARCHAR(100),
    title TEXT,
    content TEXT,
    score INTEGER,
    num_comments INTEGER,
    created_at TIMESTAMP,
    crawled_at TIMESTAMP,
    INDEX idx_subreddit (subreddit),
    INDEX idx_created_at (created_at)
);
```

**效果：**
- 可以累积历史数据
- 可以分析趋势
- 可以回溯历史

---

**建议 4：智能去重策略**

```python
# 只存储新帖子或有更新的帖子
async def upsert_post(post):
    existing = await db.get_post(post.id)
    if not existing or existing.score != post.score:
        await db.save_post(post)
```

**效果：**
- 减少存储空间
- 避免重复数据
- 保持数据更新

---

## 📊 存储空间估算

### 单条帖子数据大小

```json
{
  "id": "abc123",
  "subreddit": "entrepreneur",
  "title": "How I built a $10k/month SaaS...",
  "selftext": "Long story...",
  "score": 150,
  "num_comments": 45,
  "created_utc": 1697500000,
  "url": "https://reddit.com/..."
}
```

**估算：**
- 平均大小：~2 KB/条
- 8,380 条：~16 MB
- 100,000 条：~200 MB
- 1,000,000 条：~2 GB

---

### Redis 缓存空间

**当前配置：**
```
100 个社区 × 8,380 条 × 2 KB = ~16 MB
```

**扩展到 month：**
```
100 个社区 × 15,000 条 × 2 KB = ~30 MB
```

**完全可接受！**

---

### PostgreSQL 存储空间

**7 天累积：**
```
300,000 条 × 2 KB = ~600 MB
```

**30 天累积：**
```
800,000 条 × 2 KB = ~1.6 GB
```

**需要定期清理！**

---

## 🎯 最终推荐

### 推荐配置（平衡方案）

**抓取配置：**
- 频率：**每 2 小时**（降低 API 压力）
- 时间范围：**最近 1 个月**（更多数据）
- 每社区帖子数：**100 条**（保持不变）
- 缓存 TTL：**7200 秒**（2 小时）

**预期数据量：**
- 单次爬取：~15,000 条帖子
- 24 小时：~15,000 条（缓存覆盖）
- 存储空间：~30 MB（Redis）

**优点：**
- ✅ 数据量提升 78%
- ✅ 覆盖范围扩大 4 倍
- ✅ API 压力减少 50%
- ✅ 存储空间可控

**实施步骤：**
1. 修改 `time_filter` 为 `"month"`
2. 修改爬取频率为 `crontab(minute="0", hour="*/2")`
3. 修改缓存 TTL 为 `7200` 秒
4. 重启 Celery Beat

---

## 📝 总结

### 当前配置（每 1 小时，week）

```
24 小时数据量：8,380 条（固定，不累积）
存储空间：~16 MB
数据新鲜度：<1 小时
```

### 推荐配置（每 2 小时，month）

```
24 小时数据量：~15,000 条（固定，不累积）
存储空间：~30 MB
数据新鲜度：<2 小时
```

### 如果需要累积（需要开发）

```
24 小时数据量：~100,000 条（去重后）
7 天数据量：~300,000 条
存储空间：~600 MB
需要：数据库存储 + 去重逻辑 + 清理策略
```

---

**你希望我现在实施推荐配置吗？（每 2 小时，month）**

