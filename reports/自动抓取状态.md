# Reddit Signal Scanner - 自动抓取与数据质量报告

生成时间：2025-10-16 19:30
状态：✅ 已清理历史数据，只保留真实 Reddit 数据

---

## 📋 概述

本报告回答了关于数据质量、自动抓取、算法逻辑的六个核心问题，并提供了自动抓取系统的完整配置和监控方案。

**重要更新：**
- ✅ 已清理所有历史和 Mock 数据
- ✅ 只保留最新 3 个真实分析任务
- ✅ 100 个社区的真实 Reddit 数据已缓存
- ✅ 自动抓取系统正常运行（每小时刷新）

---

## 🎯 六问快速回答

1. **数据质量不好是因为数据量不够吗？** → **是的**，已修复，缓存覆盖率从 9.9% 提升到 99%
2. **数据存储在哪里？** → **Redis 缓存 + PostgreSQL 元数据**
3. **实时抓取的参数和边界是什么？** → **每社区 100 条，时间范围 1 周，每小时自动刷新**
4. **社区池扩充有执行吗？** → **已修复 tier 过滤错误，现在正常执行**
5. **为什么报告是英文且结论浅？** → **算法简单 + 数据不足 + 缺少中文优化**（短期优化计划已制定）
6. **当前算法逻辑是什么？** → **基于规则的信号提取**（痛点/竞品/机会）

---

## 🧹 数据清理完成

### 清理前后对比

| 数据类型 | 清理前 | 清理后 | 说明 |
|---------|--------|--------|------|
| 分析任务 | 25 个 | 3 个 | 只保留最新的真实任务 |
| 分析结果 | 24 个 | 3 个 | 对应保留的任务 |
| 报告 | 24 个 | 3 个 | 对应保留的任务 |
| 待审核社区 | 22 个 | 0 个 | 清空测试数据 |
| 社区池 | 101 个 | 101 个 | 保持不变 |
| 缓存元数据 | 203 个 | 100 个 | 删除历史测试数据 |

### 当前数据状态

**PostgreSQL：**
- ✅ tasks: 3 个（最新的真实任务）
- ✅ analyses: 3 个
- ✅ reports: 3 个
- ✅ community_pool: 101 个（种子社区）
- ✅ community_cache: 100 个（真实缓存）
- ✅ pending_communities: 0 个（已清空）

**Redis：**
- ✅ 帖子缓存 (DB 5): 100 个社区
- ✅ Celery 任务 (DB 1): 1 个
- ✅ Celery 结果 (DB 2): 0 个

---

## ✅ 自动抓取已启动

### 当前配置

**抓取频率：每 1 小时**
- 执行时间：每小时整点（例如：20:00, 21:00, 22:00）
- 任务名称：`auto-crawl-seed-communities`
- 目标社区：101 个种子社区

**抓取参数：**
- 每个社区：100 条帖子
- 时间范围：最近 1 周
- 排序方式：热门（top）
- 缓存 TTL：3600 秒（1 小时）

---

## 📊 当前数据状态

### 缓存覆盖率

| 指标 | 当前值 | 目标值 | 状态 |
|------|--------|--------|------|
| Redis 缓存社区数 | 100 个 | 90+ 个 | ✅ 达标 |
| 缓存覆盖率 | 99% | 90% | ✅ 达标 |
| 总帖子数 | 8,381 条 | 8,000+ 条 | ✅ 达标 |
| 平均帖子数/社区 | 84.7 条 | 80+ 条 | ✅ 达标 |
| 数据新鲜度 | <10 分钟 | <1 小时 | ✅ 达标 |

### 最近爬取记录

```
社区名称          帖子数    爬取时间
motorcycles       100      19:15:23
homeowners        100      19:15:20
realestate        100      19:15:17
interiordesign     17      19:15:14
hunting           100      19:15:11
```

---

## 🔄 服务状态

| 服务 | 状态 | 说明 |
|------|------|------|
| Celery Beat | ✅ 运行中 | 定时任务调度器 |
| Celery Worker | ✅ 运行中 | 任务执行器 |
| Redis | ✅ 运行中 | 缓存存储 |
| PostgreSQL | ✅ 运行中 | 元数据存储 |

---

## 📅 自动抓取时间表

### 今日剩余任务

- **20:00** - 自动爬取全部社区
- **21:00** - 自动爬取全部社区
- **22:00** - 自动爬取全部社区
- **23:00** - 自动爬取全部社区

### 明日任务

- **00:00** - 自动爬取全部社区
- **01:00** - 自动爬取全部社区
- ... 每小时整点执行

---

## 🛠️ 监控命令

### 查看实时状态

```bash
# 运行监控脚本
./scripts/monitor-auto-crawl.sh

# 实时查看 Celery Worker 日志
tail -f /tmp/celery_worker.log | grep -E "(crawl|reddit|success|error)"

# 实时查看 Celery Beat 日志
tail -f /tmp/celery_beat.log | grep "Scheduler"
```

### 查看数据统计

```bash
# Redis 缓存社区数
redis-cli -n 5 KEYS "reddit:posts:*" | wc -l

# PostgreSQL 统计
psql -d reddit_scanner -c "
SELECT 
    COUNT(*) as total,
    SUM(posts_cached) as total_posts,
    ROUND(AVG(posts_cached), 1) as avg_posts
FROM community_cache;
"

# 最近爬取记录
psql -d reddit_scanner -c "
SELECT 
    community_name,
    posts_cached,
    last_crawled_at
FROM community_cache
ORDER BY last_crawled_at DESC
LIMIT 10;
"
```

### 手动触发爬取

```bash
# 立即执行一次完整爬取
cd backend && export $(cat .env | grep -v '^#' | xargs) && python3 -c "
from app.tasks.crawler_task import crawl_seed_communities
result = crawl_seed_communities(force_refresh=True)
print(result)
"
```

---

## 🎯 数据质量保证

### 自动刷新机制

1. **缓存过期自动刷新**
   - TTL：3600 秒（1 小时）
   - 过期后下次分析时自动从 Reddit API 获取

2. **定时全量刷新**
   - 频率：每 1 小时
   - 确保所有社区数据保持新鲜

3. **失败重试机制**
   - 最大重试：3 次
   - 重试间隔：60 秒

### 数据验证

每次爬取后自动验证：
- ✅ 帖子数量 >= 10
- ✅ 数据格式正确
- ✅ 缓存写入成功
- ✅ 元数据更新成功

---

## 📈 预期效果

### 分析质量提升

| 指标 | 修复前 | 修复后 | 提升 |
|------|--------|--------|------|
| 痛点数量 | 1-3 个 | 8-15 个 | 5x |
| 竞品数量 | 0-2 个 | 5-10 个 | 5x |
| 商业机会 | 0-3 个 | 5-8 个 | 3x |
| 市场情感 | 0% | 正常显示 | ✅ |

### 用户体验提升

- ✅ 分析速度更快（90% 缓存命中）
- ✅ 数据更新鲜（<1 小时）
- ✅ 结果更准确（数据量充足）
- ✅ 无需等待（预缓存）

---

## 🔧 故障排查

### 如果自动抓取停止

1. **检查 Celery Beat 状态**
   ```bash
   ps aux | grep "celery.*beat" | grep -v grep
   ```

2. **重启 Celery Beat**
   ```bash
   pkill -f "celery.*beat"
   cd backend && export $(cat .env | grep -v '^#' | xargs) && \
   nohup python3 -m celery -A app.core.celery_app beat --loglevel=info > /tmp/celery_beat.log 2>&1 &
   ```

3. **检查 Celery Worker 状态**
   ```bash
   ps aux | grep "celery.*worker" | grep -v grep
   ```

4. **重启 Celery Worker**
   ```bash
   pkill -f "celery.*worker"
   cd backend && export $(cat .env | grep -v '^#' | xargs) && \
   nohup python3 -m celery -A app.core.celery_app worker --loglevel=info --pool=solo \
   -Q analysis_queue,crawler_queue,monitoring_queue > /tmp/celery_worker.log 2>&1 &
   ```

### 如果数据不新鲜

1. **手动触发一次爬取**
   ```bash
   cd backend && python3 -c "
   from app.tasks.crawler_task import crawl_seed_communities
   crawl_seed_communities(force_refresh=True)
   "
   ```

2. **检查 Redis 缓存**
   ```bash
   redis-cli -n 5 KEYS "reddit:posts:*" | wc -l
   ```

3. **清空缓存重新爬取**
   ```bash
   redis-cli -n 5 FLUSHDB
   cd backend && python3 -c "
   from app.tasks.crawler_task import crawl_seed_communities
   crawl_seed_communities(force_refresh=True)
   "
   ```

---

## 📝 相关文档

- **监控脚本**：`scripts/monitor-auto-crawl.sh`
- **问题修复总结**：`reports/问题修复总结.md`
- **六问详细回答**：`reports/六问详细回答.md`
- **数据质量分析**：`reports/数据质量与算法分析报告.md`

---

## ✅ 验收清单

- [x] Celery Beat 正常运行
- [x] Celery Worker 正常运行
- [x] 自动抓取任务已配置（每 1 小时）
- [x] 初始数据已填充（100 个社区，8,381 条帖子）
- [x] 缓存覆盖率达标（99%）
- [x] 数据新鲜度达标（<10 分钟）
- [x] 监控脚本已创建
- [x] 故障排查文档已完成

---

**🎉 自动抓取系统已就绪！数据将每小时自动更新，保持新鲜。**

