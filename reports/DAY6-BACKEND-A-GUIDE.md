# Day 6 Backend A å¼€å‘æŒ‡å—

> **è§’è‰²**: Backend Agent A (èµ„æ·±åç«¯)
> **æ—¥æœŸ**: 2025-10-12 (Day 6)
> **æ ¸å¿ƒä»»åŠ¡**: å®ç°åˆ†æå¼•æ“Step 1 - ç¤¾åŒºå‘ç°ç®—æ³•
> **é¢„è®¡ç”¨æ—¶**: 8å°æ—¶

---

## ğŸ¯ ä»Šæ—¥ç›®æ ‡

1. âœ… å¯åŠ¨BackendæœåŠ¡æ”¯æŒFrontendè”è°ƒ
2. âœ… å®ç°TF-IDFå…³é”®è¯æå–ç®—æ³•
3. âœ… å®ç°ç¤¾åŒºå‘ç°æ ¸å¿ƒç®—æ³•
4. âœ… å®Œæˆå•å…ƒæµ‹è¯•ä¸æ€§èƒ½ä¼˜åŒ–
5. âœ… MyPy --strict 0 errors

---

## ğŸ“‹ è¯¦ç»†ä»»åŠ¡æ¸…å•

### ä¸Šåˆä»»åŠ¡ (9:00-12:00)

#### âœ… ä»»åŠ¡1: å¯åŠ¨BackendæœåŠ¡ (9:00-9:30, 30åˆ†é’Ÿ)

**ç›®æ ‡**: æ”¯æŒFrontendè¿›è¡ŒAPIé›†æˆæµ‹è¯•

**æ‰§è¡Œæ­¥éª¤**:

```bash
# 1. å¯åŠ¨ä¾èµ–æœåŠ¡
cd /Users/hujia/Desktop/RedditSignalScanner
docker-compose up -d postgres redis

# 2. ç¡®è®¤æœåŠ¡è¿è¡Œ
docker-compose ps

# 3. è¿è¡Œæ•°æ®åº“è¿ç§»
cd backend
source ../venv/bin/activate
alembic upgrade head

# 4. å¯åŠ¨FastAPIæœåŠ¡
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# 5. æ–°ç»ˆç«¯éªŒè¯æœåŠ¡
curl http://localhost:8000/health
# æœŸæœ›: {"status": "healthy"}

curl http://localhost:8000/docs
# æœŸæœ›: è¿”å›Swagger UI HTML

# 6. ç”Ÿæˆæµ‹è¯•Token(å¦‚éœ€è¦)
python scripts/generate_test_token.py
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Dockerå®¹å™¨æ­£å¸¸è¿è¡Œ
- [ ] æ•°æ®åº“è¿ç§»æˆåŠŸ
- [ ] FastAPIæœåŠ¡å¯åŠ¨æ— é”™è¯¯
- [ ] /healthç«¯ç‚¹è¿”å›200
- [ ] /docså¯è®¿é—®

**é€šçŸ¥Frontend**:
```
âœ… BackendæœåŠ¡å·²å¯åŠ¨
- æœåŠ¡åœ°å€: http://localhost:8000
- Swaggeræ–‡æ¡£: http://localhost:8000/docs
- Health check: http://localhost:8000/health
- æµ‹è¯•Tokenå·²å‡†å¤‡å¥½,å¯ä»¥å¼€å§‹é›†æˆæµ‹è¯•
```

---

#### âœ… ä»»åŠ¡2: å®ç°TF-IDFå…³é”®è¯æå– (9:30-11:30, 2å°æ—¶)

**ç›®æ ‡**: å®ç°é«˜è´¨é‡çš„å…³é”®è¯æå–ç®—æ³•

**2.1 å®‰è£…ä¾èµ–** (5åˆ†é’Ÿ)

```bash
# æ·»åŠ scikit-learnåˆ°requirements.txt
echo "scikit-learn==1.3.2" >> backend/requirements.txt

# å®‰è£…ä¾èµ–
pip install scikit-learn==1.3.2
```

**2.2 åˆ›å»ºæ¨¡å—æ–‡ä»¶** (5åˆ†é’Ÿ)

```bash
# åˆ›å»ºanalysisæœåŠ¡ç›®å½•
mkdir -p backend/app/services/analysis
touch backend/app/services/analysis/__init__.py

# åˆ›å»ºå…³é”®è¯æå–æ¨¡å—
touch backend/app/services/analysis/keyword_extraction.py

# åˆ›å»ºæµ‹è¯•ç›®å½•
mkdir -p backend/tests/services/analysis
touch backend/tests/services/analysis/__init__.py
touch backend/tests/services/analysis/test_keyword_extraction.py
```

**2.3 å®ç°å…³é”®è¯æå–ç®—æ³•** (40åˆ†é’Ÿ)

```python
# backend/app/services/analysis/keyword_extraction.py
"""
å…³é”®è¯æå–ç®—æ³• - TF-IDFå®ç°
åŸºäºPRD-03 Â§ 3.1è®¾è®¡
"""
from __future__ import annotations

from typing import List, Dict
from dataclasses import dataclass
import re

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


@dataclass
class KeywordExtractionResult:
    """å…³é”®è¯æå–ç»“æœ"""
    keywords: List[str]
    weights: Dict[str, float]
    total_keywords: int


async def extract_keywords(
    text: str,
    max_keywords: int = 20,
    min_keyword_length: int = 3,
) -> KeywordExtractionResult:
    """
    TF-IDFå…³é”®è¯æå–ç®—æ³•

    ç®—æ³•æ­¥éª¤:
    1. æ–‡æœ¬é¢„å¤„ç†(æ¸…æ´—ç‰¹æ®Šå­—ç¬¦ã€URLã€é‚®ç®±)
    2. TF-IDFå‘é‡åŒ–(æ”¯æŒ1-2 gram)
    3. æå–Top-Kå…³é”®è¯
    4. è¿‡æ»¤çŸ­è¯(<3å­—ç¬¦)

    Args:
        text: è¾“å…¥æ–‡æœ¬(äº§å“æè¿°)
        max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡
        min_keyword_length: æœ€å°å…³é”®è¯é•¿åº¦

    Returns:
        KeywordExtractionResult: å…³é”®è¯åŠå…¶æƒé‡

    Raises:
        ValueError: è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­

    ç¤ºä¾‹:
        >>> result = await extract_keywords(
        ...     "AI-powered note-taking app for researchers",
        ...     max_keywords=5
        ... )
        >>> print(result.keywords)
        ['ai powered', 'note taking', 'researchers', 'app']
    """
    # 1. è¾“å…¥éªŒè¯
    if not text or len(text) < 10:
        raise ValueError(
            f"Input text must be at least 10 characters long, got {len(text)}"
        )

    # 2. æ–‡æœ¬é¢„å¤„ç†
    cleaned_text = _preprocess_text(text)

    if not cleaned_text or len(cleaned_text) < 10:
        raise ValueError(
            "Text became too short after preprocessing"
        )

    # 3. TF-IDFè®¡ç®—
    vectorizer = TfidfVectorizer(
        max_features=max_keywords * 2,  # æå–æ›´å¤šå€™é€‰è¯
        stop_words='english',
        lowercase=True,
        min_df=1,
        ngram_range=(1, 2),  # å•è¯ + åŒè¯ç»„åˆ
        token_pattern=r'(?u)\b[a-z]{2,}\b',  # åªä¿ç•™å­—æ¯,æœ€å°‘2å­—ç¬¦
    )

    try:
        # æ‹Ÿåˆå¹¶è½¬æ¢
        tfidf_matrix = vectorizer.fit_transform([cleaned_text])
        feature_names = vectorizer.get_feature_names_out()

        # 4. æå–å…³é”®è¯å’Œæƒé‡
        scores = tfidf_matrix.toarray()[0]

        # è¿‡æ»¤çŸ­å…³é”®è¯
        keyword_scores = [
            (feature_names[i], scores[i])
            for i in range(len(feature_names))
            if len(feature_names[i]) >= min_keyword_length
        ]

        # 5. æŒ‰æƒé‡æ’åº
        keyword_scores.sort(key=lambda x: x[1], reverse=True)

        # 6. é€‰æ‹©Top-Kå…³é”®è¯
        top_keywords = keyword_scores[:max_keywords]

        if not top_keywords:
            raise ValueError(
                "No keywords extracted. Text may be too short or contain only stop words."
            )

        keywords = [kw for kw, _ in top_keywords]
        weights = {kw: float(score) for kw, score in top_keywords}

        return KeywordExtractionResult(
            keywords=keywords,
            weights=weights,
            total_keywords=len(keywords)
        )

    except Exception as e:
        raise ValueError(f"TF-IDF extraction failed: {str(e)}")


def _preprocess_text(text: str) -> str:
    """
    æ–‡æœ¬é¢„å¤„ç†

    å¤„ç†æ­¥éª¤:
    1. è½¬å°å†™
    2. ç§»é™¤URL
    3. ç§»é™¤é‚®ç®±
    4. ç§»é™¤ç‰¹æ®Šå­—ç¬¦(ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦)
    5. è§„èŒƒåŒ–ç©ºæ ¼

    Args:
        text: åŸå§‹æ–‡æœ¬

    Returns:
        str: æ¸…æ´—åçš„æ–‡æœ¬

    ç¤ºä¾‹:
        >>> _preprocess_text("Check out https://example.com!")
        'check out'
    """
    # è½¬å°å†™
    text = text.lower()

    # ç§»é™¤URL (http/https)
    text = re.sub(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
        '',
        text
    )

    # ç§»é™¤é‚®ç®±
    text = re.sub(r'\S+@\S+', '', text)

    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)

    # ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’Œè¿å­—ç¬¦
    text = re.sub(r'[^a-z0-9\s-]', ' ', text)

    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()

    return text


__all__ = ["extract_keywords", "KeywordExtractionResult"]
```

**2.4 ç¼–å†™å•å…ƒæµ‹è¯•** (50åˆ†é’Ÿ)

```python
# backend/tests/services/analysis/test_keyword_extraction.py
"""
å…³é”®è¯æå–ç®—æ³•å•å…ƒæµ‹è¯•
è¦†ç›–ç‡ç›®æ ‡: >90%
"""
import pytest
from app.services.analysis.keyword_extraction import (
    extract_keywords,
    KeywordExtractionResult,
    _preprocess_text,
)


class TestExtractKeywords:
    """å…³é”®è¯æå–åŠŸèƒ½æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_basic_extraction(self):
        """æµ‹è¯•åŸºæœ¬å…³é”®è¯æå–åŠŸèƒ½"""
        text = (
            "AI-powered note-taking app for researchers and creators. "
            "Helps automatically organize and connect ideas using "
            "machine learning algorithms."
        )

        result = await extract_keywords(text, max_keywords=10)

        assert isinstance(result, KeywordExtractionResult)
        assert len(result.keywords) <= 10
        assert len(result.keywords) > 0
        assert result.total_keywords == len(result.keywords)

        # éªŒè¯è¿”å›çš„æ˜¯å…³é”®è¯(åŒ…å«ç›¸å…³è¯æ±‡)
        keywords_str = ' '.join(result.keywords).lower()
        relevant_words = ['ai', 'note', 'research', 'learn', 'app', 'machine']
        assert any(word in keywords_str for word in relevant_words)

    @pytest.mark.asyncio
    async def test_keyword_weights(self):
        """æµ‹è¯•å…³é”®è¯æƒé‡è®¡ç®—"""
        text = "machine learning machine learning deep learning AI AI AI"

        result = await extract_keywords(text, max_keywords=5)

        # éªŒè¯æƒé‡å­—å…¸
        assert len(result.weights) == len(result.keywords)
        assert all(0.0 <= w <= 1.0 for w in result.weights.values())

        # AIå‡ºç°æœ€å¤š,åº”è¯¥æƒé‡æœ€é«˜
        sorted_keywords = sorted(
            result.weights.items(),
            key=lambda x: x[1],
            reverse=True
        )
        top_keyword = sorted_keywords[0][0]
        assert 'ai' in top_keyword or 'machine' in top_keyword

    @pytest.mark.asyncio
    async def test_min_length_filter(self):
        """æµ‹è¯•æœ€å°é•¿åº¦è¿‡æ»¤"""
        text = "AI ML NLP machine learning deep learning algorithms data science"

        result = await extract_keywords(
            text,
            max_keywords=10,
            min_keyword_length=3
        )

        # æ‰€æœ‰å…³é”®è¯é•¿åº¦>=3
        assert all(len(kw) >= 3 for kw in result.keywords)

    @pytest.mark.asyncio
    async def test_bigram_extraction(self):
        """æµ‹è¯•åŒè¯ç»„åˆæå–"""
        text = (
            "machine learning is powerful. "
            "deep learning is a subset of machine learning. "
            "neural networks are used in deep learning."
        )

        result = await extract_keywords(text, max_keywords=10)

        # åº”è¯¥æå–åˆ°åŒè¯ç»„åˆ
        has_bigram = any(
            ' ' in kw
            for kw in result.keywords
        )
        assert has_bigram, "Should extract bigrams like 'machine learning'"

    @pytest.mark.asyncio
    async def test_special_characters_handling(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        text = (
            "AI-powered! @note-taking #app for $researchers. "
            "Visit https://example.com or email test@example.com"
        )

        result = await extract_keywords(text, max_keywords=5)

        # URLå’Œé‚®ç®±åº”è¯¥è¢«ç§»é™¤
        keywords_str = ' '.join(result.keywords)
        assert 'http' not in keywords_str
        assert '@' not in keywords_str
        assert 'example.com' not in keywords_str

        # æœ‰æ•ˆè¯åº”è¯¥ä¿ç•™
        assert any(
            word in keywords_str
            for word in ['ai', 'note', 'app', 'research']
        )


class TestInputValidation:
    """è¾“å…¥éªŒè¯æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_empty_input(self):
        """æµ‹è¯•ç©ºè¾“å…¥"""
        with pytest.raises(ValueError, match="at least 10 characters"):
            await extract_keywords("")

    @pytest.mark.asyncio
    async def test_short_input(self):
        """æµ‹è¯•è¿‡çŸ­è¾“å…¥"""
        with pytest.raises(ValueError, match="at least 10 characters"):
            await extract_keywords("short")

    @pytest.mark.asyncio
    async def test_whitespace_only(self):
        """æµ‹è¯•ä»…ç©ºæ ¼è¾“å…¥"""
        with pytest.raises(ValueError, match="at least 10 characters"):
            await extract_keywords("        ")

    @pytest.mark.asyncio
    async def test_special_chars_only(self):
        """æµ‹è¯•ä»…ç‰¹æ®Šå­—ç¬¦è¾“å…¥"""
        with pytest.raises(ValueError, match="too short after preprocessing"):
            await extract_keywords("!@#$%^&*()[]{},./<>?")


class TestTextPreprocessing:
    """æ–‡æœ¬é¢„å¤„ç†æµ‹è¯•"""

    def test_lowercase_conversion(self):
        """æµ‹è¯•è½¬å°å†™"""
        result = _preprocess_text("AI Machine Learning")
        assert result == "ai machine learning"

    def test_url_removal(self):
        """æµ‹è¯•URLç§»é™¤"""
        text = "Check https://example.com and http://test.com"
        result = _preprocess_text(text)
        assert 'http' not in result
        assert 'example.com' not in result

    def test_email_removal(self):
        """æµ‹è¯•é‚®ç®±ç§»é™¤"""
        text = "Contact us at test@example.com"
        result = _preprocess_text(text)
        assert '@' not in result
        assert 'example.com' not in result

    def test_html_removal(self):
        """æµ‹è¯•HTMLæ ‡ç­¾ç§»é™¤"""
        text = "<p>AI app</p> <div>machine learning</div>"
        result = _preprocess_text(text)
        assert '<' not in result
        assert '>' not in result
        assert 'ai app' in result

    def test_special_char_removal(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦ç§»é™¤"""
        text = "AI-powered! @note #app $100"
        result = _preprocess_text(text)
        # è¿å­—ç¬¦åº”è¯¥ä¿ç•™ä¸ºç©ºæ ¼
        assert result == "ai powered note app 100"

    def test_whitespace_normalization(self):
        """æµ‹è¯•ç©ºæ ¼è§„èŒƒåŒ–"""
        text = "AI    machine     learning"
        result = _preprocess_text(text)
        assert result == "ai machine learning"


class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_very_long_text(self):
        """æµ‹è¯•è¶…é•¿æ–‡æœ¬"""
        text = "AI machine learning " * 1000  # 20000+ å­—ç¬¦
        result = await extract_keywords(text, max_keywords=10)

        assert len(result.keywords) <= 10
        assert result.total_keywords > 0

    @pytest.mark.asyncio
    async def test_unicode_characters(self):
        """æµ‹è¯•Unicodeå­—ç¬¦"""
        text = "AI application dÃ©veloppement äººå·¥æ™ºèƒ½ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ"
        # åº”è¯¥åªä¿ç•™è‹±æ–‡éƒ¨åˆ†
        result = await extract_keywords(text, max_keywords=5)

        keywords_str = ' '.join(result.keywords)
        # Unicodeå­—ç¬¦åº”è¯¥è¢«ç§»é™¤
        assert all(ord(c) < 128 for c in keywords_str if c != ' ')

    @pytest.mark.asyncio
    async def test_repeated_stopwords(self):
        """æµ‹è¯•å¤§é‡åœç”¨è¯"""
        text = "the the the a a an is are was were machine learning"
        result = await extract_keywords(text, max_keywords=5)

        # åœç”¨è¯åº”è¯¥è¢«è¿‡æ»¤
        keywords_str = ' '.join(result.keywords)
        assert 'the' not in keywords_str
        assert 'machine' in keywords_str or 'learning' in keywords_str
```

**2.5 è¿è¡Œæµ‹è¯•** (20åˆ†é’Ÿ)

```bash
# è¿è¡Œå…³é”®è¯æå–æµ‹è¯•
pytest backend/tests/services/analysis/test_keyword_extraction.py -v

# æœŸæœ›è¾“å‡º:
# test_keyword_extraction.py::TestExtractKeywords::test_basic_extraction PASSED
# test_keyword_extraction.py::TestExtractKeywords::test_keyword_weights PASSED
# ...
# ==================== 20 passed in 2.35s ====================

# æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡
pytest backend/tests/services/analysis/test_keyword_extraction.py \
    --cov=backend.app.services.analysis.keyword_extraction \
    --cov-report=term-missing

# æœŸæœ›è¦†ç›–ç‡: >90%

# MyPyç±»å‹æ£€æŸ¥
mypy --strict backend/app/services/analysis/keyword_extraction.py

# æœŸæœ›: Success: no issues found
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] TF-IDFç®—æ³•æ­£ç¡®å®ç°
- [ ] æ”¯æŒ1-2 gramæå–
- [ ] æ­£ç¡®å¤„ç†ç‰¹æ®Šå­—ç¬¦ã€URLã€é‚®ç®±
- [ ] å•å…ƒæµ‹è¯•20+é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡>90%
- [ ] MyPy --strict 0 errors

---

### ä¸‹åˆä»»åŠ¡ (14:00-18:00)

#### âœ… ä»»åŠ¡3: å®ç°ç¤¾åŒºå‘ç°ç®—æ³• (14:00-16:30, 2.5å°æ—¶)

**ç›®æ ‡**: å®ç°ç¤¾åŒºç›¸å…³æ€§è¯„åˆ†å’ŒTop-Ké€‰æ‹©

**3.1 åˆ›å»ºç¤¾åŒºå‘ç°æ¨¡å—** (10åˆ†é’Ÿ)

```bash
# åˆ›å»ºç¤¾åŒºå‘ç°æ¨¡å—
touch backend/app/services/analysis/community_discovery.py

# åˆ›å»ºæµ‹è¯•æ–‡ä»¶
touch backend/tests/services/analysis/test_community_discovery.py

# åˆ›å»ºé…ç½®æ–‡ä»¶ç›®å½•
mkdir -p backend/config
touch backend/config/community_discovery.yml
```

**3.2 å®šä¹‰æ•°æ®æ¨¡å‹** (15åˆ†é’Ÿ)

```python
# backend/app/models/community.py
"""
ç¤¾åŒºæ•°æ®æ¨¡å‹
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import List


@dataclass
class Community:
    """Redditç¤¾åŒºæ¨¡å‹"""
    name: str  # ç¤¾åŒºåç§° (å¦‚ "r/productivity")
    description: str  # ç¤¾åŒºæè¿°
    description_keywords: List[str]  # é¢„æå–çš„æè¿°å…³é”®è¯
    daily_posts: int  # æ¯æ—¥å¸–å­æ•°
    avg_comment_length: int  # å¹³å‡è¯„è®ºé•¿åº¦
    category: str  # ç¤¾åŒºç±»åˆ« (å¦‚ "productivity", "tech")
    relevance_score: float = 0.0  # ç›¸å…³æ€§åˆ†æ•° (0-1)
    subscribers: int = 0  # è®¢é˜…äººæ•°
    activity_level: float = 0.0  # æ´»è·ƒåº¦ (0-1)
    quality_score: float = 0.0  # è´¨é‡åˆ†æ•° (0-1)


@dataclass
class CommunityPool:
    """ç¤¾åŒºæ± """
    communities: List[Community]
    total_count: int

    def __post_init__(self):
        self.total_count = len(self.communities)


# ç¤ºä¾‹ç¤¾åŒºæ±  (ç®€åŒ–ç‰ˆ,å®é™…åº”è¯¥ä»æ•°æ®åº“åŠ è½½)
SAMPLE_COMMUNITY_POOL = CommunityPool(
    communities=[
        Community(
            name="r/productivity",
            description="Tips and tricks for being more productive",
            description_keywords=["productivity", "tips", "efficient", "workflow"],
            daily_posts=150,
            avg_comment_length=180,
            category="productivity",
            subscribers=500000,
            activity_level=0.8,
            quality_score=0.85
        ),
        Community(
            name="r/PKM",
            description="Personal Knowledge Management systems and tools",
            description_keywords=["knowledge", "management", "notes", "systems"],
            daily_posts=50,
            avg_comment_length=250,
            category="knowledge",
            subscribers=50000,
            activity_level=0.6,
            quality_score=0.9
        ),
        Community(
            name="r/ObsidianMD",
            description="Discussion about Obsidian note-taking app",
            description_keywords=["obsidian", "notes", "markdown", "linking"],
            daily_posts=100,
            avg_comment_length=220,
            category="tools",
            subscribers=150000,
            activity_level=0.75,
            quality_score=0.88
        ),
        Community(
            name="r/Notion",
            description="Notion workspace tips and templates",
            description_keywords=["notion", "workspace", "templates", "productivity"],
            daily_posts=200,
            avg_comment_length=160,
            category="tools",
            subscribers=300000,
            activity_level=0.9,
            quality_score=0.75
        ),
        Community(
            name="r/SaaS",
            description="Discussion about Software as a Service",
            description_keywords=["saas", "software", "business", "startups"],
            daily_posts=80,
            avg_comment_length=200,
            category="business",
            subscribers=100000,
            activity_level=0.7,
            quality_score=0.8
        ),
    ]
)
```

**3.3 å®ç°ç¤¾åŒºå‘ç°ç®—æ³•** (90åˆ†é’Ÿ)

```python
# backend/app/services/analysis/community_discovery.py
"""
Step 1: æ™ºèƒ½ç¤¾åŒºå‘ç°ç®—æ³•
åŸºäºPRD-03 Â§ 3.1è®¾è®¡
"""
from __future__ import annotations

from typing import List, Dict, Tuple
from dataclasses import dataclass
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

from app.services.analysis.keyword_extraction import extract_keywords
from app.models.community import Community, CommunityPool, SAMPLE_COMMUNITY_POOL


@dataclass
class CommunityDiscoveryResult:
    """ç¤¾åŒºå‘ç°ç»“æœ"""
    communities: List[Community]
    total_discovered: int
    avg_relevance_score: float
    discovery_time_seconds: float


async def discover_communities(
    product_description: str,
    max_communities: int = 20,
    cache_hit_rate: float = 0.7,
    community_pool: CommunityPool | None = None,
) -> List[Community]:
    """
    æ™ºèƒ½ç¤¾åŒºå‘ç°ç®—æ³•

    åŸºäºäº§å“æè¿°,ä»ç¤¾åŒºæ± ä¸­å‘ç°æœ€ç›¸å…³çš„Redditç¤¾åŒº

    ç®—æ³•æ­¥éª¤:
    1. å…³é”®è¯æå– (TF-IDF)
    2. åŠ¨æ€è°ƒæ•´ç›®æ ‡ç¤¾åŒºæ•°é‡ (åŸºäºç¼“å­˜å‘½ä¸­ç‡)
    3. å€™é€‰ç¤¾åŒºè¯„åˆ† (æè¿°åŒ¹é…40% + æ´»è·ƒåº¦30% + è´¨é‡30%)
    4. Top-Ké€‰æ‹© + å¤šæ ·æ€§ä¿è¯

    Args:
        product_description: äº§å“æè¿°æ–‡æœ¬
        max_communities: æœ€å¤§ç¤¾åŒºæ•°é‡
        cache_hit_rate: å½“å‰ç¼“å­˜å‘½ä¸­ç‡ (ç”¨äºåŠ¨æ€è°ƒæ•´)
        community_pool: å€™é€‰ç¤¾åŒºæ±  (Noneåˆ™ä½¿ç”¨é»˜è®¤æ± )

    Returns:
        List[Community]: ç›¸å…³ç¤¾åŒºåˆ—è¡¨ (æŒ‰ç›¸å…³æ€§æ’åº)

    Raises:
        ValueError: äº§å“æè¿°ä¸ºç©ºæˆ–è¿‡çŸ­

    ç¤ºä¾‹:
        >>> communities = await discover_communities(
        ...     "AI note-taking app for researchers",
        ...     max_communities=10
        ... )
        >>> print(len(communities))
        10
        >>> print(communities[0].relevance_score)
        0.85
    """
    # 1. éªŒè¯è¾“å…¥
    if not product_description or len(product_description) < 10:
        raise ValueError(
            "Product description must be at least 10 characters long"
        )

    # 2. ä½¿ç”¨é»˜è®¤ç¤¾åŒºæ± (å¦‚æœæœªæä¾›)
    if community_pool is None:
        community_pool = SAMPLE_COMMUNITY_POOL

    if not community_pool.communities:
        raise ValueError("Community pool is empty")

    # 3. å…³é”®è¯æå–
    keyword_result = await extract_keywords(
        product_description,
        max_keywords=20
    )

    # 4. åŠ¨æ€è°ƒæ•´ç›®æ ‡ç¤¾åŒºæ•°é‡
    target_communities = _calculate_target_communities(cache_hit_rate)
    target_communities = min(
        target_communities,
        max_communities,
        len(community_pool.communities)
    )

    # 5. å€™é€‰ç¤¾åŒºè¯„åˆ†
    scored_communities = await _score_communities(
        keyword_result.keywords,
        community_pool.communities
    )

    # 6. Top-Ké€‰æ‹© + å¤šæ ·æ€§ä¿è¯
    selected_communities = _select_diverse_top_k(
        scored_communities,
        k=target_communities
    )

    return selected_communities


async def _score_communities(
    keywords: List[str],
    community_pool: List[Community],
) -> List[Tuple[Community, float]]:
    """
    ç¤¾åŒºç›¸å…³æ€§è¯„åˆ†ç®—æ³•

    è¯„åˆ†å…¬å¼:
    score = description_match * 0.4 + activity_level * 0.3 + quality_score * 0.3

    Args:
        keywords: äº§å“å…³é”®è¯
        community_pool: å€™é€‰ç¤¾åŒºåˆ—è¡¨

    Returns:
        List[Tuple[Community, float]]: (ç¤¾åŒº, ç›¸å…³æ€§åˆ†æ•°) åˆ—è¡¨
    """
    scored_communities: List[Tuple[Community, float]] = []

    for community in community_pool:
        # æè¿°åŒ¹é…åˆ†æ•° (40%æƒé‡)
        description_score = _calculate_description_match(
            keywords,
            community.description_keywords
        )

        # æ´»è·ƒåº¦åˆ†æ•° (30%æƒé‡)
        # å½’ä¸€åŒ–: 100 daily posts = 1.0
        activity_score = min(community.daily_posts / 100.0, 1.0)

        # è´¨é‡æŒ‡æ ‡åˆ†æ•° (30%æƒé‡)
        # å½’ä¸€åŒ–: 200 avg comment length = 1.0
        quality_score = min(community.avg_comment_length / 200.0, 1.0)

        # ç»¼åˆè¯„åˆ†
        total_score = (
            description_score * 0.4 +
            activity_score * 0.3 +
            quality_score * 0.3
        )

        # æ›´æ–°ç¤¾åŒºçš„ç›¸å…³æ€§åˆ†æ•°
        community.relevance_score = total_score

        scored_communities.append((community, total_score))

    return scored_communities


def _calculate_description_match(
    keywords: List[str],
    community_keywords: List[str],
) -> float:
    """
    è®¡ç®—æè¿°åŒ¹é…åˆ†æ•° (ä½™å¼¦ç›¸ä¼¼åº¦)

    ä½¿ç”¨TF-IDFå‘é‡åŒ–åè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

    Args:
        keywords: äº§å“å…³é”®è¯
        community_keywords: ç¤¾åŒºæè¿°å…³é”®è¯

    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° [0.0, 1.0]
    """
    if not keywords or not community_keywords:
        return 0.0

    # åˆå¹¶å…³é”®è¯ä¸ºæ–‡æœ¬
    product_text = ' '.join(keywords)
    community_text = ' '.join(community_keywords)

    try:
        # TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([product_text, community_text])

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

        return float(similarity)

    except Exception:
        # Fallback: ç®€å•çš„å…³é”®è¯é‡å ç‡
        overlap = len(set(keywords) & set(community_keywords))
        return overlap / max(len(keywords), len(community_keywords))


def _select_diverse_top_k(
    scored_communities: List[Tuple[Community, float]],
    k: int,
    max_same_category: int = 5,
) -> List[Community]:
    """
    Top-Ké€‰æ‹© + å¤šæ ·æ€§ä¿è¯

    ç¡®ä¿é€‰ä¸­çš„ç¤¾åŒºæ¥è‡ªä¸åŒç±»åˆ«,é¿å…é‡å¤

    ç®—æ³•:
    1. æŒ‰åˆ†æ•°æ’åº
    2. ä¾æ¬¡é€‰æ‹©ç¤¾åŒº
    3. å¦‚æœåŒç±»åˆ«ç¤¾åŒº>5ä¸ª,è·³è¿‡
    4. ç›´åˆ°é€‰å¤Ÿkä¸ª

    Args:
        scored_communities: (ç¤¾åŒº, åˆ†æ•°) åˆ—è¡¨
        k: é€‰æ‹©æ•°é‡
        max_same_category: åŒç±»åˆ«æœ€å¤§æ•°é‡

    Returns:
        List[Community]: é€‰ä¸­çš„ç¤¾åŒºåˆ—è¡¨ (æŒ‰ç›¸å…³æ€§æ’åº)
    """
    # 1. æŒ‰åˆ†æ•°é™åºæ’åº
    sorted_communities = sorted(
        scored_communities,
        key=lambda x: x[1],
        reverse=True
    )

    # 2. åº”ç”¨å¤šæ ·æ€§çº¦æŸ
    selected: List[Community] = []
    category_count: Dict[str, int] = {}

    for community, score in sorted_communities:
        if len(selected) >= k:
            break

        # æ£€æŸ¥ç±»åˆ«é…é¢
        current_count = category_count.get(community.category, 0)
        if current_count >= max_same_category:
            continue  # è¯¥ç±»åˆ«å·²æ»¡,è·³è¿‡

        # é€‰æ‹©è¯¥ç¤¾åŒº
        selected.append(community)
        category_count[community.category] = current_count + 1

    return selected


def _calculate_target_communities(cache_hit_rate: float) -> int:
    """
    æ ¹æ®ç¼“å­˜å‘½ä¸­ç‡åŠ¨æ€è®¡ç®—ç›®æ ‡ç¤¾åŒºæ•°é‡

    ç­–ç•¥:
    - ç¼“å­˜å‘½ä¸­ç‡ > 80%: åˆ†æ30ä¸ªç¤¾åŒº (ç§¯ææ¨¡å¼)
    - ç¼“å­˜å‘½ä¸­ç‡ 60-80%: åˆ†æ20ä¸ªç¤¾åŒº (å¹³è¡¡æ¨¡å¼)
    - ç¼“å­˜å‘½ä¸­ç‡ < 60%: åˆ†æ10ä¸ªç¤¾åŒº (ä¿å®ˆæ¨¡å¼)

    Args:
        cache_hit_rate: å½“å‰ç¼“å­˜å‘½ä¸­ç‡ [0.0, 1.0]

    Returns:
        int: ç›®æ ‡ç¤¾åŒºæ•°é‡

    ç¤ºä¾‹:
        >>> _calculate_target_communities(0.9)
        30
        >>> _calculate_target_communities(0.7)
        20
        >>> _calculate_target_communities(0.5)
        10
    """
    if cache_hit_rate > 0.8:
        return 30  # ç§¯ææ¨¡å¼
    elif cache_hit_rate > 0.6:
        return 20  # å¹³è¡¡æ¨¡å¼
    else:
        return 10  # ä¿å®ˆæ¨¡å¼


__all__ = [
    "discover_communities",
    "CommunityDiscoveryResult",
]
```

**3.4 ç¼–å†™å•å…ƒæµ‹è¯•** (45åˆ†é’Ÿ)

```python
# backend/tests/services/analysis/test_community_discovery.py
"""
ç¤¾åŒºå‘ç°ç®—æ³•å•å…ƒæµ‹è¯•
"""
import pytest
from app.services.analysis.community_discovery import (
    discover_communities,
    _score_communities,
    _calculate_description_match,
    _select_diverse_top_k,
    _calculate_target_communities,
)
from app.models.community import Community, CommunityPool


class TestDiscoverCommunities:
    """ç¤¾åŒºå‘ç°åŠŸèƒ½æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_basic_discovery(self):
        """æµ‹è¯•åŸºæœ¬ç¤¾åŒºå‘ç°"""
        product_desc = "AIç¬”è®°åº”ç”¨,å¸®åŠ©ç ”ç©¶è€…è‡ªåŠ¨ç»„ç»‡å’Œè¿æ¥æƒ³æ³•"

        communities = await discover_communities(
            product_desc,
            max_communities=20
        )

        assert len(communities) <= 20
        assert len(communities) > 0
        assert all(c.relevance_score > 0.0 for c in communities)

        # ç¤¾åŒºåº”è¯¥æŒ‰ç›¸å…³æ€§æ’åº
        scores = [c.relevance_score for c in communities]
        assert scores == sorted(scores, reverse=True)

    @pytest.mark.asyncio
    async def test_cache_based_adjustment(self):
        """æµ‹è¯•åŸºäºç¼“å­˜å‘½ä¸­ç‡çš„åŠ¨æ€è°ƒæ•´"""
        product_desc = "note taking app for students"

        # é«˜ç¼“å­˜å‘½ä¸­ç‡ â†’ æ›´å¤šç¤¾åŒº
        communities_high = await discover_communities(
            product_desc,
            cache_hit_rate=0.9
        )

        # ä½ç¼“å­˜å‘½ä¸­ç‡ â†’ æ›´å°‘ç¤¾åŒº
        communities_low = await discover_communities(
            product_desc,
            cache_hit_rate=0.5
        )

        # é«˜å‘½ä¸­ç‡åº”è¯¥è¿”å›æ›´å¤šç¤¾åŒº
        # (å®é™…æ•°é‡å–å†³äºcommunity_poolå¤§å°)
        assert len(communities_high) >= len(communities_low)


class TestCommunityScoring:
    """ç¤¾åŒºè¯„åˆ†æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_score_calculation(self):
        """æµ‹è¯•è¯„åˆ†è®¡ç®—"""
        keywords = ["productivity", "notes", "workflow"]
        communities = [
            Community(
                name="r/productivity",
                description="Productivity tips",
                description_keywords=["productivity", "tips", "workflow"],
                daily_posts=100,
                avg_comment_length=200,
                category="productivity"
            ),
            Community(
                name="r/gaming",
                description="Gaming discussion",
                description_keywords=["gaming", "games", "esports"],
                daily_posts=500,
                avg_comment_length=150,
                category="gaming"
            ),
        ]

        scored = await _score_communities(keywords, communities)

        # r/productivityåº”è¯¥å¾—åˆ†æ›´é«˜
        productivity_score = next(
            score for comm, score in scored
            if comm.name == "r/productivity"
        )
        gaming_score = next(
            score for comm, score in scored
            if comm.name == "r/gaming"
        )

        assert productivity_score > gaming_score


class TestDescriptionMatch:
    """æè¿°åŒ¹é…æµ‹è¯•"""

    def test_perfect_match(self):
        """æµ‹è¯•å®Œå…¨åŒ¹é…"""
        keywords = ["ai", "machine", "learning"]
        community_keywords = ["ai", "machine", "learning"]

        score = _calculate_description_match(keywords, community_keywords)
        assert score > 0.9

    def test_partial_match(self):
        """æµ‹è¯•éƒ¨åˆ†åŒ¹é…"""
        keywords = ["ai", "productivity", "notes"]
        community_keywords = ["productivity", "workflow"]

        score = _calculate_description_match(keywords, community_keywords)
        assert 0.0 < score < 1.0

    def test_no_match(self):
        """æµ‹è¯•æ— åŒ¹é…"""
        keywords = ["ai", "machine", "learning"]
        community_keywords = ["cooking", "recipes"]

        score = _calculate_description_match(keywords, community_keywords)
        assert score < 0.3


class TestDiversitySelection:
    """å¤šæ ·æ€§é€‰æ‹©æµ‹è¯•"""

    def test_diversity_enforcement(self):
        """æµ‹è¯•å¤šæ ·æ€§çº¦æŸ"""
        communities = [
            (Community(name=f"r/tech{i}", description="", description_keywords=[],
                      daily_posts=100, avg_comment_length=200, category="tech"), 0.9)
            for i in range(10)
        ] + [
            (Community(name="r/business", description="", description_keywords=[],
                      daily_posts=100, avg_comment_length=200, category="business"), 0.8)
        ]

        selected = _select_diverse_top_k(
            communities,
            k=10,
            max_same_category=5
        )

        # åŒç±»åˆ«ä¸åº”è¶…è¿‡5ä¸ª
        tech_count = sum(1 for c in selected if c.category == "tech")
        assert tech_count <= 5

        # åº”è¯¥åŒ…å«businessç±»åˆ«
        has_business = any(c.category == "business" for c in selected)
        assert has_business


class TestCacheAdjustment:
    """ç¼“å­˜è°ƒæ•´æµ‹è¯•"""

    def test_aggressive_mode(self):
        """æµ‹è¯•ç§¯ææ¨¡å¼(é«˜ç¼“å­˜å‘½ä¸­ç‡)"""
        assert _calculate_target_communities(0.9) == 30

    def test_balanced_mode(self):
        """æµ‹è¯•å¹³è¡¡æ¨¡å¼(ä¸­ç­‰å‘½ä¸­ç‡)"""
        assert _calculate_target_communities(0.7) == 20

    def test_conservative_mode(self):
        """æµ‹è¯•ä¿å®ˆæ¨¡å¼(ä½å‘½ä¸­ç‡)"""
        assert _calculate_target_communities(0.5) == 10


class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_empty_description(self):
        """æµ‹è¯•ç©ºæè¿°"""
        with pytest.raises(ValueError, match="at least 10 characters"):
            await discover_communities("")

    @pytest.mark.asyncio
    async def test_very_long_description(self):
        """æµ‹è¯•è¶…é•¿æè¿°"""
        long_desc = "AI productivity app " * 1000
        communities = await discover_communities(long_desc, max_communities=5)
        assert len(communities) <= 5

    @pytest.mark.asyncio
    async def test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        desc = "AI-powered! @note-taking #app $100"
        communities = await discover_communities(desc, max_communities=5)
        assert len(communities) > 0
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ç¤¾åŒºè¯„åˆ†ç®—æ³•æ­£ç¡®å®ç°
- [ ] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ­£ç¡®
- [ ] å¤šæ ·æ€§çº¦æŸç”Ÿæ•ˆ
- [ ] åŠ¨æ€æ•°é‡è°ƒæ•´æ­£ç¡®
- [ ] å•å…ƒæµ‹è¯•15+é€šè¿‡

---

#### âœ… ä»»åŠ¡4: æµ‹è¯•ä¸æ€§èƒ½ä¼˜åŒ– (16:30-18:00, 1.5å°æ—¶)

**ç›®æ ‡**: ç¡®ä¿ç®—æ³•æ€§èƒ½å’Œè´¨é‡

**4.1 æ€§èƒ½æµ‹è¯•** (30åˆ†é’Ÿ)

```python
# backend/tests/services/analysis/test_community_discovery_performance.py
"""
ç¤¾åŒºå‘ç°æ€§èƒ½æµ‹è¯•
"""
import pytest
import time
from app.services.analysis.community_discovery import discover_communities
from app.models.community import CommunityPool, Community


@pytest.mark.asyncio
async def test_discovery_performance_small_pool():
    """æµ‹è¯•å°æ± æ€§èƒ½(<1ç§’)"""
    product_desc = "AIç¬”è®°åº”ç”¨,å¸®åŠ©ç ”ç©¶è€…è‡ªåŠ¨ç»„ç»‡å’Œè¿æ¥æƒ³æ³•"

    start_time = time.time()
    communities = await discover_communities(
        product_desc,
        max_communities=20
    )
    duration = time.time() - start_time

    assert duration < 1.0, f"Discovery took {duration:.2f}s, expected <1s"
    assert len(communities) > 0


@pytest.mark.asyncio
async def test_discovery_performance_large_pool():
    """æµ‹è¯•å¤§æ± æ€§èƒ½(<30ç§’)"""
    # åˆ›å»º500ä¸ªç¤¾åŒºçš„å¤§æ± 
    large_pool = CommunityPool(
        communities=[
            Community(
                name=f"r/test{i}",
                description=f"Test community {i}",
                description_keywords=["test", f"keyword{i}"],
                daily_posts=100,
                avg_comment_length=200,
                category="test"
            )
            for i in range(500)
        ]
    )

    product_desc = "AIç¬”è®°åº”ç”¨æµ‹è¯•æ€§èƒ½" * 5

    start_time = time.time()
    communities = await discover_communities(
        product_desc,
        max_communities=20,
        community_pool=large_pool
    )
    duration = time.time() - start_time

    assert duration < 30.0, f"Discovery took {duration:.2f}s, expected <30s"
    assert len(communities) == 20
```

**4.2 é›†æˆæµ‹è¯•** (30åˆ†é’Ÿ)

```python
# backend/tests/services/analysis/test_analysis_integration.py
"""
åˆ†æå¼•æ“é›†æˆæµ‹è¯•
æµ‹è¯•å…³é”®è¯æå– â†’ ç¤¾åŒºå‘ç°çš„å®Œæ•´æµç¨‹
"""
import pytest
from app.services.analysis.keyword_extraction import extract_keywords
from app.services.analysis.community_discovery import discover_communities


@pytest.mark.asyncio
async def test_full_discovery_pipeline():
    """æµ‹è¯•å®Œæ•´å‘ç°æµç¨‹"""
    product_desc = (
        "AI-powered note-taking app for researchers and creators. "
        "Automatically organize ideas and connect concepts using "
        "machine learning algorithms. Perfect for academic writing "
        "and personal knowledge management."
    )

    # Step 1: å…³é”®è¯æå–
    keywords = await extract_keywords(product_desc, max_keywords=20)
    assert len(keywords.keywords) > 0

    # Step 2: ç¤¾åŒºå‘ç°
    communities = await discover_communities(
        product_desc,
        max_communities=10
    )
    assert len(communities) <= 10

    # éªŒè¯ç¤¾åŒºç›¸å…³æ€§
    assert all(c.relevance_score > 0.0 for c in communities)

    # éªŒè¯å¤šæ ·æ€§
    categories = set(c.category for c in communities)
    assert len(categories) > 1, "Should have diverse categories"


@pytest.mark.asyncio
async def test_discovery_with_different_products():
    """æµ‹è¯•ä¸åŒäº§å“ç±»å‹"""
    test_cases = [
        {
            "description": "Project management tool for software teams",
            "expected_min_communities": 5,
        },
        {
            "description": "E-commerce platform for small businesses",
            "expected_min_communities": 5,
        },
        {
            "description": "Fitness tracking app with AI coaching",
            "expected_min_communities": 5,
        },
    ]

    for case in test_cases:
        communities = await discover_communities(
            case["description"],
            max_communities=10
        )

        assert len(communities) >= case["expected_min_communities"]
        assert all(c.relevance_score > 0.0 for c in communities)
```

**4.3 ä»£ç è´¨é‡æ£€æŸ¥** (30åˆ†é’Ÿ)

```bash
# 1. MyPyç±»å‹æ£€æŸ¥
mypy --strict backend/app/services/analysis/

# æœŸæœ›: Success: no issues found

# 2. è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest backend/tests/services/analysis/ -v

# æœŸæœ›: æ‰€æœ‰æµ‹è¯•é€šè¿‡

# 3. æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
pytest backend/tests/services/analysis/ \
    --cov=backend.app.services.analysis \
    --cov-report=html \
    --cov-report=term-missing

# æœŸæœ›: è¦†ç›–ç‡>80%

# 4. ä»£ç æ ¼å¼æ£€æŸ¥
black backend/app/services/analysis/ --check
isort backend/app/services/analysis/ --check

# æœŸæœ›: All done! âœ¨
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡(å°æ± <1s, å¤§æ± <30s)
- [ ] é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] MyPy --strict 0 errors
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%
- [ ] ä»£ç æ ¼å¼è§„èŒƒ

---

## ğŸ“Š ä»Šæ—¥éªŒæ”¶æ¸…å•

### åŠŸèƒ½éªŒæ”¶
- [ ] âœ… BackendæœåŠ¡å¯åŠ¨,Frontendå¯è”è°ƒ
- [ ] âœ… TF-IDFå…³é”®è¯æå–å®ç°å®Œæˆ
- [ ] âœ… ç¤¾åŒºå‘ç°ç®—æ³•å®ç°å®Œæˆ
- [ ] âœ… ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ­£ç¡®
- [ ] âœ… å¤šæ ·æ€§çº¦æŸæ­£å¸¸å·¥ä½œ
- [ ] âœ… åŠ¨æ€æ•°é‡è°ƒæ•´æ­£ç¡®

### æµ‹è¯•éªŒæ”¶
- [ ] âœ… å…³é”®è¯æå–æµ‹è¯•20+é€šè¿‡
- [ ] âœ… ç¤¾åŒºå‘ç°æµ‹è¯•15+é€šè¿‡
- [ ] âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡(<30ç§’)
- [ ] âœ… é›†æˆæµ‹è¯•é€šè¿‡
- [ ] âœ… æµ‹è¯•è¦†ç›–ç‡>80%

### è´¨é‡éªŒæ”¶
- [ ] âœ… MyPy --strict 0 errors
- [ ] âœ… Blackæ ¼å¼åŒ–é€šè¿‡
- [ ] âœ… æ— æ–°å¢æŠ€æœ¯å€º
- [ ] âœ… ä»£ç reviewå®Œæˆ

---

## ğŸš€ å¿«é€Ÿå‚è€ƒ

### å…³é”®å‘½ä»¤
```bash
# å¯åŠ¨æœåŠ¡
uvicorn app.main:app --reload

# è¿è¡Œæµ‹è¯•
pytest backend/tests/services/analysis/ -v

# ç±»å‹æ£€æŸ¥
mypy --strict backend/app/services/analysis/

# è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=backend.app.services.analysis --cov-report=term-missing
```

### é‡è¦æ–‡ä»¶
- `backend/app/services/analysis/keyword_extraction.py` - TF-IDFå®ç°
- `backend/app/services/analysis/community_discovery.py` - ç¤¾åŒºå‘ç°
- `backend/app/models/community.py` - æ•°æ®æ¨¡å‹
- `backend/tests/services/analysis/` - æµ‹è¯•ç›®å½•

### å‚è€ƒæ–‡æ¡£
- `docs/PRD/PRD-03-åˆ†æå¼•æ“.md` - åˆ†æå¼•æ“è®¾è®¡
- `backend/docs/ANALYSIS_ENGINE_DESIGN.md` - å¼•æ“è®¾è®¡æ–‡æ¡£
- `DAY6-TASK-ASSIGNMENT.md` - ä»Šæ—¥ä»»åŠ¡æ€»è§ˆ

---

**Day 6 Backend A åŠ æ²¹! ğŸš€**

åˆ†æå¼•æ“çš„ç¬¬ä¸€æ­¥å³å°†å®Œæˆ,è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒ!
