# 增量抓取实施完成报告

生成时间：2025-10-16 20:35  
状态：✅ 代码实施完成，待测试验证

---

## 📋 实施概述

按照用户需求，已完成"冷热分层：增量累积 + 实时热缓存"架构的代码实施。

**核心改造：**
1. ✅ 数据库迁移（冷热分层表结构）
2. ✅ 增量抓取服务（`IncrementalCrawler`）
3. ✅ 抓取器改造（`crawler_task.py`）
4. ✅ Celery Beat 配置（每 2 小时增量抓取）
5. ⏳ 测试验证（待执行）

---

## ✅ 已完成的工作

### 1. 数据库架构（已完成）

**新增表：**
- `posts_raw`：冷库，增量累积，90天滚动窗口
- `posts_hot`：热缓存，覆盖式刷新，24小时TTL
- `posts_latest`：物化视图，只保留最新版本

**扩展表：**
- `community_cache`：添加水位线字段（`last_seen_post_id`, `last_seen_created_at`, `total_posts_fetched`, `dedup_rate`）

**辅助函数：**
- `text_norm_hash()`：文本归一化哈希
- `fill_normalized_fields()`：自动填充触发器
- `refresh_posts_latest()`：刷新物化视图
- `cleanup_expired_hot_cache()`：清理过期热缓存
- `cleanup_old_posts()`：清理旧数据（90天窗口）
- `get_storage_stats()`：获取存储统计

---

### 2. 增量抓取服务（新增）

**文件：`backend/app/services/incremental_crawler.py`**

**核心类：`IncrementalCrawler`**

```python
class IncrementalCrawler:
    """
    增量抓取器：实现冷热双写 + 水位线机制
    
    核心原则：
    1. 先写冷库（持久层），再写热缓存
    2. 使用水位线避免重复抓取
    3. 去重策略：(source, source_post_id, text_norm_hash)
    4. SCD2 版本追踪
    """
```

**核心方法：**

1. **`crawl_community_incremental()`**
   - 获取水位线
   - 抓取新帖子
   - 过滤（只保留新于水位线的）
   - 双写（先冷库，再热缓存）
   - 更新水位线

2. **`_dual_write()`**
   - 先写 `posts_raw`（冷库）
   - 再写 `posts_hot`（热缓存）
   - 使用 PostgreSQL `INSERT ... ON CONFLICT DO UPDATE`

3. **`_upsert_to_cold_storage()`**
   - 使用 SQLAlchemy `pg_insert().on_conflict_do_update()`
   - 冲突键：`(source, source_post_id, version)`
   - 更新字段：`score`, `num_comments`, `fetched_at`

4. **`_upsert_to_hot_cache()`**
   - 覆盖式更新
   - 冲突键：`(source, source_post_id)`
   - 更新所有字段

5. **`_update_watermark()`**
   - 更新 `community_cache` 表
   - 记录 `last_seen_post_id`, `last_seen_created_at`
   - 累加 `total_posts_fetched`
   - 计算 `dedup_rate`

---

### 3. 抓取器改造（已完成）

**文件：`backend/app/tasks/crawler_task.py`**

**新增任务：**

```python
@celery_app.task(name="tasks.crawler.crawl_seed_communities_incremental")
def crawl_seed_communities_incremental(force_refresh: bool = False):
    """新版增量抓取任务（冷热双写 + 水位线）"""
    return asyncio.run(_crawl_seeds_incremental_impl(force_refresh))
```

**新增实现函数：**

```python
async def _crawl_seeds_incremental_impl(force_refresh: bool = False):
    """新版增量抓取：冷热双写 + 水位线机制"""
    # 1. 加载社区池
    # 2. 创建 IncrementalCrawler
    # 3. 并发抓取（带限流）
    # 4. 返回统计结果
```

**保留旧任务：**
- `crawl_seed_communities()`：旧版抓取（只写 Redis 缓存）
- 用于兼容性，可以逐步迁移

---

### 4. Celery Beat 配置（已更新）

**文件：`backend/app/core/celery_app.py`**

**新配置：**

```python
celery_app.conf.beat_schedule = {
    # 增量抓取：每 2 小时执行一次（冷热双写 + 水位线）
    "auto-crawl-incremental": {
        "task": "tasks.crawler.crawl_seed_communities_incremental",
        "schedule": crontab(minute="0", hour="*/2"),  # 每 2 小时整点执行
    },
    # ... 其他任务
}
```

**调整原因：**
- 从每 1 小时改为每 2 小时
- 时间范围从 `week` 改为 `month`（通过环境变量 `CRAWLER_TIME_FILTER`）
- 数据量提升：8K → ~15K（单次）

---

### 5. 环境变量（新增）

**文件：`backend/.env`**

```bash
# 抓取器配置
CRAWLER_TIME_FILTER=month  # week/month
HOT_CACHE_TTL_HOURS=24     # 热缓存 TTL（小时）
```

---

## 📊 数据流程

### 增量抓取流程

```
1. 获取水位线
   ↓
2. 调用 Reddit API（limit=100, time_filter=month）
   ↓
3. 过滤：只保留 created_at > watermark 的帖子
   ↓
4. 双写：
   ├─ 先写 posts_raw（冷库）
   │  └─ INSERT ... ON CONFLICT DO UPDATE
   └─ 再写 posts_hot（热缓存）
      └─ INSERT ... ON CONFLICT DO UPDATE
   ↓
5. 更新水位线
   └─ last_seen_created_at = max(posts.created_at)
```

---

### 数据去重策略

**多键组合：**
- `(source, source_post_id)`：平台内唯一
- `text_norm_hash`：防止转贴/改写（由触发器自动填充）

**SCD2 版本追踪：**
- `version`：版本号，每次编辑 +1
- `valid_from` / `valid_to`：版本生效/失效时间
- `is_current`：是否当前版本

**注意：** 当前实现暂未启用 SCD2 版本追踪（TODO），只做简单去重。

---

## 🎯 预期收益

### 数据量对比

| 模式 | 24小时 | 7天 | 30天 | 90天 |
|------|--------|-----|------|------|
| **旧模式（刷新）** | 8K | 8K | 8K | 8K |
| **新模式（累积）** | 15K | 100K | 300K | 800K |
| **提升倍数** | 2x | 12x | 37x | 100x |

### 算法收益

1. **趋势分析**：支持 7/14/30 天窗口
2. **精准去重**：防止转贴/改写
3. **历史回测**：有样本可回溯
4. **异常探测**：识别噪声周期

---

## 🧪 测试验证

### 测试脚本

**文件：`scripts/test-incremental-crawl.sh`**

**测试步骤：**
1. 检查数据库表是否存在
2. 查看当前存储统计
3. 手动触发增量抓取（3个社区，每个50条）
4. 查看抓取后统计
5. 查看冷库样本
6. 查看热缓存样本
7. 查看水位线状态

**执行命令：**
```bash
./scripts/test-incremental-crawl.sh
```

---

## 📝 待完成工作

### 高优先级

- [ ] **测试验证**：执行 `test-incremental-crawl.sh`
- [ ] **修复 Bug**：根据测试结果修复问题
- [ ] **SCD2 实现**：启用版本追踪（检测编辑）
- [ ] **分析引擎改造**：读取冷热混合数据

### 中优先级

- [ ] **定时任务**：
  - 每小时刷新物化视图
  - 每天清理过期热缓存
  - 每周清理旧数据（90天窗口）

- [ ] **监控仪表盘**：
  - 新增抓取量/小时
  - 累计样本/天
  - 去重率
  - 编辑更新率
  - 删除/失效率

### 低优先级

- [ ] **性能优化**：
  - 批量 upsert（使用 staging table）
  - 分区表（按月分区）
  - 索引优化

- [ ] **错误处理**：
  - 重试机制
  - 死信队列
  - 告警通知

---

## 🔧 配置说明

### 环境变量

```bash
# 抓取器配置
CRAWLER_BATCH_SIZE=12           # 批量大小
CRAWLER_MAX_CONCURRENCY=5       # 最大并发数
CRAWLER_POST_LIMIT=100          # 每个社区抓取的帖子数
CRAWLER_TIME_FILTER=month       # 时间范围（week/month）
HOT_CACHE_TTL_HOURS=24          # 热缓存 TTL（小时）

# Reddit API 配置
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=your_user_agent
REDDIT_RATE_LIMIT=30            # 限流：30 req/min
```

### Celery Beat 调度

```python
# 每 2 小时增量抓取
"auto-crawl-incremental": {
    "task": "tasks.crawler.crawl_seed_communities_incremental",
    "schedule": crontab(minute="0", hour="*/2"),
}
```

---

## 🚀 下一步

### 立即执行

1. **测试验证**：
   ```bash
   ./scripts/test-incremental-crawl.sh
   ```

2. **检查结果**：
   - 冷库是否有数据？
   - 热缓存是否有数据？
   - 水位线是否更新？
   - 去重率是否合理？

3. **修复问题**：
   - 根据测试结果修复 Bug
   - 调整参数（limit, time_filter, TTL）

### 本周完成

1. **分析引擎改造**：
   - 优先读取热缓存
   - 补读冷库（最近 30 天）
   - 支持时间窗口查询

2. **定时任务配置**：
   - 刷新物化视图
   - 清理过期缓存
   - 清理旧数据

### 下周完成

1. **监控仪表盘**：
   - 抓取量趋势
   - 去重率趋势
   - 存储空间使用

2. **性能优化**：
   - 批量 upsert
   - 分区表
   - 索引优化

---

## 🎉 总结

**已完成：**
- ✅ 数据库架构改造
- ✅ 增量抓取服务
- ✅ 抓取器改造
- ✅ Celery Beat 配置
- ✅ 测试脚本

**待完成：**
- ⏳ 测试验证
- ⏳ Bug 修复
- ⏳ 分析引擎改造
- ⏳ 定时任务配置
- ⏳ 监控仪表盘

**预期收益：**
- 📈 数据量：8K → 100K+（12x）
- 📊 趋势分析：支持 7/14/30 天窗口
- 🔍 去重准确：防止转贴/改写
- 🎯 算法优化：有历史样本可回测
- 💾 存储可控：~1.6 GB/90天

---

**🚀 增量抓取代码实施完成！下一步：测试验证。**

