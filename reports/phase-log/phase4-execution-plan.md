# Phase 4 æ‰§è¡Œè®¡åˆ’ï¼šè¿­ä»£ä¸å»¶ä¼¸

**åˆ›å»ºæ—¶é—´**: 2025-10-20  
**é¢„ä¼°å·¥æœŸ**: 12 å¤©ï¼ˆT+18~30 å¤©ï¼‰  
**ä»»åŠ¡æ€»æ•°**: 44 ä¸ªï¼ˆ1 ä¸ªæ ¹ä»»åŠ¡ + 6 ä¸ªä¸»ä»»åŠ¡ + 37 ä¸ªå­ä»»åŠ¡ï¼‰

---

## ğŸ“Š Phase 4 æ€»è§ˆ

**ç›®æ ‡**: ä¸¤å‘¨æ€»ç»“ã€NERã€è¶‹åŠ¿åˆ†æã€è¯æ®å›¾è°±

| ä»»åŠ¡ | æè¿° | å­ä»»åŠ¡æ•° | é¢„ä¼°æ—¶é—´ | ä¼˜å…ˆçº§ | ä¾èµ– |
|------|------|----------|----------|--------|------|
| **T4.1** | ç”Ÿæˆä¸¤å‘¨è¿­ä»£æ€»ç»“ | 6 | 2h | P0 | T3.6 |
| **T4.2** | å®ç°è½»é‡ NER | 6 | 4h | P1 | T4.1 |
| **T4.3** | å®ç°è¶‹åŠ¿åˆ†æ | 6 | 3h | P1 | T4.1 |
| **T4.4** | å®ç°è¯æ®å›¾è°± | 7 | 4h | P1 | T4.1 |
| **T4.5** | å®ç°å®ä½“è¯å…¸ | 7 | 3h | P2 | T4.2 |
| **T4.6** | å®ç°æ€åº¦ææ€§è¿‡æ»¤ | 6 | 2h | P1 | T4.1 |

---

## ğŸ¯ T4.1: ç”Ÿæˆä¸¤å‘¨è¿­ä»£æ€»ç»“

### ç›®æ ‡
å¤ç›˜ç¤¾åŒºæ‰©å®¹ã€è§„åˆ™æ”¹é€ ã€é˜ˆå€¼è°ƒæ•´æ•ˆæœï¼Œç”Ÿæˆæ€»ç»“æŠ¥å‘Šå¹¶è§„åˆ’ä¸‹ä¸€æœˆå·¥ä½œã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆï¼ˆ`reports/phase-log/phase5-summary.md`ï¼‰
- âœ… ä¸‹ä¸€æœˆè®¡åˆ’å·²åˆ¶å®š

### å®æ–½è¦ç‚¹

#### T4.1.1: æ”¶é›†ç¤¾åŒºæ‰©å®¹æ•°æ®
```sql
-- æŸ¥è¯¢ç¤¾åŒºæ•°é‡å¢é•¿
SELECT 
    DATE(created_at) as date,
    COUNT(DISTINCT subreddit) as community_count
FROM posts_raw
WHERE created_at >= NOW() - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY date;
```

#### T4.1.2: æ”¶é›†æ ·æœ¬é‡æå‡æ•°æ®
```sql
-- æŸ¥è¯¢æ ·æœ¬é‡å¢é•¿
SELECT 
    DATE(created_at) as date,
    COUNT(*) as post_count
FROM posts_raw
WHERE created_at >= NOW() - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY date;
```

#### T4.1.3: æ”¶é›†è§„åˆ™ä¼˜åŒ–æ•°æ®
```python
# è¯»å–é˜ˆå€¼ä¼˜åŒ–ç»“æœ
import pandas as pd
df = pd.read_csv('reports/threshold_optimization.csv')
best_threshold = df.loc[df['precision_at_50'].idxmax()]
print(f"æœ€ä¼˜é˜ˆå€¼: {best_threshold['threshold']:.2f}")
print(f"Precision@50: {best_threshold['precision_at_50']:.2f}")
print(f"F1 Score: {best_threshold['f1_score']:.2f}")
```

#### T4.1.4: æ”¶é›†çº¢çº¿è§¦å‘æ¬¡æ•°
```python
# ä»æ—¥å¿—æˆ–æ•°æ®åº“ç»Ÿè®¡çº¢çº¿è§¦å‘æ¬¡æ•°
# ç¤ºä¾‹ï¼šgrep "Red line triggered" logs/celery.log | wc -l
```

#### T4.1.5-T4.1.6: å†™å…¥æ€»ç»“æŠ¥å‘Šå¹¶è§„åˆ’ä¸‹ä¸€æœˆå·¥ä½œ
**æŠ¥å‘Šæ¨¡æ¿**:
```markdown
# Phase 5 è¿­ä»£æ€»ç»“

## æ•°æ®å¢é•¿
- ç¤¾åŒºæ•°é‡: 102 â†’ 300 (+194%)
- æ ·æœ¬é‡: 3K â†’ 15K+ (+400%)

## è§„åˆ™ä¼˜åŒ–
- Precision@50: 0.45 â†’ 0.62 (+38%)
- F1 Score: 0.52 â†’ 0.68 (+31%)

## çº¢çº¿è§¦å‘
- çº¢çº¿ 1ï¼ˆæœ‰æ•ˆå¸–å­ä¸è¶³ï¼‰: 3 æ¬¡
- çº¢çº¿ 2ï¼ˆç¼“å­˜å‘½ä¸­ç‡ä½ï¼‰: 1 æ¬¡
- çº¢çº¿ 3ï¼ˆé‡å¤ç‡é«˜ï¼‰: 0 æ¬¡
- çº¢çº¿ 4ï¼ˆç²¾å‡†ç‡ä½ï¼‰: 2 æ¬¡

## ä¸‹ä¸€æœˆè®¡åˆ’
1. å®ç° NER æå‡å®ä½“è¯†åˆ«èƒ½åŠ›
2. å®ç°è¶‹åŠ¿åˆ†ææ”¯æŒæ—¶é—´åºåˆ—æ´å¯Ÿ
3. å®ç°è¯æ®å›¾è°±å¢å¼ºå¯è§£é‡Šæ€§
```

---

## ğŸ¤– T4.2: å®ç°è½»é‡ NER

### ç›®æ ‡
ä½¿ç”¨ spaCy æˆ–è¯å…¸+æ­£åˆ™æå–å®ä½“ï¼ˆäº§å“ã€åŠŸèƒ½ã€å—ä¼—ã€è¡Œä¸šï¼‰ã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… å®ä½“æå–å‡†ç¡®ç‡ â‰¥70%
- âœ… é›†æˆåˆ°è¯„åˆ†å™¨

### æœ€ä½³å®è·µï¼ˆæ¥è‡ª exa-codeï¼‰

#### spaCy NER åŸºç¡€ç”¨æ³•
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for ent in doc.ents:
    print(ent.text, ent.label_)
# Output:
# Apple ORG
# U.K. GPE
# $1 billion MONEY
```

#### Entity Rulerï¼ˆè‡ªå®šä¹‰å®ä½“ï¼‰
```python
from spacy.lang.en import English

nlp = English()
ruler = nlp.add_pipe("entity_ruler")
patterns = [
    {"label": "PRODUCT", "pattern": "CRM"},
    {"label": "PRODUCT", "pattern": "ERP"},
    {"label": "FEATURE", "pattern": [{"LOWER": "export"}, {"LOWER": "feature"}]}
]
ruler.add_patterns(patterns)

doc = nlp("Our CRM export feature is broken.")
print([(ent.text, ent.label_) for ent in doc.ents])
# Output: [('CRM', 'PRODUCT'), ('export feature', 'FEATURE')]
```

### å®æ–½è¦ç‚¹

#### T4.2.1: å®‰è£… spaCy å’Œæ¨¡å‹
```bash
pip install spacy
python -m spacy download en_core_web_sm
```

#### T4.2.2-T4.2.4: åˆ›å»º EntityExtractor ç±»
**æ–‡ä»¶**: `backend/app/services/analysis/entity_extractor.py`

```python
import spacy
from typing import Dict, List, Set

class EntityExtractor:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        # æ·»åŠ è‡ªå®šä¹‰å®ä½“è§„åˆ™
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        patterns = self._load_entity_patterns()
        ruler.add_patterns(patterns)
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å®ä½“å¹¶åˆ†ç±»ä¸ºäº§å“ã€åŠŸèƒ½ã€å—ä¼—ã€è¡Œä¸š"""
        doc = self.nlp(text)
        entities = {
            "products": [],
            "features": [],
            "audiences": [],
            "industries": []
        }
        
        for ent in doc.ents:
            if ent.label_ in ["PRODUCT", "ORG"]:
                entities["products"].append(ent.text)
            elif ent.label_ == "FEATURE":
                entities["features"].append(ent.text)
            elif ent.label_ in ["PERSON", "NORP"]:
                entities["audiences"].append(ent.text)
            elif ent.label_ == "INDUSTRY":
                entities["industries"].append(ent.text)
        
        return entities
    
    def _load_entity_patterns(self) -> List[Dict]:
        """åŠ è½½è‡ªå®šä¹‰å®ä½“æ¨¡å¼"""
        return [
            {"label": "PRODUCT", "pattern": "CRM"},
            {"label": "PRODUCT", "pattern": "ERP"},
            {"label": "FEATURE", "pattern": "export"},
            # ... æ›´å¤šæ¨¡å¼
        ]
```

#### T4.2.5: é›†æˆåˆ° OpportunityScorer
```python
# åœ¨ OpportunityScorer ä¸­
from app.services.analysis.entity_extractor import EntityExtractor

class OpportunityScorer:
    def __init__(self):
        self.entity_extractor = EntityExtractor()
    
    def score(self, post: RedditPost) -> float:
        # ... åŸæœ‰è¯„åˆ†é€»è¾‘
        
        # å®ä½“åŒ¹é…åŠ åˆ†
        entities = self.entity_extractor.extract_entities(post.body)
        entity_bonus = 0.0
        if entities["products"]:
            entity_bonus += 0.1
        if entities["features"]:
            entity_bonus += 0.05
        
        return base_score + entity_bonus
```

---

## ğŸ“ˆ T4.3: å®ç°è¶‹åŠ¿åˆ†æ

### ç›®æ ‡
è¾“å‡ºä¸»é¢˜è¶‹åŠ¿æ›²çº¿ï¼ˆ7/14/30 å¤©ï¼‰ï¼Œæ”¯æŒæ—¶é—´åºåˆ—å¯è§†åŒ–ã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… è¶‹åŠ¿å›¾ç”ŸæˆæˆåŠŸ
- âœ… æ”¯æŒ 7/14/30 å¤©çª—å£

### æœ€ä½³å®è·µï¼ˆæ¥è‡ª exa-codeï¼‰

#### Matplotlib æ—¶é—´åºåˆ—å¯è§†åŒ–
```python
import matplotlib.pyplot as plt
import pandas as pd

# å‡†å¤‡æ•°æ®
df = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=30, freq='D'),
    'count': [10, 12, 15, 18, 20, ...]
})

# ç»˜åˆ¶è¶‹åŠ¿å›¾
plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['count'], marker='o', label='å¸–å­æ•°é‡')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å¸–å­æ•°é‡')
plt.title('30 å¤©è¶‹åŠ¿åˆ†æ')
plt.legend()
plt.grid(True)
plt.savefig('reports/trends/2023-01-30.png')
```

### å®æ–½è¦ç‚¹

#### T4.3.1-T4.3.2: åˆ›å»º TrendAnalyzer ç±»
**æ–‡ä»¶**: `backend/app/services/analysis/trend_analyzer.py`

```python
from datetime import date, timedelta
from typing import List, Dict
import pandas as pd
import matplotlib.pyplot as plt

class TrendAnalyzer:
    async def analyze_trends(
        self,
        *,
        window_days: int = 30,
        keywords: List[str] = None,
        session_factory = SessionFactory
    ) -> pd.DataFrame:
        """åˆ†æè¶‹åŠ¿æ•°æ®"""
        async with session_factory() as session:
            # æŸ¥è¯¢æ•°æ®
            query = select(
                func.date(PostsRaw.created_at).label('date'),
                func.count(PostsRaw.id).label('count')
            ).where(
                PostsRaw.created_at >= date.today() - timedelta(days=window_days)
            ).group_by(
                func.date(PostsRaw.created_at)
            ).order_by('date')
            
            result = await session.execute(query)
            rows = result.all()
            
            return pd.DataFrame(rows, columns=['date', 'count'])
    
    def plot_trends(
        self,
        df: pd.DataFrame,
        output_path: str,
        title: str = "è¶‹åŠ¿åˆ†æ"
    ):
        """ç»˜åˆ¶è¶‹åŠ¿å›¾"""
        plt.figure(figsize=(12, 6))
        plt.plot(df['date'], df['count'], marker='o', linewidth=2)
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('å¸–å­æ•°é‡')
        plt.title(title)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300)
        plt.close()
```

---

## ğŸ•¸ï¸ T4.4: å®ç°è¯æ®å›¾è°±

### ç›®æ ‡
æ„å»ºè¯æ®å›¾è°±æ•°æ®ç»“æ„ï¼Œæ”¯æŒæœºä¼š-è¯æ®å…³ç³»å¯è§†åŒ–ã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… è¯æ®å›¾è°±ç”ŸæˆæˆåŠŸ
- âœ… JSON æ ¼å¼æ­£ç¡®ï¼ˆnode-link æ ¼å¼ï¼‰

### æœ€ä½³å®è·µï¼ˆæ¥è‡ª exa-codeï¼‰

#### NetworkX å›¾è°±æ„å»ºä¸å¯¼å‡º
```python
import networkx as nx
from networkx.readwrite import json_graph
import json

# åˆ›å»ºå›¾
G = nx.DiGraph()

# æ·»åŠ èŠ‚ç‚¹
G.add_node("opp1", type="opportunity", problem="CRM export broken")
G.add_node("post1", type="evidence", title="Export doesn't work")

# æ·»åŠ è¾¹
G.add_edge("opp1", "post1", relevance=0.95)

# å¯¼å‡º JSON
data = json_graph.node_link_data(G)
with open('evidence_graph.json', 'w') as f:
    json.dump(data, f, indent=2)
```

### å®æ–½è¦ç‚¹

#### T4.4.1-T4.4.6: åˆ›å»º EvidenceGraph ç±»
**æ–‡ä»¶**: `backend/app/services/reporting/evidence_graph.py`

```python
import networkx as nx
from networkx.readwrite import json_graph
from typing import List, Dict, Any
import json

class EvidenceGraph:
    def __init__(self):
        self.graph = nx.DiGraph()
    
    def add_opportunity_node(
        self,
        opportunity_id: str,
        problem_definition: str,
        priority: float
    ):
        """æ·»åŠ æœºä¼šèŠ‚ç‚¹"""
        self.graph.add_node(
            opportunity_id,
            type="opportunity",
            problem=problem_definition,
            priority=priority
        )
    
    def add_evidence_node(
        self,
        post_id: str,
        title: str,
        url: str | None,
        score: float
    ):
        """æ·»åŠ è¯æ®èŠ‚ç‚¹"""
        self.graph.add_node(
            post_id,
            type="evidence",
            title=title,
            url=url,
            score=score
        )
    
    def add_edge(
        self,
        opportunity_id: str,
        post_id: str,
        relevance_score: float
    ):
        """æ·»åŠ è¾¹ï¼ˆæœºä¼š â†’ è¯æ®ï¼‰"""
        self.graph.add_edge(
            opportunity_id,
            post_id,
            relevance=relevance_score
        )
    
    def to_json(self) -> Dict[str, Any]:
        """å¯¼å‡º JSON æ ¼å¼"""
        return json_graph.node_link_data(self.graph)
    
    def save(self, output_path: str):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        with open(output_path, 'w') as f:
            json.dump(self.to_json(), f, indent=2)
```

---

## ğŸ“š T4.5: å®ç°å®ä½“è¯å…¸

### ç›®æ ‡
å»ºç«‹è¡Œä¸šå®ä½“è¯å…¸ï¼Œæ”¯æŒæ§½ä½åŒ¹é…ã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… è¯å…¸åˆ›å»ºæˆåŠŸï¼ˆ`config/entity_dictionary.yaml`ï¼‰
- âœ… æ§½ä½åŒ¹é…ç”Ÿæ•ˆ

### å®æ–½è¦ç‚¹

#### T4.5.1-T4.5.5: åˆ›å»ºå®ä½“è¯å…¸é…ç½®æ–‡ä»¶
**æ–‡ä»¶**: `config/entity_dictionary.yaml`

```yaml
products:
  - CRM
  - ERP
  - SaaS
  - API
  - Dashboard
  - Analytics
  - Workflow

features:
  - export
  - import
  - integration
  - automation
  - reporting
  - analytics
  - collaboration

audiences:
  - startup
  - enterprise
  - SMB
  - developer
  - marketer
  - sales team
  - product manager

industries:
  - fintech
  - healthcare
  - e-commerce
  - SaaS
  - AI
  - edtech
  - logistics

competitors:
  - Salesforce
  - HubSpot
  - Zendesk
  - Intercom
  - Slack
```

#### T4.5.6: å®ç°æ§½ä½åŒ¹é…é€»è¾‘
```python
import yaml
from typing import Dict, List, Set

class EntityDictionary:
    def __init__(self, config_path: str = "config/entity_dictionary.yaml"):
        with open(config_path) as f:
            self.dictionary = yaml.safe_load(f)
    
    def match_entity_slots(self, text: str) -> Dict[str, List[str]]:
        """åŒ¹é…æ–‡æœ¬ä¸­çš„å®ä½“æ§½ä½"""
        text_lower = text.lower()
        matches = {
            "products": [],
            "features": [],
            "audiences": [],
            "industries": [],
            "competitors": []
        }
        
        for category, entities in self.dictionary.items():
            for entity in entities:
                if entity.lower() in text_lower:
                    matches[category].append(entity)
        
        return matches
```

---

## ğŸ˜¡ T4.6: å®ç°æ€åº¦ææ€§è¿‡æ»¤

### ç›®æ ‡
å®šä¹‰å¼ºè´Ÿé¢è¯åº“å¹¶è¿‡æ»¤ï¼Œé¿å…å°†æŠ±æ€¨è¯¯åˆ¤ä¸ºæœºä¼šã€‚

### éªŒæ”¶æ ‡å‡†
- âœ… è´Ÿé¢å¸–å­è¢«è¿‡æ»¤
- âœ… æŠ±æ€¨ä¸è¢«å½“æˆæœºä¼š

### å®æ–½è¦ç‚¹

#### T4.6.1: åˆ›å»ºè´Ÿé¢è¯åº“é…ç½®
**æ–‡ä»¶**: `config/negative_keywords.yaml`

```yaml
strong_negative:
  - hate
  - terrible
  - awful
  - useless
  - garbage
  - worst

moderate_negative:
  - doesn't work
  - broken
  - frustrating
  - annoying
  - disappointing
  - confusing
```

#### T4.6.2-T4.6.4: å®ç°ææ€§æ£€æµ‹å’Œè¿‡æ»¤é€»è¾‘
```python
import yaml

class SentimentFilter:
    def __init__(self, config_path: str = "config/negative_keywords.yaml"):
        with open(config_path) as f:
            self.keywords = yaml.safe_load(f)
    
    def detect_sentiment_polarity(self, text: str) -> tuple[str, float]:
        """æ£€æµ‹æƒ…æ„Ÿææ€§ï¼Œè¿”å› (ææ€§, é™æƒç³»æ•°)"""
        text_lower = text.lower()
        
        # æ£€æŸ¥å¼ºè´Ÿé¢è¯
        for keyword in self.keywords["strong_negative"]:
            if keyword in text_lower:
                return ("strong_negative", 0.0)  # ç›´æ¥è¿‡æ»¤
        
        # æ£€æŸ¥ä¸­ç­‰è´Ÿé¢è¯
        for keyword in self.keywords["moderate_negative"]:
            if keyword in text_lower:
                return ("moderate_negative", 0.5)  # é™æƒ 50%
        
        return ("neutral", 1.0)  # æ— é™æƒ
```

---

## ğŸ“… æ¨èæ‰§è¡Œé¡ºåº

### ç¬¬ 1 å¤©ï¼šè¿­ä»£æ€»ç»“
- âœ… T4.1.1-T4.1.6: ç”Ÿæˆä¸¤å‘¨è¿­ä»£æ€»ç»“ï¼ˆ2 å°æ—¶ï¼‰

### ç¬¬ 2-3 å¤©ï¼šNER å®ç°
- âœ… T4.2.1-T4.2.6: å®ç°è½»é‡ NERï¼ˆ4 å°æ—¶ï¼‰
- âœ… T4.6.1-T4.6.6: å®ç°æ€åº¦ææ€§è¿‡æ»¤ï¼ˆ2 å°æ—¶ï¼‰

### ç¬¬ 4-5 å¤©ï¼šè¶‹åŠ¿åˆ†æ
- âœ… T4.3.1-T4.3.6: å®ç°è¶‹åŠ¿åˆ†æï¼ˆ3 å°æ—¶ï¼‰

### ç¬¬ 6-7 å¤©ï¼šè¯æ®å›¾è°±
- âœ… T4.4.1-T4.4.7: å®ç°è¯æ®å›¾è°±ï¼ˆ4 å°æ—¶ï¼‰

### ç¬¬ 8-9 å¤©ï¼šå®ä½“è¯å…¸
- âœ… T4.5.1-T4.5.7: å®ç°å®ä½“è¯å…¸ï¼ˆ3 å°æ—¶ï¼‰

### ç¬¬ 10-12 å¤©ï¼šé›†æˆæµ‹è¯•ä¸ä¼˜åŒ–
- âœ… é›†æˆæ‰€æœ‰åŠŸèƒ½åˆ°åˆ†æå¼•æ“
- âœ… ç«¯åˆ°ç«¯æµ‹è¯•
- âœ… æ€§èƒ½ä¼˜åŒ–

---

## ğŸ“ é¢„è®¡æ–°å¢æ–‡ä»¶

### åç«¯æ–‡ä»¶ï¼ˆ8 ä¸ªï¼‰
- `backend/app/services/analysis/entity_extractor.py`
- `backend/app/services/analysis/trend_analyzer.py`
- `backend/app/services/reporting/evidence_graph.py`
- `backend/tests/services/analysis/test_entity_extractor.py`
- `backend/tests/services/analysis/test_trend_analyzer.py`
- `backend/tests/services/reporting/test_evidence_graph.py`

### é…ç½®æ–‡ä»¶ï¼ˆ3 ä¸ªï¼‰
- `config/entity_dictionary.yaml`
- `config/negative_keywords.yaml`

### æŠ¥å‘Šæ–‡ä»¶
- `reports/phase-log/phase5-summary.md`
- `reports/trends/YYYY-MM-DD.png`ï¼ˆè¶‹åŠ¿å›¾ï¼‰
- `reports/evidence_graphs/YYYY-MM-DD.json`ï¼ˆè¯æ®å›¾è°±ï¼‰

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

1. **T4.1**: æ€»ç»“æŠ¥å‘Šå®Œæ•´ï¼Œæ•°æ®å‡†ç¡®
2. **T4.2**: NER å‡†ç¡®ç‡ â‰¥70%ï¼Œé›†æˆåˆ°è¯„åˆ†å™¨
3. **T4.3**: è¶‹åŠ¿å›¾æ¸…æ™°ï¼Œæ”¯æŒ 7/14/30 å¤©çª—å£
4. **T4.4**: è¯æ®å›¾è°± JSON æ ¼å¼æ­£ç¡®ï¼Œå¯å¯¼å…¥å‰ç«¯
5. **T4.5**: å®ä½“è¯å…¸è¦†ç›–ä¸»è¦é¢†åŸŸï¼Œæ§½ä½åŒ¹é…ç”Ÿæ•ˆ
6. **T4.6**: è´Ÿé¢å¸–å­è¢«æ­£ç¡®è¿‡æ»¤ï¼ŒæŠ±æ€¨ä¸è¢«è¯¯åˆ¤ä¸ºæœºä¼š

---

**åˆ›å»ºäºº**: AI Agent  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-20

