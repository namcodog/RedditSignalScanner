# T1.7: 实现分级调度策略 - 验收报告

## 执行时间
- 开始时间: 2025-10-17
- 完成时间: 2025-10-17
- 执行人: Agent

## 任务目标
根据社区质量分（avg_valid_posts）将社区分为三档，并应用不同的抓取策略（频率、sort、time_filter），实现差异化调度以提升整体抓取效率和成功率。

## 执行过程

### 1. 社区质量分析
**当前数据状态**:
- community_cache 中有记录的社区: 103 个
- avg_valid_posts 平均值: 12.70
- avg_valid_posts 最大值: 30
- avg_valid_posts 最小值: 0

**Top 20 高质量社区**:
- programming, 3dprinting, investing, writing, learnprogramming
- cooking, fitness, personalfinance, cscareerquestions, remotework
- cryptocurrency, technology, machinelearning, startups, meditation
- diy, datascience, entrepreneur, art, gamedev

### 2. 分级策略设计
基于当前数据分布，定义三档调度策略：

#### Tier 1: 高活跃社区
- **阈值**: avg_valid_posts > 20
- **频率**: 每 2 小时
- **策略**: sort=new, time_filter=week, limit=50
- **目标**: 优先抓取最新内容，保证实时性
- **社区数**: 35 个

#### Tier 2: 中活跃社区
- **阈值**: 10 < avg_valid_posts ≤ 20
- **频率**: 每 6 小时
- **策略**: sort=top, time_filter=week, limit=80
- **目标**: 平衡热门与新增内容
- **社区数**: 11 个

#### Tier 3: 低活跃社区
- **阈值**: avg_valid_posts ≤ 10
- **频率**: 每 24 小时
- **策略**: sort=top, time_filter=month, limit=100
- **目标**: 覆盖历史高质量内容
- **社区数**: 49 个

### 3. 实现分级调度脚本
**脚本**: `scripts/tiered_scheduling.py`

**核心功能**:
1. **calculate_tier_assignment()**: 计算社区分级分配
   - 读取 community_pool 和 community_cache 数据
   - 根据 avg_valid_posts 分配到对应档位
   - 排除黑名单社区
   - 标记无数据社区

2. **apply_tier_to_cache()**: 应用分级配置到数据库
   - 更新 community_cache 的 crawl_frequency_hours
   - 更新 quality_tier 字段
   - 批量提交事务

3. **run_tiered_crawl()**: 执行指定档位的抓取
   - 设置对应的环境变量（CRAWLER_SORT, CRAWLER_TIME_FILTER, CRAWLER_POST_LIMIT）
   - 提供抓取命令建议
   - 支持 dry-run 模式

**使用方法**:
```bash
# 查看分级统计（dry-run）
PYTHONPATH=backend python3 scripts/tiered_scheduling.py --dry-run

# 应用分级配置到数据库
PYTHONPATH=backend python3 scripts/tiered_scheduling.py

# 执行指定档位的抓取（dry-run）
PYTHONPATH=backend python3 scripts/tiered_scheduling.py --dry-run --tier tier1

# 实际执行 Tier 1 抓取
PYTHONPATH=backend python3 scripts/tiered_scheduling.py --tier tier1
```

### 4. 测试与验证
**Dry-run 测试**:
```bash
PYTHONPATH=backend python3 scripts/tiered_scheduling.py --dry-run
```

**结果**:
- Tier 1（高活跃）: 35 个社区
- Tier 2（中活跃）: 11 个社区
- Tier 3（低活跃）: 49 个社区
- 无数据社区: 174 个（新扩容的社区）
- 黑名单社区: 0 个

**实际应用**:
```bash
PYTHONPATH=backend python3 scripts/tiered_scheduling.py
```

**结果**: ✅ 已更新 95 个社区的调度配置

**Tier 1 抓取测试**:
```bash
PYTHONPATH=backend python3 scripts/tiered_scheduling.py --dry-run --tier tier1
```

**结果**: 
- 社区数: 35
- 策略: sort=new, time_filter=week, limit=50
- 前 10 个社区: personalfinance, cybersecurity, technology, investing, cryptocurrency, fitness, learnprogramming, 3dprinting, devops, writing

## 验收结果

### ✅ 分级策略定义清晰
- 三档策略明确（Tier 1/2/3）
- 阈值合理（基于当前数据分布）
- 频率差异化（2h / 6h / 24h）
- 参数策略差异化（sort, time_filter, limit）

### ✅ 脚本功能完整
- 自动计算分级分配
- 应用配置到数据库
- 支持指定档位抓取
- 支持 dry-run 模式
- 日志输出清晰

### ✅ 数据库更新成功
- 95 个社区的 crawl_frequency_hours 已更新
- quality_tier 字段已设置
- 数据一致性验证通过

### ✅ 黑名单集成
- 自动排除黑名单社区
- 与 T1.6 黑名单配置无缝集成

## 分级调度效果预测

### 资源优化
**之前（统一调度）**:
- 所有社区同频率抓取
- 低活跃社区浪费资源
- 高活跃社区更新不及时

**之后（分级调度）**:
- Tier 1（35个）: 每天 12 次 = 420 次抓取
- Tier 2（11个）: 每天 4 次 = 44 次抓取
- Tier 3（49个）: 每天 1 次 = 49 次抓取
- **总计**: 每天 513 次抓取（vs 之前 95 * 12 = 1140 次）
- **资源节省**: ~55%

### 成功率提升
**Tier 1 策略优势**:
- sort=new: 避免重复命中热门榜（T1.4 验证：成功率 95% vs 5%）
- 高频抓取: 保证实时性
- 预期成功率: **70-90%**

**Tier 2/3 策略**:
- sort=top: 覆盖历史高质量内容
- 低频抓取: 减少空结果
- 预期成功率: **40-60%**

**整体预期**:
- 平均成功率: **60-80%**（vs 之前 5-30%）
- 新增帖子量: **提升 2-3 倍**

### 数据质量提升
- 高活跃社区: 实时性强，机会信号新鲜
- 中活跃社区: 平衡热门与新增
- 低活跃社区: 覆盖历史精华内容
- 整体: **信号质量与覆盖度双提升**

## 后续集成计划

### Celery Beat 集成（可选）
如需完整自动化调度，可集成到 Celery Beat:

```python
# backend/app/celery_app.py
from celery.schedules import crontab

app.conf.beat_schedule = {
    'crawl-tier1-every-2h': {
        'task': 'app.tasks.crawler_task.crawl_tier',
        'schedule': crontab(minute=0, hour='*/2'),  # 每 2 小时
        'args': ('tier1',),
    },
    'crawl-tier2-every-6h': {
        'task': 'app.tasks.crawler_task.crawl_tier',
        'schedule': crontab(minute=0, hour='*/6'),  # 每 6 小时
        'args': ('tier2',),
    },
    'crawl-tier3-daily': {
        'task': 'app.tasks.crawler_task.crawl_tier',
        'schedule': crontab(minute=0, hour=2),  # 每天凌晨 2 点
        'args': ('tier3',),
    },
}
```

### 动态调整机制
根据抓取效果动态调整分级:
- 成功率持续低于 30% → 降级
- 成功率持续高于 80% → 升级
- avg_valid_posts 更新后重新计算分级

### 监控与告警
- 每日统计各档位的成功率、新增量
- 写入 crawl_metrics 表
- 异常情况告警（成功率骤降、空结果率过高）

## 使用建议

### 立即执行
建议立即执行一轮 Tier 1 抓取验证效果:
```bash
PYTHONPATH=backend \
CRAWLER_SORT=new \
CRAWLER_TIME_FILTER=week \
CRAWLER_POST_LIMIT=50 \
CRAWLER_BATCH_SIZE=15 \
CRAWLER_MAX_CONCURRENCY=3 \
python3 scripts/run-incremental-crawl.py
```

### 定期维护
- 每周审查分级分配（avg_valid_posts 更新后）
- 每月调整阈值（根据数据分布变化）
- 季度优化策略（根据抓取效果）

### 扩展方向
- 增加 Tier 0（超高活跃，每小时抓取）
- 细化 Tier 3（按类目差异化）
- 引入机器学习预测最佳抓取时机

## 验收结论
✅ **T1.7 任务完成**

- 分级策略定义清晰（Tier 1/2/3，差异化频率与参数）
- 分级调度脚本实现完整（计算、应用、执行）
- 数据库配置更新成功（95 个社区）
- 黑名单集成无缝（自动排除）
- 测试验证通过（dry-run + 实际应用）

**预期效果**:
- 资源节省: ~55%
- 成功率提升: 60-80%（vs 之前 5-30%）
- 新增帖子量: 提升 2-3 倍

**下一步**: T1.8 精准补抓任务实施。

