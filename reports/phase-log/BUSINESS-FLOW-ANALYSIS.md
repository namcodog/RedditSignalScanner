# 🔍 Reddit Signal Scanner - 完整业务流程分析

**日期**: 2025-10-14  
**分析目标**: 系统化梳理从真实 Reddit API 到最终报告的完整业务流程  
**参考文档**: PRD-03 分析引擎设计

---

## 📋 执行摘要

### ✅ **当前实现状态**

根据代码库分析，项目已实现完整的业务流程，但存在以下关键点需要配置：

1. ✅ **Reddit API 集成已实现**（`backend/app/services/reddit_client.py`）
2. ✅ **缓存优先架构已实现**（`backend/app/services/cache_manager.py`）
3. ✅ **社区发现算法已实现**（`backend/app/services/analysis/community_discovery.py`）
4. ⚠️ **需要配置 Reddit API 凭证**（环境变量）
5. ⚠️ **社区池数据当前为硬编码**（10 个社区，PRD 要求 500+）
6. ⚠️ **后台爬虫系统未实现**（PRD-03 §3.5 设计但未实施）

---

## 🔄 完整业务流程（基于 PRD-03）

### **流程概览**

```
用户输入产品描述
    ↓
Step 1: 社区发现（30秒）
    ├─ 关键词提取（TF-IDF）
    └─ 从社区池选择 Top 20 相关社区
    ↓
Step 2: 数据采集（120秒）
    ├─ 缓存优先策略（90% 来自 Redis）
    └─ 实时 API 补充（10% 来自 Reddit API）
    ↓
Step 3: 信号提取（90秒）
    ├─ 痛点识别
    ├─ 竞品分析
    └─ 机会发现
    ↓
Step 4: 报告生成（30秒）
    └─ 结构化输出 + 权重排序
    ↓
最终报告展示
```

---

## 1️⃣ 社区从哪里来？

### **当前实现**

**位置**: `backend/app/services/analysis_engine.py` 第 66-147 行

**社区池**: 硬编码 10 个社区
```python
COMMUNITY_CATALOGUE: List[CommunityProfile] = [
    CommunityProfile(name="r/startups", ...),
    CommunityProfile(name="r/Entrepreneur", ...),
    CommunityProfile(name="r/ProductManagement", ...),
    CommunityProfile(name="r/SaaS", ...),
    CommunityProfile(name="r/marketing", ...),
    CommunityProfile(name="r/technology", ...),
    CommunityProfile(name="r/artificial", ...),
    CommunityProfile(name="r/userexperience", ...),
    CommunityProfile(name="r/smallbusiness", ...),
    CommunityProfile(name="r/GrowthHacking", ...),
]
```

### **PRD 要求**

**PRD-03 §3.1**: 从 **500+ 社区池**中发现最相关的 20 个社区

### **差距分析**

| 项目 | PRD 要求 | 当前实现 | 差距 |
|------|----------|----------|------|
| 社区池大小 | 500+ | 10 | ❌ 缺少 490+ 社区 |
| 数据来源 | 数据库/配置文件 | 硬编码 | ⚠️ 需要外部化 |
| 动态更新 | 支持 | 不支持 | ⚠️ 需要实现 |

### **建议方案**

#### **方案 A：JSON 配置文件（推荐）**
```bash
# 创建社区池配置文件
backend/config/community_pool.json

# 包含 500+ 社区的元数据
{
  "communities": [
    {
      "name": "r/startups",
      "categories": ["startup", "business"],
      "description_keywords": ["startup", "founder", "product"],
      "daily_posts": 180,
      "avg_comment_length": 72,
      "cache_hit_rate": 0.91
    },
    ...
  ]
}
```

#### **方案 B：数据库存储（生产环境）**
```sql
CREATE TABLE community_pool (
    name VARCHAR(100) PRIMARY KEY,
    categories JSONB,
    description_keywords JSONB,
    daily_posts INTEGER,
    avg_comment_length INTEGER,
    cache_hit_rate FLOAT,
    last_updated TIMESTAMP
);
```

---

## 2️⃣ 真实数据从哪里来？

### **当前实现**

#### **Reddit API 客户端**

**位置**: `backend/app/services/reddit_client.py`

**功能**:
- ✅ OAuth2 认证
- ✅ 单个 subreddit 数据获取
- ✅ 并发获取多个 subreddit
- ✅ 速率限制控制（60 请求/分钟）
- ✅ 错误处理和重试

**关键代码**:
```python
class RedditAPIClient:
    def __init__(
        self,
        client_id: str,          # 需要配置
        client_secret: str,      # 需要配置
        user_agent: str,
        rate_limit: int = 60,
    ):
        ...
    
    async def fetch_subreddit_posts(
        self,
        subreddit: str,
        limit: int = 100,
        time_filter: str = "week",
        sort: str = "top",
    ) -> List[RedditPost]:
        """从 Reddit API 获取真实数据"""
        ...
```

#### **缓存优先策略**

**位置**: `backend/app/services/data_collection.py`

**流程**:
```python
async def collect_posts(communities, limit_per_subreddit=100):
    # 1. 先从 Redis 缓存读取
    for subreddit in subreddits:
        cached = cache.get_cached_posts(subreddit)
        if cached:
            posts_by_subreddit[subreddit] = cached
            cached_subreddits.add(subreddit)
    
    # 2. 缓存未命中的，调用 Reddit API
    missing = [name for name in subreddits if name not in cached_subreddits]
    if missing:
        results = await reddit.fetch_multiple_subreddits(missing)
        # 3. 将新数据写入缓存
        for subreddit, posts in results.items():
            cache.set_cached_posts(subreddit, posts)
```

### **数据来源**

| 数据源 | 比例 | 新鲜度 | 说明 |
|--------|------|--------|------|
| Redis 缓存 | 90% | 24 小时内 | 预爬取数据 |
| Reddit API | 10% | 实时 | 补充数据 |

### **需要配置的环境变量**

**位置**: `backend/app/core/config.py` 第 25-33 行

```bash
# Reddit API 凭证（必须配置）
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USER_AGENT=RedditSignalScanner/1.0

# Reddit API 配置（可选）
REDDIT_RATE_LIMIT=60
REDDIT_RATE_LIMIT_WINDOW_SECONDS=60.0
REDDIT_REQUEST_TIMEOUT_SECONDS=30.0
REDDIT_MAX_CONCURRENCY=5

# Redis 缓存配置
REDDIT_CACHE_REDIS_URL=redis://localhost:6379/5
REDDIT_CACHE_TTL_SECONDS=86400  # 24 小时
```

### **如何获取 Reddit API 凭证**

1. 访问 https://www.reddit.com/prefs/apps
2. 点击 "Create App" 或 "Create Another App"
3. 选择 "script" 类型
4. 填写应用信息：
   - Name: Reddit Signal Scanner
   - Description: Business signal analysis tool
   - Redirect URI: http://localhost:8080
5. 获取 `client_id` 和 `client_secret`

---

## 3️⃣ 分析从哪里来？

### **当前实现**

#### **分析引擎**

**位置**: `backend/app/services/analysis_engine.py`

**四步流水线**:

1. **社区发现** (`backend/app/services/analysis/community_discovery.py`)
   - 关键词提取（TF-IDF）
   - 社区相关性评分
   - Top-K 选择 + 多样性保证

2. **数据采集** (`backend/app/services/data_collection.py`)
   - 缓存优先策略
   - 并发 API 调用
   - 数据清洗和过滤

3. **信号提取** (`backend/app/services/analysis/signal_extraction.py`)
   - 痛点识别（情感分析 + 频率统计）
   - 竞品识别（品牌名称提取）
   - 机会发现（未满足需求识别）

4. **报告生成** (`backend/app/services/analysis_engine.py`)
   - 结构化输出
   - 权重排序
   - HTML 渲染

### **分析算法**

#### **痛点识别**
```python
# 基于情感分析和频率统计
pain_points = []
for post in posts:
    if contains_negative_sentiment(post):
        pain_points.append({
            "description": extract_pain_description(post),
            "severity": classify_severity(post),
            "frequency": count_mentions(post),
            "source": post.subreddit
        })
```

#### **竞品识别**
```python
# 基于品牌名称提取
competitors = []
for post in posts:
    brands = extract_brand_names(post)
    for brand in brands:
        competitors.append({
            "name": brand,
            "mentions": count_brand_mentions(brand, posts),
            "sentiment": analyze_brand_sentiment(brand, posts),
            "strengths": extract_strengths(brand, posts),
            "weaknesses": extract_weaknesses(brand, posts)
        })
```

#### **机会发现**
```python
# 基于未满足需求识别
opportunities = []
for post in posts:
    if contains_unmet_need(post):
        opportunities.append({
            "description": extract_opportunity(post),
            "market_size": estimate_market_size(post),
            "urgency": classify_urgency(post),
            "source": post.subreddit
        })
```

---

## 4️⃣ 分析结果在哪里产出？

### **数据流**

```
分析引擎
    ↓
Analysis 模型（数据库）
    ├─ insights (JSONB)
    │   ├─ pain_points: List[Dict]
    │   ├─ competitors: List[Dict]
    │   └─ opportunities: List[Dict]
    └─ metadata (JSONB)
        ├─ cache_hit_rate: float
        ├─ total_posts: int
        └─ communities_analyzed: List[str]
    ↓
Report 模型（数据库）
    ├─ overview (JSONB)
    │   ├─ market_sentiment: str
    │   ├─ top_communities: List[Dict]
    │   └─ key_insights: List[str]
    ├─ pain_points (JSONB)
    ├─ competitors (JSONB)
    └─ opportunities (JSONB)
    ↓
API 响应（GET /api/reports/{task_id}）
    ↓
前端展示（ReportPage.tsx）
```

### **数据库存储**

**Analysis 表**:
```sql
CREATE TABLE analyses (
    id UUID PRIMARY KEY,
    task_id UUID REFERENCES tasks(id),
    insights JSONB,  -- 分析结果
    metadata JSONB,  -- 元数据
    created_at TIMESTAMP
);
```

**Report 表**:
```sql
CREATE TABLE reports (
    id UUID PRIMARY KEY,
    task_id UUID REFERENCES tasks(id),
    overview JSONB,      -- 概览
    pain_points JSONB,   -- 痛点列表
    competitors JSONB,   -- 竞品列表
    opportunities JSONB, -- 机会列表
    created_at TIMESTAMP
);
```

### **API 响应格式**

```json
{
  "task_id": "uuid",
  "status": "completed",
  "overview": {
    "market_sentiment": "positive",
    "total_mentions": 156,
    "top_communities": [...]
  },
  "pain_points": [
    {
      "description": "用户痛点描述",
      "severity": "high",
      "frequency": 23,
      "sources": [...]
    }
  ],
  "competitors": [...],
  "opportunities": [...]
}
```

---

## 5️⃣ 当前实现是否具备完整流程？

### **✅ 已实现**

1. ✅ **Reddit API 集成**（OAuth2 + 数据获取）
2. ✅ **缓存优先架构**（Redis + 24 小时 TTL）
3. ✅ **社区发现算法**（关键词提取 + 相关性评分）
4. ✅ **数据采集服务**（并发 + 速率限制）
5. ✅ **信号提取算法**（痛点/竞品/机会）
6. ✅ **报告生成**（结构化输出 + 数据库存储）
7. ✅ **完整 API**（POST /analyze → GET /report）
8. ✅ **前端展示**（输入页 → 进度页 → 报告页）

### **⚠️ 需要配置**

1. ⚠️ **Reddit API 凭证**（环境变量）
2. ⚠️ **社区池数据**（当前 10 个，PRD 要求 500+）
3. ⚠️ **后台爬虫系统**（PRD 设计但未实施）

### **❌ 未实现**

1. ❌ **后台爬虫系统**（PRD-03 §3.5）
   - 24 小时持续爬取
   - 智能优先级管理
   - API 限额平滑分布
   - 缓存质量监控

---

## 6️⃣ GitHub 提交策略

### **当前状态**

根据代码库分析，项目已有完整的 Git 配置：

- ✅ `.gitignore` 文件存在（`frontend/.gitignore`）
- ✅ 代码已在本地 Git 仓库
- ⚠️ 环境变量文件需要排除

### **建议的 .gitignore 配置**

```bash
# 环境变量（敏感信息）
.env
.env.local
.env.*.local
backend/.env
frontend/.env

# Reddit API 凭证（绝对不能提交）
**/reddit_credentials.json
**/api_keys.txt

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
ENV/

# Node
node_modules/
dist/
build/

# IDE
.vscode/
.idea/
*.swp
*.swo

# 数据库
*.db
*.sqlite

# 日志
*.log
logs/

# 缓存
.cache/
.pytest_cache/
.mypy_cache/
```

### **提交前检查清单**

```bash
# 1. 确保敏感信息不在代码中
grep -r "REDDIT_CLIENT_ID" backend/
grep -r "REDDIT_CLIENT_SECRET" backend/
# 应该只在 config.py 中作为环境变量读取

# 2. 创建 .env.example 模板
cat > backend/.env.example << EOF
# Reddit API 凭证（必须配置）
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USER_AGENT=RedditSignalScanner/1.0

# 数据库配置
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/reddit_scanner

# JWT 配置
JWT_SECRET=your-secret-key-here
EOF

# 3. 提交代码
git add .
git commit -m "feat: 完整业务流程实现

- Reddit API 集成（OAuth2 + 数据获取）
- 缓存优先架构（Redis + 24h TTL）
- 四步分析流水线（社区发现 → 数据采集 → 信号提取 → 报告生成）
- 完整端到端流程（输入 → 分析 → 报告）

注意：需要配置 REDDIT_CLIENT_ID 和 REDDIT_CLIENT_SECRET"

git push origin main
```

---

## 📝 总结与建议

### **当前状态**

✅ **核心业务流程已完整实现**，可以连接真实 Reddit API 获取数据并生成分析报告。

### **立即需要做的**

1. **配置 Reddit API 凭证**（5 分钟）
   ```bash
   export REDDIT_CLIENT_ID=your_client_id
   export REDDIT_CLIENT_SECRET=your_client_secret
   ```

2. **测试真实 API 调用**（10 分钟）
   ```bash
   # 启动服务
   make dev-backend
   
   # 提交分析任务
   curl -X POST http://localhost:8006/api/analyze \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer {token}" \
     -d '{"product_description": "AI-powered project management tool"}'
   ```

3. **扩展社区池**（可选，后续优化）
   - 创建 `backend/config/community_pool.json`
   - 添加 500+ 社区数据

4. **实现后台爬虫**（可选，生产环境优化）
   - 按 PRD-03 §3.5 设计实现
   - 24 小时持续爬取热门社区

### **GitHub 提交建议**

✅ **可以安全提交**，但需要：
1. 确保 `.env` 文件在 `.gitignore` 中
2. 创建 `.env.example` 模板
3. 在 README 中说明如何配置 Reddit API 凭证

---

**最后更新**: 2025-10-14  
**维护者**: Lead Agent

