# Day 13 ä»»åŠ¡åˆ†é…è¡¨

**æ—¥æœŸ**: 2025-10-14ï¼ˆDay 13ï¼‰  
**é˜¶æ®µ**: Phase 0 å‡†å¤‡é˜¶æ®µ + Phase 1 åŸºç¡€ç¼“å­˜é¢„çƒ­ï¼ˆå¯åŠ¨ï¼‰  
**ç›®æ ‡**: å®Œæˆæ•°æ®åº“è¿ç§»ã€ç§å­ç¤¾åŒºå‡†å¤‡ã€çˆ¬è™«ç³»ç»Ÿå®ç°å¹¶å¯åŠ¨é¢„çƒ­

---

## ğŸ¯ Day 13 æ€»ä½“ç›®æ ‡

- âœ… æ•°æ®åº“è¿ç§»å®Œæˆï¼ˆ`community_pool` å’Œ `pending_communities` è¡¨ï¼‰
- âœ… ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡å®Œæˆï¼ˆ50-100 ä¸ªç¤¾åŒº JSON æ–‡ä»¶ï¼‰
- âœ… ç¤¾åŒºæ± åŠ è½½å™¨å®ç°å®Œæˆ
- âœ… çˆ¬è™«ä»»åŠ¡å®ç°å®Œæˆ
- âœ… é¢„çƒ­çˆ¬è™«å¯åŠ¨å¹¶è¿è¡Œ
- âœ… ç›‘æ§ç³»ç»Ÿæ­å»ºå®Œæˆ

---

## ğŸ‘¨â€ğŸ’» Backend Agent A - ä»»åŠ¡æ¸…å•

### ä¸Šåˆä»»åŠ¡ï¼ˆ9:00-12:00ï¼Œ3å°æ—¶ï¼‰

#### ä»»åŠ¡ A1: æ•°æ®åº“è¿ç§»ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 9:00-11:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»º Alembic è¿ç§»æ–‡ä»¶
  ```bash
  cd backend
  alembic revision -m "add_community_pool_and_pending_communities"
  ```

- [ ] ç¼–å†™ `community_pool` è¡¨è¿ç§» SQL
  ```python
  # backend/alembic/versions/xxx_add_community_pool.py
  
  def upgrade():
      op.create_table(
          'community_pool',
          sa.Column('id', sa.Integer(), nullable=False),
          sa.Column('name', sa.String(100), nullable=False),
          sa.Column('tier', sa.String(20), nullable=False),
          sa.Column('categories', sa.JSON(), nullable=False),
          sa.Column('description_keywords', sa.JSON(), nullable=False),
          sa.Column('daily_posts', sa.Integer(), default=0),
          sa.Column('avg_comment_length', sa.Integer(), default=0),
          sa.Column('quality_score', sa.Numeric(3, 2), default=0.50),
          sa.Column('user_feedback_count', sa.Integer(), default=0),
          sa.Column('discovered_count', sa.Integer(), default=0),
          sa.Column('is_active', sa.Boolean(), default=True),
          sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),
          sa.Column('updated_at', sa.DateTime(), server_default=sa.func.now(), onupdate=sa.func.now()),
          sa.PrimaryKeyConstraint('id'),
          sa.UniqueConstraint('name')
      )
      
      op.create_index('idx_community_pool_tier', 'community_pool', ['tier'])
      op.create_index('idx_community_pool_is_active', 'community_pool', ['is_active'])
      op.create_index('idx_community_pool_quality_score', 'community_pool', ['quality_score'])
  ```

- [ ] ç¼–å†™ `pending_communities` è¡¨è¿ç§» SQL
  ```python
  def upgrade():
      op.create_table(
          'pending_communities',
          sa.Column('id', sa.Integer(), nullable=False),
          sa.Column('name', sa.String(100), nullable=False),
          sa.Column('discovered_from_keywords', sa.JSON()),
          sa.Column('discovered_count', sa.Integer(), default=1),
          sa.Column('first_discovered_at', sa.DateTime(), server_default=sa.func.now()),
          sa.Column('last_discovered_at', sa.DateTime(), server_default=sa.func.now()),
          sa.Column('status', sa.String(20), default='pending'),
          sa.Column('admin_reviewed_at', sa.DateTime()),
          sa.Column('admin_notes', sa.Text()),
          sa.PrimaryKeyConstraint('id'),
          sa.UniqueConstraint('name')
      )
      
      op.create_index('idx_pending_communities_status', 'pending_communities', ['status'])
      op.create_index('idx_pending_communities_discovered_count', 'pending_communities', ['discovered_count'])
  ```

- [ ] æ‰§è¡Œè¿ç§»
  ```bash
  alembic upgrade head
  ```

- [ ] éªŒè¯è¡¨ç»“æ„
  ```bash
  psql -d reddit_scanner -c "\d community_pool"
  psql -d reddit_scanner -c "\d pending_communities"
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… `community_pool` è¡¨åˆ›å»ºæˆåŠŸï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µå’Œç´¢å¼•
- âœ… `pending_communities` è¡¨åˆ›å»ºæˆåŠŸ
- âœ… è¿ç§»å¯ä»¥æ­£å¸¸å›æ»šï¼ˆ`alembic downgrade -1`ï¼‰

**è¾“å‡ºæ–‡ä»¶**:
- `backend/alembic/versions/xxx_add_community_pool_and_pending_communities.py`

---

### ä¸‹åˆä»»åŠ¡ï¼ˆ13:00-18:00ï¼Œ5å°æ—¶ï¼‰

#### ä»»åŠ¡ A2: ç¤¾åŒºæ± åŠ è½½å™¨å®ç°ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 13:00-16:00ï¼ˆ3å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»ºæ•°æ®æ¨¡å‹
  ```bash
  # backend/app/models/community_pool.py
  ```

- [ ] å®ç° `CommunityPool` å’Œ `PendingCommunity` SQLAlchemy æ¨¡å‹
  ```python
  class CommunityPool(Base):
      __tablename__ = "community_pool"
      
      id = Column(Integer, primary_key=True)
      name = Column(String(100), unique=True, nullable=False)
      tier = Column(String(20), nullable=False)
      categories = Column(JSON, nullable=False)
      description_keywords = Column(JSON, nullable=False)
      daily_posts = Column(Integer, default=0)
      avg_comment_length = Column(Integer, default=0)
      quality_score = Column(Numeric(3, 2), default=0.50)
      user_feedback_count = Column(Integer, default=0)
      discovered_count = Column(Integer, default=0)
      is_active = Column(Boolean, default=True)
      created_at = Column(DateTime, server_default=func.now())
      updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
  ```

- [ ] åˆ›å»ºç¤¾åŒºæ± åŠ è½½å™¨æœåŠ¡
  ```bash
  # backend/app/services/community_pool_loader.py
  ```

- [ ] å®ç° `load_seed_communities()` å‡½æ•°ï¼ˆä» JSON åŠ è½½ï¼‰
- [ ] å®ç° `import_to_database()` å‡½æ•°ï¼ˆå¯¼å…¥åˆ°æ•°æ®åº“ï¼‰
- [ ] å®ç° `load_community_pool()` å‡½æ•°ï¼ˆä»æ•°æ®åº“åŠ è½½ï¼Œå¸¦ç¼“å­˜ï¼‰
- [ ] å®ç° `get_community_by_name()` å‡½æ•°
- [ ] å®ç° `get_communities_by_tier()` å‡½æ•°

**æ ¸å¿ƒä»£ç **:
```python
class CommunityPoolLoader:
    def __init__(self):
        self._cache: List[CommunityProfile] = []
        self._last_refresh: datetime | None = None
        self._refresh_interval = timedelta(hours=1)
    
    async def load_seed_communities(self) -> List[Dict]:
        """ä» JSON æ–‡ä»¶åŠ è½½ç§å­ç¤¾åŒº"""
        json_path = Path("backend/config/seed_communities.json")
        with open(json_path) as f:
            data = json.load(f)
        return data["seed_communities"]
    
    async def import_to_database(self):
        """å¯¼å…¥ç§å­ç¤¾åŒºåˆ°æ•°æ®åº“"""
        seed_communities = await self.load_seed_communities()
        async with get_session() as session:
            for community_data in seed_communities:
                existing = await session.execute(
                    select(CommunityPool).where(CommunityPool.name == community_data["name"])
                )
                if not existing.scalar_one_or_none():
                    new_community = CommunityPool(**community_data)
                    session.add(new_community)
            await session.commit()
    
    async def load_community_pool(self, force_refresh: bool = False) -> List[CommunityProfile]:
        """ä»æ•°æ®åº“åŠ è½½ç¤¾åŒºæ± ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if force_refresh or self._should_refresh():
            async with get_session() as session:
                result = await session.execute(
                    select(CommunityPool).where(CommunityPool.is_active == True)
                )
                communities = result.scalars().all()
                self._cache = [self._to_profile(c) for c in communities]
                self._last_refresh = datetime.now(timezone.utc)
        
        return self._cache
```

- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
  ```bash
  # backend/tests/services/test_community_pool_loader.py
  ```

- [ ] è¿è¡Œæµ‹è¯•
  ```bash
  pytest backend/tests/services/test_community_pool_loader.py -v
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥ä» JSON æ–‡ä»¶åŠ è½½ç§å­ç¤¾åŒº
- âœ… å¯ä»¥å¯¼å…¥åˆ°æ•°æ®åº“ï¼ˆå»é‡ï¼‰
- âœ… å¯ä»¥ä»æ•°æ®åº“åŠ è½½ç¤¾åŒºæ± ï¼ˆå¸¦ç¼“å­˜ï¼‰
- âœ… å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ç‡ > 80%ï¼‰

**è¾“å‡ºæ–‡ä»¶**:
- `backend/app/models/community_pool.py`
- `backend/app/services/community_pool_loader.py`
- `backend/tests/services/test_community_pool_loader.py`

---

#### ä»»åŠ¡ A3: ååŠ©ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 16:00-17:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] ä¸ Lead åä½œéªŒè¯ç§å­ç¤¾åŒºåç§°æœ‰æ•ˆæ€§
- [ ] ç¼–å†™éªŒè¯è„šæœ¬
  ```bash
  # backend/scripts/validate_seed_communities.py
  ```

- [ ] éªŒè¯ JSON æ–‡ä»¶æ ¼å¼
- [ ] éªŒè¯ç¤¾åŒºåç§°ï¼ˆè°ƒç”¨ Reddit APIï¼‰
- [ ] ç”ŸæˆéªŒè¯æŠ¥å‘Š

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ‰€æœ‰ç¤¾åŒºåç§°æœ‰æ•ˆ
- âœ… JSON æ ¼å¼æ­£ç¡®
- âœ… éªŒè¯æŠ¥å‘Šç”Ÿæˆ

**è¾“å‡ºæ–‡ä»¶**:
- `backend/scripts/validate_seed_communities.py`
- `backend/config/seed_communities_validation_report.json`

---

#### ä»»åŠ¡ A4: ä»£ç å®¡æŸ¥ä¸æ–‡æ¡£ï¼ˆä¼˜å…ˆçº§ P1ï¼‰

**æ—¶é—´**: 17:00-18:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] å®¡æŸ¥ Backend Agent B çš„çˆ¬è™«ä»»åŠ¡ä»£ç 
- [ ] æ›´æ–° API æ–‡æ¡£ï¼ˆå¦‚æœ‰æ–°å¢æ¥å£ï¼‰
- [ ] æ›´æ–° READMEï¼ˆå¦‚æœ‰æ–°å¢ä¾èµ–ï¼‰
- [ ] æäº¤ä»£ç å¹¶æ¨é€åˆ°è¿œç¨‹ä»“åº“

**éªŒæ”¶æ ‡å‡†**:
- âœ… ä»£ç å®¡æŸ¥å®Œæˆ
- âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ
- âœ… ä»£ç æäº¤æˆåŠŸ

---

## ğŸ‘¨â€ğŸ’» Backend Agent B - ä»»åŠ¡æ¸…å•

### ä¸Šåˆä»»åŠ¡ï¼ˆ9:00-12:00ï¼Œ3å°æ—¶ï¼‰

#### ä»»åŠ¡ B1: ç¯å¢ƒå‡†å¤‡ä¸é…ç½®ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 9:00-10:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] éªŒè¯ Celery ç¯å¢ƒ
  ```bash
  pip install celery[redis]
  ```

- [ ] éªŒè¯ Redis è¿æ¥
  ```bash
  redis-cli ping
  ```

- [ ] é…ç½® Celery åº”ç”¨
  ```python
  # backend/app/core/celery_app.py
  
  celery_app = Celery(
      "reddit_scanner",
      broker="redis://localhost:6379/0",
      backend="redis://localhost:6379/0"
  )
  
  celery_app.conf.update(
      task_serializer="json",
      accept_content=["json"],
      result_serializer="json",
      timezone="UTC",
      enable_utc=True,
  )
  ```

- [ ] æµ‹è¯• Celery è¿æ¥
  ```bash
  celery -A app.core.celery_app inspect ping
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Celery å®‰è£…æˆåŠŸ
- âœ… Redis è¿æ¥æ­£å¸¸
- âœ… Celery é…ç½®æ­£ç¡®

---

#### ä»»åŠ¡ B2: çˆ¬è™«ä»»åŠ¡å®ç°ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 10:00-12:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»ºçˆ¬è™«ä»»åŠ¡æ–‡ä»¶
  ```bash
  # backend/app/tasks/crawler_task.py
  ```

- [ ] å®ç° `crawl_community()` ä»»åŠ¡ï¼ˆçˆ¬å–å•ä¸ªç¤¾åŒºï¼‰
  ```python
  @celery_app.task(name="tasks.crawler.crawl_community", bind=True, max_retries=3)
  async def crawl_community(self, community_name: str):
      """
      çˆ¬å–å•ä¸ªç¤¾åŒº
      
      API è°ƒç”¨ï¼š1 æ¬¡
      """
      try:
          logger.info(f"å¼€å§‹çˆ¬å–ç¤¾åŒº: {community_name}")
          
          # 1. è°ƒç”¨ Reddit API
          posts = await reddit_client.fetch_subreddit_posts(
              community_name,
              limit=100,
              time_filter="week",
              sort="top"
          )
          
          # 2. æ›´æ–° Redis ç¼“å­˜
          cache_manager.set_cached_posts(
              community_name,
              posts,
              ttl_seconds=86400  # 24 å°æ—¶
          )
          
          # 3. æ›´æ–° CommunityCache è¡¨ï¼ˆå…ƒæ•°æ®ï¼‰
          await update_community_cache_metadata(
              community_name,
              posts_cached=len(posts),
              last_crawled_at=datetime.now(timezone.utc)
          )
          
          logger.info(f"âœ… {community_name}: ç¼“å­˜ {len(posts)} ä¸ªå¸–å­")
          
          return {
              "community": community_name,
              "posts_count": len(posts),
              "status": "success"
          }
          
      except Exception as e:
          logger.error(f"âŒ {community_name}: çˆ¬å–å¤±è´¥ - {e}")
          raise self.retry(exc=e, countdown=60)
  ```

- [ ] å®ç° `crawl_seed_communities()` ä»»åŠ¡ï¼ˆæ‰¹é‡çˆ¬å–ï¼‰
  ```python
  @celery_app.task(name="tasks.crawler.crawl_seed_communities")
  async def crawl_seed_communities():
      """
      çˆ¬å–ç§å­ç¤¾åŒºæ± ï¼ˆ50-100ä¸ªï¼‰
      
      ç­–ç•¥ï¼š
      - åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹ 12 ä¸ªç¤¾åŒºï¼‰
      - æ¯æ‰¹é—´éš” 30 åˆ†é’Ÿ
      - æ€» API è°ƒç”¨ï¼š12 æ¬¡/30åˆ†é’Ÿ = 0.4 æ¬¡/åˆ†é’Ÿ
      """
      loader = CommunityPoolLoader()
      seed_communities = await loader.load_community_pool()
      seed_communities = [c for c in seed_communities if c.tier == "seed"]
      
      logger.info(f"å¼€å§‹çˆ¬å– {len(seed_communities)} ä¸ªç§å­ç¤¾åŒº")
      
      # åˆ†æ‰¹çˆ¬å–ï¼ˆæ¯æ‰¹ 12 ä¸ªç¤¾åŒºï¼‰
      batch_size = 12
      for i in range(0, len(seed_communities), batch_size):
          batch = seed_communities[i:i+batch_size]
          
          # å¹¶å‘çˆ¬å–ï¼ˆæœ€å¤š 5 ä¸ªå¹¶å‘ï¼‰
          tasks = [
              crawl_community.apply_async(args=[community.name])
              for community in batch
          ]
          
          logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1}: çˆ¬å– {len(batch)} ä¸ªç¤¾åŒº")
          
          # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
          await asyncio.gather(*[AsyncResult(task.id).get() for task in tasks])
          
          # ç­‰å¾… 30 åˆ†é’Ÿåçˆ¬å–ä¸‹ä¸€æ‰¹
          if i + batch_size < len(seed_communities):
              logger.info("ç­‰å¾… 30 åˆ†é’Ÿåçˆ¬å–ä¸‹ä¸€æ‰¹...")
              await asyncio.sleep(1800)
      
      logger.info("âœ… ç§å­ç¤¾åŒºçˆ¬å–å®Œæˆ")
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥çˆ¬å–å•ä¸ªç¤¾åŒºå¹¶å­˜å…¥ Redis ç¼“å­˜
- âœ… å¯ä»¥æ‰¹é‡çˆ¬å–ç§å­ç¤¾åŒº
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶æ­£å¸¸

**è¾“å‡ºæ–‡ä»¶**:
- `backend/app/tasks/crawler_task.py`

---

### ä¸‹åˆä»»åŠ¡ï¼ˆ13:00-18:00ï¼Œ5å°æ—¶ï¼‰

#### ä»»åŠ¡ B3: Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 13:00-14:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] é…ç½® Celery Beat å®šæ—¶ä»»åŠ¡
  ```python
  # backend/app/core/celery_app.py
  
  celery_app.conf.beat_schedule = {
      # ç§å­ç¤¾åŒºçˆ¬è™«ï¼ˆæ¯ 30 åˆ†é’Ÿï¼‰
      'crawl-seed-communities': {
          'task': 'tasks.crawler.crawl_seed_communities',
          'schedule': crontab(minute='*/30'),
      },
  }
  ```

- [ ] æµ‹è¯•å®šæ—¶ä»»åŠ¡é…ç½®
  ```bash
  celery -A app.core.celery_app beat --loglevel=info --dry-run
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Celery Beat é…ç½®æ­£ç¡®
- âœ… å®šæ—¶ä»»åŠ¡å¯ä»¥æ­£å¸¸è°ƒåº¦

---

#### ä»»åŠ¡ B4: å¯åŠ¨é¢„çƒ­çˆ¬è™«ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 14:00-15:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»ºå¯åŠ¨è„šæœ¬
  ```bash
  # backend/scripts/start_warmup_crawler.sh
  ```

- [ ] å¯åŠ¨ Celery Beat
  ```bash
  celery -A app.core.celery_app beat --loglevel=info &
  ```

- [ ] å¯åŠ¨ Celery Worker
  ```bash
  celery -A app.core.celery_app worker --loglevel=info --concurrency=2 &
  ```

- [ ] æ‰‹åŠ¨è§¦å‘é¦–æ¬¡çˆ¬å–
  ```bash
  python backend/scripts/trigger_initial_crawl.py
  ```

- [ ] ç›‘æ§çˆ¬å–è¿›åº¦
  ```bash
  celery -A app.core.celery_app events
  ```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Celery Beat å’Œ Worker æ­£å¸¸è¿è¡Œ
- âœ… é¦–æ¬¡çˆ¬å–æˆåŠŸ
- âœ… 50-100 ä¸ªç§å­ç¤¾åŒºå·²çˆ¬å–
- âœ… Redis ç¼“å­˜å‘½ä¸­ç‡ï¼š100%ï¼ˆç§å­ç¤¾åŒºï¼‰

**è¾“å‡ºæ–‡ä»¶**:
- `backend/scripts/start_warmup_crawler.sh`
- `backend/scripts/trigger_initial_crawl.py`

---

#### ä»»åŠ¡ B5: ç›‘æ§ç³»ç»Ÿæ­å»ºï¼ˆä¼˜å…ˆçº§ P1ï¼‰

**æ—¶é—´**: 15:00-17:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] åˆ›å»ºç›‘æ§ä»»åŠ¡æ–‡ä»¶
  ```bash
  # backend/app/tasks/monitoring_task.py
  ```

- [ ] å®ç° API è°ƒç”¨ç›‘æ§
  ```python
  @celery_app.task(name="tasks.monitoring.monitor_api_calls")
  async def monitor_api_calls():
      """ç›‘æ§ Reddit API è°ƒç”¨æ¬¡æ•°"""
      # ä» Redis è·å–æœ€è¿‘ 1 åˆ†é’Ÿçš„ API è°ƒç”¨æ¬¡æ•°
      calls_per_minute = await redis_client.get("api_calls_per_minute")
      
      if calls_per_minute and int(calls_per_minute) > 55:
          logger.warning(f"âš ï¸ API è°ƒç”¨æ¥è¿‘é™åˆ¶: {calls_per_minute}/60")
          # å‘é€å‘Šè­¦
          await send_alert(
              level="warning",
              message=f"API è°ƒç”¨æ¥è¿‘é™åˆ¶: {calls_per_minute}/60"
          )
  ```

- [ ] å®ç°ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§
- [ ] å®ç°çˆ¬è™«å¥åº·æ£€æŸ¥
- [ ] é…ç½®å‘Šè­¦æœºåˆ¶ï¼ˆAPI è°ƒç”¨ > 55 æ¬¡/åˆ†é’Ÿï¼‰
- [ ] é…ç½®å®šæ—¶ç›‘æ§ä»»åŠ¡ï¼ˆæ¯åˆ†é’Ÿï¼‰

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥å®æ—¶ç›‘æ§ API è°ƒç”¨æ¬¡æ•°
- âœ… å¯ä»¥å®æ—¶ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡
- âœ… å‘Šè­¦æœºåˆ¶æ­£å¸¸å·¥ä½œ

**è¾“å‡ºæ–‡ä»¶**:
- `backend/app/tasks/monitoring_task.py`

---

#### ä»»åŠ¡ B6: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£ï¼ˆä¼˜å…ˆçº§ P1ï¼‰

**æ—¶é—´**: 17:00-18:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] ç¼–å†™é›†æˆæµ‹è¯•
  ```bash
  # backend/tests/tasks/test_crawler_task.py
  ```

- [ ] è¿è¡Œé›†æˆæµ‹è¯•
  ```bash
  pytest backend/tests/tasks/test_crawler_task.py -v
  ```

- [ ] æ›´æ–°çˆ¬è™«ç³»ç»Ÿæ–‡æ¡£
- [ ] æäº¤ä»£ç å¹¶æ¨é€åˆ°è¿œç¨‹ä»“åº“

**éªŒæ”¶æ ‡å‡†**:
- âœ… é›†æˆæµ‹è¯•é€šè¿‡
- âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ
- âœ… ä»£ç æäº¤æˆåŠŸ

**è¾“å‡ºæ–‡ä»¶**:
- `backend/tests/tasks/test_crawler_task.py`
- `docs/CRAWLER_SYSTEM.md`

---

## ğŸ‘¨â€ğŸ’» Frontend Agent - ä»»åŠ¡æ¸…å•

### å…¨å¤©ä»»åŠ¡ï¼ˆ9:00-18:00ï¼‰

#### ä»»åŠ¡ F1: å­¦ä¹ ä¸å‡†å¤‡ï¼ˆä¼˜å…ˆçº§ P2ï¼‰

**æ—¶é—´**: 9:00-12:00ï¼ˆ3å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] é˜…è¯» PRD-09ï¼ˆåŠ¨æ€ç¤¾åŒºæ± ä¸é¢„çƒ­æœŸå®æ–½è®¡åˆ’ï¼‰
- [ ] é˜…è¯» PRD-03 æ›´æ–°å†…å®¹ï¼ˆåå°çˆ¬è™«ç³»ç»Ÿï¼‰
- [ ] äº†è§£ç¤¾åŒºæ± æ¶æ„å’Œé¢„çƒ­æœŸè®¡åˆ’
- [ ] å‡†å¤‡ Beta æµ‹è¯•æ³¨å†Œé¡µé¢è®¾è®¡ç¨¿

**éªŒæ”¶æ ‡å‡†**:
- âœ… ç†è§£åŠ¨æ€ç¤¾åŒºæ± æ¶æ„
- âœ… ç†è§£é¢„çƒ­æœŸè®¡åˆ’
- âœ… Beta æµ‹è¯•æ³¨å†Œé¡µé¢è®¾è®¡ç¨¿å®Œæˆ

---

#### ä»»åŠ¡ F2: ç¯å¢ƒå‡†å¤‡ï¼ˆä¼˜å…ˆçº§ P2ï¼‰

**æ—¶é—´**: 13:00-15:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] éªŒè¯å‰ç«¯å¼€å‘ç¯å¢ƒ
  ```bash
  cd frontend
  npm install
  npm run dev
  ```

- [ ] éªŒè¯ä¸åç«¯ API è¿æ¥
- [ ] å‡†å¤‡ Beta æµ‹è¯•æ³¨å†Œé¡µé¢ç»„ä»¶

**éªŒæ”¶æ ‡å‡†**:
- âœ… å‰ç«¯å¼€å‘ç¯å¢ƒæ­£å¸¸
- âœ… å¯ä»¥è¿æ¥åç«¯ API
- âœ… ç»„ä»¶å‡†å¤‡å®Œæˆ

---

#### ä»»åŠ¡ F3: ä»£ç å®¡æŸ¥ä¸å­¦ä¹ ï¼ˆä¼˜å…ˆçº§ P2ï¼‰

**æ—¶é—´**: 15:00-18:00ï¼ˆ3å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] å®¡æŸ¥ç°æœ‰å‰ç«¯ä»£ç 
- [ ] å­¦ä¹  SSE å®¢æˆ·ç«¯å®ç°
- [ ] å‡†å¤‡ Day 15-16 çš„å¼€å‘ä»»åŠ¡

**éªŒæ”¶æ ‡å‡†**:
- âœ… ä»£ç å®¡æŸ¥å®Œæˆ
- âœ… ç†è§£ SSE å®¢æˆ·ç«¯å®ç°
- âœ… Day 15-16 ä»»åŠ¡å‡†å¤‡å®Œæˆ

---

## ğŸ‘¨â€ğŸ’¼ Lead - ä»»åŠ¡æ¸…å•

### å…¨å¤©ä»»åŠ¡ï¼ˆ9:00-18:00ï¼‰

#### ä»»åŠ¡ L1: ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 9:00-11:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] å‡†å¤‡ 50-100 ä¸ªç§å­ç¤¾åŒºåˆ—è¡¨
- [ ] åˆ†ç±»ç¤¾åŒºï¼ˆåˆ›ä¸šã€äº§å“ã€æŠ€æœ¯ã€è¥é”€ã€è®¾è®¡ã€é€šç”¨ï¼‰
- [ ] å¡«å……ç¤¾åŒºå…ƒæ•°æ®
  - categoriesï¼ˆç±»åˆ«ï¼‰
  - description_keywordsï¼ˆå…³é”®è¯ï¼‰
  - daily_postsï¼ˆæ¯æ—¥å¸–å­æ•°ï¼Œä¼°ç®—ï¼‰
  - avg_comment_lengthï¼ˆå¹³å‡è¯„è®ºé•¿åº¦ï¼Œä¼°ç®—ï¼‰
  - quality_scoreï¼ˆè´¨é‡åˆ†æ•°ï¼Œåˆå§‹ 0.8-0.95ï¼‰

**ç§å­ç¤¾åŒºåˆ†ç±»å»ºè®®**:
```json
{
  "version": "1.0",
  "last_updated": "2025-10-14",
  "seed_communities": [
    {
      "name": "r/startups",
      "tier": "seed",
      "categories": ["startup", "business", "founder"],
      "description_keywords": ["startup", "founder", "product", "launch", "mvp", "funding"],
      "daily_posts": 200,
      "avg_comment_length": 150,
      "quality_score": 0.95
    },
    {
      "name": "r/Entrepreneur",
      "tier": "seed",
      "categories": ["startup", "business", "entrepreneur"],
      "description_keywords": ["entrepreneur", "business", "growth", "revenue", "marketing"],
      "daily_posts": 180,
      "avg_comment_length": 140,
      "quality_score": 0.90
    }
    // ... å…± 50-100 ä¸ª
  ]
}
```

- [ ] ä¿å­˜åˆ° `backend/config/seed_communities.json`
- [ ] ä¸ Backend Agent A åä½œéªŒè¯æ•°æ®

**éªŒæ”¶æ ‡å‡†**:
- âœ… è‡³å°‘ 50 ä¸ªç§å­ç¤¾åŒº
- âœ… JSON æ ¼å¼æ­£ç¡®
- âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå®Œæ•´
- âœ… ç¤¾åŒºåç§°æœ‰æ•ˆ

**è¾“å‡ºæ–‡ä»¶**:
- `backend/config/seed_communities.json`

---

#### ä»»åŠ¡ L2: é¡¹ç›®åè°ƒä¸ç›‘ç£ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 11:00-12:00, 14:00-15:00ï¼ˆ2å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] 11:00 ä¸»æŒ Stand-up ä¼šè®®ï¼ˆ15 åˆ†é’Ÿï¼‰
  - Backend Agent A æ±‡æŠ¥æ•°æ®åº“è¿ç§»è¿›åº¦
  - Backend Agent B æ±‡æŠ¥ç¯å¢ƒå‡†å¤‡è¿›åº¦
  - Frontend Agent æ±‡æŠ¥å­¦ä¹ è¿›åº¦

- [ ] ç›‘ç£å„å›¢é˜Ÿè¿›åº¦
- [ ] è§£å†³é˜»å¡é—®é¢˜
- [ ] åè°ƒèµ„æºåˆ†é…

- [ ] 14:00 ä¸­æœŸæ£€æŸ¥ï¼ˆ15 åˆ†é’Ÿï¼‰
  - ç¡®è®¤æ•°æ®åº“è¿ç§»å®Œæˆ
  - ç¡®è®¤ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡å®Œæˆ
  - ç¡®è®¤çˆ¬è™«ä»»åŠ¡å®ç°è¿›åº¦

**éªŒæ”¶æ ‡å‡†**:
- âœ… Stand-up ä¼šè®®å®Œæˆ
- âœ… ä¸­æœŸæ£€æŸ¥å®Œæˆ
- âœ… æ— é˜»å¡é—®é¢˜

---

#### ä»»åŠ¡ L3: éªŒæ”¶ä¸æ€»ç»“ï¼ˆä¼˜å…ˆçº§ P0ï¼‰

**æ—¶é—´**: 17:00-18:00ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡æ¸…å•**:
- [ ] 17:00 ä¸»æŒ Stand-down ä¼šè®®ï¼ˆ30 åˆ†é’Ÿï¼‰
  - Backend Agent A æ¼”ç¤ºæ•°æ®åº“è¿ç§»å’Œç¤¾åŒºæ± åŠ è½½å™¨
  - Backend Agent B æ¼”ç¤ºçˆ¬è™«ç³»ç»Ÿè¿è¡ŒçŠ¶æ€
  - Frontend Agent æ±‡æŠ¥å‡†å¤‡æƒ…å†µ

- [ ] éªŒæ”¶ Day 13 æˆæœ
  - æ•°æ®åº“è¿ç§»å®Œæˆ âœ…
  - ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡å®Œæˆ âœ…
  - ç¤¾åŒºæ± åŠ è½½å™¨å®ç°å®Œæˆ âœ…
  - çˆ¬è™«ä»»åŠ¡å®ç°å®Œæˆ âœ…
  - é¢„çƒ­çˆ¬è™«å¯åŠ¨å¹¶è¿è¡Œ âœ…
  - ç›‘æ§ç³»ç»Ÿæ­å»ºå®Œæˆ âœ…

- [ ] è®°å½• Day 13 è¿›åº¦åˆ° `reports/phase-log/day13-progress.md`
- [ ] å‡†å¤‡ Day 14 ä»»åŠ¡åˆ†é…

**éªŒæ”¶æ ‡å‡†**:
- âœ… Stand-down ä¼šè®®å®Œæˆ
- âœ… æ‰€æœ‰ P0 ä»»åŠ¡å®Œæˆ
- âœ… Day 13 è¿›åº¦è®°å½•å®Œæˆ
- âœ… Day 14 ä»»åŠ¡åˆ†é…å®Œæˆ

**è¾“å‡ºæ–‡ä»¶**:
- `reports/phase-log/day13-progress.md`
- `reports/phase-log/day14-task-assignment.md`

---

## ğŸ“Š Day 13 éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆï¼ˆP0ï¼‰

- âœ… æ•°æ®åº“è¿ç§»å®Œæˆï¼ˆ`community_pool` å’Œ `pending_communities` è¡¨ï¼‰
- âœ… ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡å®Œæˆï¼ˆ50-100 ä¸ªç¤¾åŒº JSON æ–‡ä»¶ï¼‰
- âœ… ç¤¾åŒºæ± åŠ è½½å™¨å®ç°å®Œæˆ
- âœ… çˆ¬è™«ä»»åŠ¡å®ç°å®Œæˆ
- âœ… é¢„çƒ­çˆ¬è™«å¯åŠ¨å¹¶è¿è¡Œ
- âœ… 50-100 ä¸ªç§å­ç¤¾åŒºå·²çˆ¬å–
- âœ… Redis ç¼“å­˜å‘½ä¸­ç‡ï¼š100%ï¼ˆç§å­ç¤¾åŒºï¼‰

### åº”è¯¥å®Œæˆï¼ˆP1ï¼‰

- âœ… ç›‘æ§ç³»ç»Ÿæ­å»ºå®Œæˆ
- âœ… å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•é€šè¿‡
- âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ
- âœ… ä»£ç æäº¤å¹¶æ¨é€åˆ°è¿œç¨‹ä»“åº“

### å¯é€‰å®Œæˆï¼ˆP2ï¼‰

- âœ… Frontend Agent å®Œæˆå­¦ä¹ å’Œå‡†å¤‡
- âœ… Beta æµ‹è¯•æ³¨å†Œé¡µé¢è®¾è®¡ç¨¿å®Œæˆ

---

## ğŸ“… æ—¶é—´çº¿æ€»è§ˆ

```
09:00-09:15  Stand-up ä¼šè®®ï¼ˆLead ä¸»æŒï¼‰
09:15-11:00  Backend A: æ•°æ®åº“è¿ç§»
             Backend B: ç¯å¢ƒå‡†å¤‡
             Lead: ç§å­ç¤¾åŒºæ•°æ®å‡†å¤‡
             Frontend: å­¦ä¹  PRD

11:00-12:00  Backend A: æ•°æ®åº“è¿ç§»éªŒè¯
             Backend B: çˆ¬è™«ä»»åŠ¡å®ç°
             Lead: åè°ƒç›‘ç£
             Frontend: å­¦ä¹  PRD

12:00-13:00  åˆä¼‘

13:00-14:00  Backend A: ç¤¾åŒºæ± åŠ è½½å™¨å®ç°
             Backend B: Celery Beat é…ç½®
             Lead: åè°ƒç›‘ç£
             Frontend: ç¯å¢ƒå‡†å¤‡

14:00-14:15  ä¸­æœŸæ£€æŸ¥ï¼ˆLead ä¸»æŒï¼‰

14:15-15:00  Backend A: ç¤¾åŒºæ± åŠ è½½å™¨å®ç°
             Backend B: å¯åŠ¨é¢„çƒ­çˆ¬è™«
             Lead: åè°ƒç›‘ç£
             Frontend: ç¯å¢ƒå‡†å¤‡

15:00-17:00  Backend A: å•å…ƒæµ‹è¯•
             Backend B: ç›‘æ§ç³»ç»Ÿæ­å»º
             Lead: å‡†å¤‡éªŒæ”¶
             Frontend: ä»£ç å®¡æŸ¥

17:00-17:30  Stand-down ä¼šè®®ï¼ˆLead ä¸»æŒï¼‰

17:30-18:00  Backend A: ä»£ç å®¡æŸ¥ä¸æ–‡æ¡£
             Backend B: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£
             Lead: è®°å½•è¿›åº¦ï¼Œå‡†å¤‡ Day 14
             Frontend: å‡†å¤‡ Day 15-16 ä»»åŠ¡
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¶é—´**: 2025-10-14  
**è´Ÿè´£äºº**: Lead Agent  
**çŠ¶æ€**: âœ… å‡†å¤‡æ‰§è¡Œ

