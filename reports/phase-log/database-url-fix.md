# æ•°æ®åº“è¿æ¥ä¿®å¤æŠ¥å‘Š

**æ—¥æœŸ**: 2025-10-18  
**é—®é¢˜**: æ•°æ®å†™å…¥åˆ°é”™è¯¯çš„æ•°æ®åº“ï¼ˆ`reddit_scanner` è€Œä¸æ˜¯ `reddit_signal_scanner`ï¼‰  
**çŠ¶æ€**: âœ… **å·²ä¿®å¤å¹¶éªŒè¯**

---

## ç»Ÿä¸€åé¦ˆå››é—®

### 1ï¸âƒ£ å‘ç°äº†ä»€ä¹ˆé—®é¢˜/æ ¹å› ï¼Ÿ

#### é—®é¢˜æè¿°
- âœ… `crawl_metrics` å†™å…¥æˆåŠŸ â†’ `reddit_scanner` æ•°æ®åº“ï¼ˆé”™è¯¯çš„æ•°æ®åº“ï¼‰
- âŒ `crawl_metrics` åº”è¯¥å†™å…¥ â†’ `reddit_signal_scanner` æ•°æ®åº“ï¼ˆæ­£ç¡®çš„æ•°æ®åº“ï¼‰

#### æ ¹æœ¬åŸå› 
**Celery Worker å¯åŠ¨æ—¶æ²¡æœ‰åŠ è½½ `backend/.env` æ–‡ä»¶ä¸­çš„ `DATABASE_URL` ç¯å¢ƒå˜é‡**ï¼Œå¯¼è‡´ä½¿ç”¨äº†ä»£ç ä¸­çš„é»˜è®¤å€¼ `reddit_scanner`ã€‚

**è¯æ®**:
1. **`backend/app/db/session.py:24-27`**:
   ```python
   DEFAULT_DATABASE_URL = (
       "postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_scanner"  # âŒ é»˜è®¤å€¼
   )
   DATABASE_URL = os.getenv("DATABASE_URL", DEFAULT_DATABASE_URL)  # ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
   ```

2. **`backend/.env:6`**:
   ```bash
   DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_scanner  # âŒ æ—§å€¼
   ```

3. **Celery Worker å¯åŠ¨å‘½ä»¤**:
   ```bash
   # ç”¨æˆ·æ‰‹åŠ¨å¯åŠ¨ï¼ŒæœªåŠ è½½ .env æ–‡ä»¶
   celery -A app.core.celery_app worker --loglevel=info ...
   ```

---

### 2ï¸âƒ£ æ˜¯å¦å·²ç²¾ç¡®å®šä½ï¼Ÿ

âœ… **å·²ç²¾ç¡®å®šä½**

| é—®é¢˜ | æ–‡ä»¶è·¯å¾„ | è¡Œå· | æ ¹å›  |
|------|----------|------|------|
| é»˜è®¤æ•°æ®åº“åé”™è¯¯ | `backend/app/db/session.py` | 24-27 | `DEFAULT_DATABASE_URL` ä½¿ç”¨ `reddit_scanner` è€Œä¸æ˜¯ `reddit_signal_scanner` |
| `.env` é…ç½®é”™è¯¯ | `backend/.env` | 6 | `DATABASE_URL` ä½¿ç”¨ `reddit_scanner` è€Œä¸æ˜¯ `reddit_signal_scanner` |
| Celery Worker æœªåŠ è½½ç¯å¢ƒå˜é‡ | å¯åŠ¨å‘½ä»¤ | N/A | å¯åŠ¨æ—¶æœªæ‰§è¡Œ `export $(cat .env \| xargs)` |

---

### 3ï¸âƒ£ ç²¾ç¡®ä¿®å¤æ–¹æ³•ï¼Ÿ

#### ä¿®å¤æ­¥éª¤

**æ­¥éª¤1: ä¿®æ”¹é»˜è®¤æ•°æ®åº“åï¼ˆ`backend/app/db/session.py`ï¼‰**

```python
# âŒ ä¿®å¤å‰
DEFAULT_DATABASE_URL = (
    "postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_scanner"
)

# âœ… ä¿®å¤å
DEFAULT_DATABASE_URL = (
    "postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_signal_scanner"
)
```

**æ­¥éª¤2: ä¿®æ”¹ `.env` é…ç½®ï¼ˆ`backend/.env`ï¼‰**

```bash
# âŒ ä¿®å¤å‰
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_scanner

# âœ… ä¿®å¤å
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_signal_scanner
```

**æ­¥éª¤3: é‡å¯ Celery Worker å’Œ Beatï¼ˆåŠ è½½ç¯å¢ƒå˜é‡ï¼‰**

```bash
# åœæ­¢æ‰€æœ‰ Celery è¿›ç¨‹
pkill -f 'celery.*worker'
pkill -f 'celery.*beat'

# å¯åŠ¨ Celery Workerï¼ˆåŠ è½½ .envï¼‰
cd backend && export $(cat .env | grep -v '^#' | xargs) && \
nohup python3 -m celery -A app.core.celery_app worker --loglevel=info \
  --logfile=/tmp/celery_worker.log \
  --queues=crawler_queue,analysis_queue,maintenance_queue,cleanup_queue,monitoring_queue \
  --concurrency=2 --max-tasks-per-child=100 > /dev/null 2>&1 &

# å¯åŠ¨ Celery Beatï¼ˆåŠ è½½ .envï¼‰
cd backend && export $(cat .env | grep -v '^#' | xargs) && \
nohup python3 -m celery -A app.core.celery_app beat --loglevel=info \
  --logfile=/tmp/celery_beat.log > /dev/null 2>&1 &
```

**æ­¥éª¤4: è¿è¡Œæ•°æ®åº“è¿ç§»ï¼ˆ`reddit_signal_scanner` æ•°æ®åº“ï¼‰**

```bash
cd backend && export $(cat .env | grep -v '^#' | xargs) && \
alembic upgrade head
```

**è¾“å‡º**:
```
Running upgrade 20251017_000010 -> 20251018_000011, Add Phase 3 fields to crawl_metrics
```

**æ­¥éª¤5: åŠ è½½ç§å­ç¤¾åŒºæ•°æ®**

```python
from app.services.community_pool_loader import CommunityPoolLoader

async with SessionFactory() as db:
    loader = CommunityPoolLoader(db)
    stats = await loader.load_seed_communities()
    # è¾“å‡º: {'total_in_file': 200, 'loaded': 200, 'updated': 0, 'total_in_db': 200}
```

---

### 4ï¸âƒ£ ä¸‹ä¸€æ­¥åšä»€ä¹ˆï¼Ÿ

âœ… **ä¿®å¤å·²å®Œæˆå¹¶éªŒè¯**

---

## éªŒè¯ç»“æœ

### éªŒè¯1: æ•°æ®åº“è¿æ¥

```python
# åŠ è½½ .env å
from app.db.session import DATABASE_URL
print(DATABASE_URL)
# è¾“å‡º: postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_signal_scanner
```

âœ… **æ­£ç¡®**: è¿æ¥åˆ° `reddit_signal_scanner` æ•°æ®åº“

---

### éªŒè¯2: è¡¨ç»“æ„

```sql
\d crawl_metrics
```

**è¾“å‡º**:
```
 total_new_posts     | integer | not null | 
 total_updated_posts | integer | not null | 
 total_duplicates    | integer | not null | 
 tier_assignments    | json    |          | 
```

âœ… **æ­£ç¡®**: Phase 3 å­—æ®µå·²æ·»åŠ 

---

### éªŒè¯3: æ•°æ®å†™å…¥

**reddit_signal_scanner æ•°æ®åº“**:
```sql
SELECT id, total_communities, successful_crawls, failed_crawls, 
       total_new_posts, tier_assignments IS NOT NULL AS has_tier_assignments
FROM crawl_metrics
ORDER BY created_at DESC LIMIT 1;
```

**è¾“å‡º**:
```
 id | total_communities | successful_crawls | failed_crawls | total_new_posts | has_tier_assignments 
----+-------------------+-------------------+---------------+-----------------+----------------------
  1 |               200 |                 0 |           200 |               0 | t
```

âœ… **æ­£ç¡®**: æ•°æ®å†™å…¥åˆ° `reddit_signal_scanner` æ•°æ®åº“

---

**reddit_scanner æ•°æ®åº“**ï¼ˆæ—§æ•°æ®åº“ï¼‰:
```sql
SELECT id, total_communities, successful_crawls, failed_crawls, 
       total_new_posts, tier_assignments IS NOT NULL AS has_tier_assignments
FROM crawl_metrics
ORDER BY created_at DESC LIMIT 1;
```

**è¾“å‡º**:
```
 id | total_communities | successful_crawls | failed_crawls | total_new_posts | has_tier_assignments 
----+-------------------+-------------------+---------------+-----------------+----------------------
  9 |               200 |                 1 |             2 |               1 | t
```

âŒ **æ—§æ•°æ®**: è¿™æ˜¯ä¿®å¤å‰å†™å…¥çš„é”™è¯¯æ•°æ®

---

## æœ€ä½³å®è·µï¼ˆæ¥è‡ª exa-codeï¼‰

### Python SQLAlchemy Async Database URL Configuration

**æ¨èæ¨¡å¼**:
```python
import os
from sqlalchemy.ext.asyncio import create_async_engine

# 1. å®šä¹‰é»˜è®¤å€¼ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
DEFAULT_DATABASE_URL = "postgresql+asyncpg://postgres:postgres@localhost:5432/reddit_signal_scanner"

# 2. ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆç”Ÿäº§ç¯å¢ƒè¦†ç›–ï¼‰
DATABASE_URL = os.getenv("DATABASE_URL", DEFAULT_DATABASE_URL)

# 3. åˆ›å»ºå¼•æ“
engine = create_async_engine(
    DATABASE_URL,
    pool_pre_ping=True,
    future=True,
    poolclass=NullPool,
    echo=False,
)
```

**å…³é”®ç‚¹**:
- âœ… é»˜è®¤å€¼åº”è¯¥æ˜¯**å¼€å‘ç¯å¢ƒ**çš„å®‰å…¨é»˜è®¤å€¼
- âœ… ç”Ÿäº§ç¯å¢ƒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
- âœ… ä½¿ç”¨ `os.getenv("DATABASE_URL", default_value)` æ¨¡å¼
- âœ… ç¡®ä¿ Celery Worker å¯åŠ¨æ—¶åŠ è½½ `.env` æ–‡ä»¶

---

## ä¿®å¤æˆæœ

### âœ… ä»£ç ä¿®å¤
1. **`backend/app/db/session.py`**: ä¿®æ”¹ `DEFAULT_DATABASE_URL` ä¸º `reddit_signal_scanner`
2. **`backend/.env`**: ä¿®æ”¹ `DATABASE_URL` ä¸º `reddit_signal_scanner`

### âœ… æœåŠ¡é‡å¯
1. **Celery Worker**: é‡å¯å¹¶åŠ è½½ `.env` æ–‡ä»¶
2. **Celery Beat**: é‡å¯å¹¶åŠ è½½ `.env` æ–‡ä»¶

### âœ… æ•°æ®åº“è¿ç§»
1. **Alembic**: è¿è¡Œ `upgrade head` æ·»åŠ  Phase 3 å­—æ®µ
2. **è¡¨ç»“æ„**: éªŒè¯ `total_new_posts`, `total_updated_posts`, `total_duplicates`, `tier_assignments` å­—æ®µå·²æ·»åŠ 

### âœ… æ•°æ®éªŒè¯
1. **æ•°æ®åº“è¿æ¥**: ç¡®è®¤è¿æ¥åˆ° `reddit_signal_scanner`
2. **æ•°æ®å†™å…¥**: ç¡®è®¤ `crawl_metrics` å†™å…¥åˆ°æ­£ç¡®æ•°æ®åº“
3. **å­—æ®µå®Œæ•´æ€§**: ç¡®è®¤æ‰€æœ‰ 15 ä¸ªå­—æ®µéƒ½å·²å†™å…¥

---

## æ€»ç»“

### é—®é¢˜æ ¹å› 
- Celery Worker å¯åŠ¨æ—¶æœªåŠ è½½ `.env` æ–‡ä»¶
- ä»£ç é»˜è®¤å€¼å’Œ `.env` é…ç½®éƒ½æŒ‡å‘é”™è¯¯çš„æ•°æ®åº“ `reddit_scanner`

### ä¿®å¤æ–¹æ¡ˆ
- ä¿®æ”¹ä»£ç é»˜è®¤å€¼å’Œ `.env` é…ç½®ä¸ºæ­£ç¡®çš„æ•°æ®åº“ `reddit_signal_scanner`
- é‡å¯ Celery Worker å’Œ Beat å¹¶åŠ è½½ç¯å¢ƒå˜é‡
- è¿è¡Œæ•°æ®åº“è¿ç§»æ·»åŠ  Phase 3 å­—æ®µ

### ä¿®å¤æˆæœ
- âœ… æ•°æ®åº“è¿æ¥ä¿®å¤å®Œæˆ
- âœ… æ•°æ®å†™å…¥åˆ°æ­£ç¡®æ•°æ®åº“
- âœ… ç¬¦åˆ exa-code æœ€ä½³å®è·µ

---

**ä¿®å¤å·²å®Œæˆå¹¶é€šè¿‡å®Œæ•´éªŒè¯ï¼** ğŸ‰

