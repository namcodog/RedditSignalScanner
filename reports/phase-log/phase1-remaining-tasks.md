# Phase 1 æœªå®Œæˆä»»åŠ¡æ¸…å•

**æ–‡æ¡£æ—¥æœŸ**: 2025-10-17  
**é¡¹ç›®**: Reddit Signal Scanner - æ•°æ®ä¸ç®—æ³•åŒè½¨ä¼˜åŒ–  
**çŠ¶æ€**: Phase 1 å·²å®Œæˆï¼ŒPhase 2 å¾…å¯åŠ¨

---

## ğŸ“‹ è¯´æ˜

Phase 1 çš„ 8 ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼ˆT1.1-T1.8ï¼‰å·²å…¨éƒ¨å®Œæˆã€‚æœ¬æ–‡æ¡£åˆ—å‡ºçš„æ˜¯**ä¼˜åŒ–é¡¹**å’Œ**Phase 2 å‡†å¤‡å·¥ä½œ**ï¼Œéé˜»å¡æ€§ä»»åŠ¡ã€‚

---

## 1. Phase 1 ä¼˜åŒ–é¡¹ï¼ˆå¯é€‰ï¼‰

### 1.1 ç¤¾åŒºæ± æ‰©å®¹è‡³ç›®æ ‡å®¹é‡

**ä»»åŠ¡ç¼–å·**: T1.5-è¡¥å……  
**ä¼˜å…ˆçº§**: ä¸­  
**é¢„è®¡å·¥æ—¶**: 2 å°æ—¶  
**å½“å‰çŠ¶æ€**: 200/300 (67%)

**ä»»åŠ¡æè¿°**:
è¡¥å…… 100 ä¸ªé«˜è´¨é‡ç¤¾åŒºï¼Œè¾¾åˆ°ç›®æ ‡å®¹é‡ 300 ä¸ªã€‚

**æ‰§è¡Œæ­¥éª¤**:
1. ä» Reddit ç­›é€‰ 100 ä¸ªé«˜è´¨é‡ç¤¾åŒºï¼ˆsubscribers > 50Kï¼Œæ´»è·ƒåº¦é«˜ï¼‰
2. æ›´æ–° `backend/data/community_expansion_300.json`
3. è¿è¡Œå¯¼å…¥è„šæœ¬ï¼š
   ```bash
   PYTHONPATH=backend python3 scripts/import_community_expansion.py
   ```
4. éªŒè¯ç¤¾åŒºæ•°é‡ï¼š
   ```sql
   SELECT COUNT(*) FROM community_pool WHERE is_active = true;
   ```

**éªŒæ”¶æ ‡å‡†**:
- ç¤¾åŒºæ€»æ•° â‰¥ 300
- æ–°å¢ç¤¾åŒº quality_score â‰¥ 0.60
- ç±»ç›®åˆ†å¸ƒå‡è¡¡ï¼ˆtech/business/finance/lifestyleï¼‰

---

### 1.2 åˆ›å»ºé»‘åå•é…ç½®æ–‡ä»¶

**ä»»åŠ¡ç¼–å·**: T1.6-è¡¥å……  
**ä¼˜å…ˆçº§**: ä½  
**é¢„è®¡å·¥æ—¶**: 30 åˆ†é’Ÿ  
**å½“å‰çŠ¶æ€**: æ•°æ®åº“å­—æ®µå·²åˆ›å»ºï¼Œé…ç½®æ–‡ä»¶ç¼ºå¤±

**ä»»åŠ¡æè¿°**:
åˆ›å»º `config/community_blacklist.yaml` é…ç½®æ–‡ä»¶ï¼Œå¯ç”¨é»‘åå•åŠŸèƒ½ã€‚

**æ‰§è¡Œæ­¥éª¤**:
1. åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š
   ```bash
   mkdir -p config
   touch config/community_blacklist.yaml
   ```

2. å¡«å†™é…ç½®å†…å®¹ï¼š
   ```yaml
   # å®Œå…¨é»‘åå•ï¼ˆä¸æŠ“å–ï¼‰
   blacklist:
     - name: "spam_community"
       reason: "åƒåœ¾å†…å®¹"
       penalty: 100
     - name: "nsfw_community"
       reason: "ä¸é€‚åˆå†…å®¹"
       penalty: 100

   # é™æƒç¤¾åŒºï¼ˆé™ä½ä¼˜å…ˆçº§ï¼‰
   downgrade:
     - name: "low_quality_community"
       reason: "è´¨é‡ä¸‹é™"
       penalty: 30
     - name: "inactive_community"
       reason: "æ´»è·ƒåº¦ä½"
       penalty: 20

   # ç™½åå•ï¼ˆå¼ºåˆ¶æŠ“å–ï¼‰
   whitelist:
     - name: "high_value_community"
       reason: "é«˜ä»·å€¼ç¤¾åŒº"
       bonus: 50
   ```

3. åº”ç”¨é…ç½®ï¼š
   ```bash
   PYTHONPATH=backend python3 scripts/apply_blacklist_config.py
   ```

**éªŒæ”¶æ ‡å‡†**:
- é…ç½®æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
- é»‘åå•ç¤¾åŒº `is_blacklisted = true`
- é™æƒç¤¾åŒº `priority_penalty > 0`
- æ—¥å¿—ä¸å†æ˜¾ç¤º "é»‘åå•é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"

---

### 1.3 ä¼˜åŒ– crawl_metrics å†™å…¥é€»è¾‘

**ä»»åŠ¡ç¼–å·**: T1.3-ä¼˜åŒ–  
**ä¼˜å…ˆçº§**: ä½  
**é¢„è®¡å·¥æ—¶**: 1 å°æ—¶  
**å½“å‰çŠ¶æ€**: å¶å‘å†™å…¥å¤±è´¥ï¼ˆéé˜»å¡ï¼‰

**ä»»åŠ¡æè¿°**:
ä¼˜åŒ– `_record_crawl_metrics()` å‡½æ•°ï¼Œå‡å°‘æ•°æ®åº“è¿æ¥æ± ç«äº‰ã€‚

**é—®é¢˜ç°è±¡**:
```
ERROR app.tasks.crawler_task:crawler_task.py:236 å†™å…¥ crawl_metrics å¤±è´¥
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
1. ä½¿ç”¨ç‹¬ç«‹çš„ DB sessionï¼ˆé¿å…ä¸ä¸»æŠ“å–æµç¨‹å…±äº«ï¼‰
2. å¢åŠ é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
3. å¼‚æ­¥å†™å…¥ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰

**ä»£ç ä¿®æ”¹**:
```python
# backend/app/tasks/crawler_task.py

async def _record_crawl_metrics(...):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with SessionFactory() as metrics_db:  # ç‹¬ç«‹ session
                # ... å†™å…¥é€»è¾‘
                await metrics_db.commit()
            break
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"å†™å…¥ crawl_metrics å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
            else:
                await asyncio.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
```

**éªŒæ”¶æ ‡å‡†**:
- æ—¥å¿—ä¸å†æ˜¾ç¤º "å†™å…¥ crawl_metrics å¤±è´¥"
- crawl_metrics è¡¨æ•°æ®å®Œæ•´

---

### 1.4 æ¸…ç† pytest é…ç½®è­¦å‘Š

**ä»»åŠ¡ç¼–å·**: æµ‹è¯•ä¼˜åŒ–  
**ä¼˜å…ˆçº§**: ä½  
**é¢„è®¡å·¥æ—¶**: 15 åˆ†é’Ÿ  
**å½“å‰çŠ¶æ€**: è­¦å‘Šä¸å½±å“æµ‹è¯•è¿è¡Œ

**é—®é¢˜ç°è±¡**:
```
PytestConfigWarning: Unknown config option: asyncio_default_fixture_loop_scope
```

**è§£å†³æ–¹æ¡ˆ**:
1. å‡çº§ pytest-asyncioï¼š
   ```bash
   pip install --upgrade pytest-asyncio
   ```

2. æˆ–ç§»é™¤é…ç½®é¡¹ï¼š
   ```ini
   # backend/pytest.ini
   # åˆ é™¤æˆ–æ³¨é‡Šä»¥ä¸‹è¡Œ
   # asyncio_default_fixture_loop_scope = function
   ```

**éªŒæ”¶æ ‡å‡†**:
- pytest è¿è¡Œæ— è­¦å‘Š

---

## 2. Phase 2 å‡†å¤‡å·¥ä½œ

### 2.1 æ•°æ®ç§¯ç´¯

**ä»»åŠ¡ç¼–å·**: Phase 2 å‰ç½®  
**ä¼˜å…ˆçº§**: é«˜  
**é¢„è®¡å·¥æ—¶**: 7-14 å¤©ï¼ˆè‡ªåŠ¨è¿è¡Œï¼‰  
**å½“å‰çŠ¶æ€**: 1-2 å¤©æ•°æ®

**ä»»åŠ¡æè¿°**:
è¿è¡Œå¢é‡æŠ“å– 7-14 å¤©ï¼Œç§¯ç´¯è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œä¸ºç®—æ³•ä¼˜åŒ–æä¾›åŸºç¡€ã€‚

**æ‰§è¡Œæ­¥éª¤**:
1. é…ç½® Celery Beat å®šæ—¶ä»»åŠ¡ï¼š
   ```python
   # backend/app/celery_app.py
   
   beat_schedule = {
       'incremental-crawl-tier1': {
           'task': 'app.tasks.crawler_task.incremental_crawl_task',
           'schedule': crontab(minute=0),  # æ¯å°æ—¶
           'args': ('tier1',)
       },
       'incremental-crawl-tier2': {
           'task': 'app.tasks.crawler_task.incremental_crawl_task',
           'schedule': crontab(minute=0, hour='*/4'),  # æ¯ 4 å°æ—¶
           'args': ('tier2',)
       },
       'incremental-crawl-tier3': {
           'task': 'app.tasks.crawler_task.incremental_crawl_task',
           'schedule': crontab(minute=0, hour='*/12'),  # æ¯ 12 å°æ—¶
           'args': ('tier3',)
       },
   }
   ```

2. å¯åŠ¨ Celery Beatï¼š
   ```bash
   cd backend
   celery -A app.celery_app beat -l info
   ```

3. ç›‘æ§æ•°æ®ç§¯ç´¯ï¼š
   ```sql
   SELECT 
       DATE(created_at) as date,
       COUNT(*) as posts_count
   FROM posts_raw
   GROUP BY DATE(created_at)
   ORDER BY date DESC;
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®ç§¯ç´¯ â‰¥ 7 å¤©
- æ¯æ—¥æ–°å¢å¸–å­ â‰¥ 1,000 æ¡
- æŠ“å–æˆåŠŸç‡ â‰¥ 90%

---

### 2.2 ç›‘æ§é¢æ¿å¼€å‘

**ä»»åŠ¡ç¼–å·**: Phase 2-ç›‘æ§  
**ä¼˜å…ˆçº§**: ä¸­  
**é¢„è®¡å·¥æ—¶**: 4 å°æ—¶  
**å½“å‰çŠ¶æ€**: æœªå¼€å§‹

**ä»»åŠ¡æè¿°**:
å¼€å‘å®æ—¶ç›‘æ§é¢æ¿ï¼Œå±•ç¤ºå…³é”®æŒ‡æ ‡ã€‚

**åŠŸèƒ½éœ€æ±‚**:
1. **æŠ“å–æˆåŠŸç‡è¶‹åŠ¿å›¾**ï¼ˆ24 å°æ—¶ï¼‰
2. **ç¤¾åŒºæ´»è·ƒåº¦åˆ†å¸ƒå›¾**ï¼ˆTier 1/2/3ï¼‰
3. **æ•°æ®ä¸€è‡´æ€§ç›‘æ§**ï¼ˆPostHot vs PostRawï¼‰
4. **æ°´ä½çº¿è¦†ç›–ç‡**
5. **å‘Šè­¦åˆ—è¡¨**ï¼ˆæˆåŠŸç‡ < 90%ï¼Œæ•°æ®ä¸ä¸€è‡´ç­‰ï¼‰

**æŠ€æœ¯æ ˆ**:
- Frontend: React + Chart.js
- Backend: FastAPI + WebSocket (å®æ—¶æ¨é€)

**API ç«¯ç‚¹**:
```python
# backend/app/api/v1/monitoring.py

@router.get("/metrics/crawl-success-rate")
async def get_crawl_success_rate(hours: int = 24):
    """è·å–æŠ“å–æˆåŠŸç‡è¶‹åŠ¿"""
    pass

@router.get("/metrics/community-activity")
async def get_community_activity():
    """è·å–ç¤¾åŒºæ´»è·ƒåº¦åˆ†å¸ƒ"""
    pass

@router.get("/metrics/data-consistency")
async def get_data_consistency():
    """è·å–æ•°æ®ä¸€è‡´æ€§çŠ¶æ€"""
    pass
```

**éªŒæ”¶æ ‡å‡†**:
- ç›‘æ§é¢æ¿å¯è®¿é—®
- æ•°æ®å®æ—¶æ›´æ–°ï¼ˆWebSocketï¼‰
- å‘Šè­¦åŠŸèƒ½æ­£å¸¸

---

### 2.3 æ™ºèƒ½å‚æ•°ç»„åˆä¼˜åŒ–ï¼ˆPhase 2 æ ¸å¿ƒï¼‰

**ä»»åŠ¡ç¼–å·**: Phase 2-ç®—æ³•ä¼˜åŒ–  
**ä¼˜å…ˆçº§**: é«˜  
**é¢„è®¡å·¥æ—¶**: 16 å°æ—¶  
**å½“å‰çŠ¶æ€**: æœªå¼€å§‹

**ä»»åŠ¡æè¿°**:
å®ç°æ™ºèƒ½å‚æ•°ç»„åˆä¼˜åŒ–ï¼Œæå‡æŠ“å–æ•ˆç‡å’Œæ•°æ®è´¨é‡ã€‚

**å­ä»»åŠ¡**:

#### 2.3.1 åŒè½®æŠ“å–ç­–ç•¥

**ç›®æ ‡**: æå‡å®æ—¶æ€§ + è¦†ç›–è´¨é‡

**å®ç°æ–¹æ¡ˆ**:
1. **ç¬¬ä¸€è½®**: sort=new, time_filter=dayï¼ˆå®æ—¶æ€§ï¼‰
2. **ç¬¬äºŒè½®**: sort=top, time_filter=weekï¼ˆè´¨é‡ï¼‰
3. åˆå¹¶å»é‡

**ä»£ç ç¤ºä¾‹**:
```python
async def dual_round_crawl(community: str):
    # ç¬¬ä¸€è½®ï¼šå®æ—¶æ€§
    round1 = await crawl_community(
        community, sort='new', time_filter='day', limit=50
    )
    
    # ç¬¬äºŒè½®ï¼šè´¨é‡
    round2 = await crawl_community(
        community, sort='top', time_filter='week', limit=30
    )
    
    # åˆå¹¶å»é‡
    all_posts = merge_and_deduplicate(round1, round2)
    return all_posts
```

**éªŒæ”¶æ ‡å‡†**:
- æ–°å¢å¸–å­æ•°æå‡ â‰¥ 50%
- é«˜è´¨é‡å¸–å­æ¯”ä¾‹æå‡ â‰¥ 30%

#### 2.3.2 è‡ªé€‚åº” limit

**ç›®æ ‡**: æ ¹æ®å†å²æˆåŠŸç‡åŠ¨æ€è°ƒæ•´ limit

**å®ç°æ–¹æ¡ˆ**:
```python
def calculate_adaptive_limit(community: str) -> int:
    cache = get_community_cache(community)
    
    if cache.avg_valid_posts > 80:
        return 100  # é«˜æ´»è·ƒåº¦
    elif cache.avg_valid_posts > 30:
        return 50   # ä¸­æ´»è·ƒåº¦
    else:
        return 20   # ä½æ´»è·ƒåº¦
```

**éªŒæ”¶æ ‡å‡†**:
- æŠ“å–æ•ˆç‡æå‡ â‰¥ 30%ï¼ˆå‡å°‘æ— æ•ˆè¯·æ±‚ï¼‰
- æˆåŠŸç‡ä¿æŒ â‰¥ 90%

#### 2.3.3 æ—¶é—´çª—å£è‡ªé€‚åº”

**ç›®æ ‡**: æ ¹æ®ç¤¾åŒºå‘å¸–é¢‘ç‡è°ƒæ•´ time_filter

**å®ç°æ–¹æ¡ˆ**:
```python
def calculate_adaptive_time_filter(community: str) -> str:
    cache = get_community_cache(community)
    
    if cache.avg_valid_posts > 100:
        return 'day'    # é«˜é¢‘ç¤¾åŒº
    elif cache.avg_valid_posts > 30:
        return 'week'   # ä¸­é¢‘ç¤¾åŒº
    else:
        return 'month'  # ä½é¢‘ç¤¾åŒº
```

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®è¦†ç›–ç‡æå‡ â‰¥ 20%
- æŠ“å–æˆåŠŸç‡ä¿æŒ â‰¥ 90%

---

## 3. é•¿æœŸä¼˜åŒ–é¡¹

### 3.1 ç®—æ³•ä¼˜åŒ–

- **ä¿¡å·è¯†åˆ«ç®—æ³•**: åŸºäº NLP è¯†åˆ«é«˜ä»·å€¼å¸–å­
- **è¶‹åŠ¿é¢„æµ‹ç®—æ³•**: é¢„æµ‹ç¤¾åŒºæ´»è·ƒåº¦å˜åŒ–
- **æ¨èç®—æ³•**: æ¨èé«˜ä»·å€¼ç¤¾åŒº

### 3.2 æ€§èƒ½ä¼˜åŒ–

- **æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–**: æ·»åŠ å¤åˆç´¢å¼•
- **ç¼“å­˜ä¼˜åŒ–**: Redis ç¼“å­˜çƒ­ç‚¹æ•°æ®
- **å¹¶å‘ä¼˜åŒ–**: æå‡æŠ“å–å¹¶å‘åº¦

### 3.3 åŠŸèƒ½æ‰©å±•

- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒéè‹±è¯­ç¤¾åŒº
- **å¤šå¹³å°æ”¯æŒ**: æ”¯æŒ Twitter, HackerNews ç­‰
- **API å¼€æ”¾**: æä¾› RESTful API

---

## 4. ä»»åŠ¡ä¼˜å…ˆçº§æ€»ç»“

### é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³æ‰§è¡Œï¼‰

1. âœ… Phase 1 éªŒæ”¶ï¼ˆå·²å®Œæˆï¼‰
2. ğŸ”„ æ•°æ®ç§¯ç´¯ï¼ˆ7-14 å¤©ï¼‰
3. ğŸ”„ Phase 2 å‡†å¤‡ï¼ˆæ™ºèƒ½å‚æ•°ç»„åˆä¼˜åŒ–ï¼‰

### ä¸­ä¼˜å…ˆçº§ï¼ˆ1-2 å‘¨å†…ï¼‰

1. ç¤¾åŒºæ± æ‰©å®¹è‡³ 300
2. ç›‘æ§é¢æ¿å¼€å‘

### ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰

1. åˆ›å»ºé»‘åå•é…ç½®æ–‡ä»¶
2. ä¼˜åŒ– crawl_metrics å†™å…¥é€»è¾‘
3. æ¸…ç† pytest é…ç½®è­¦å‘Š

---

**æ–‡æ¡£ç»´æŠ¤**: AI Agent  
**æœ€åæ›´æ–°**: 2025-10-17

