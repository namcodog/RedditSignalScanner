# Phase 5: æ•°æ®ä¸ç®—æ³•åŒè½¨ä¼˜åŒ– - æ•´åˆå®æ–½è®¡åˆ’

ç”Ÿæˆæ—¶é—´ï¼š2025-10-16 21:00  
çŠ¶æ€ï¼šâœ… å†·çƒ­åˆ†å±‚å®Œæˆï¼Œå¼€å§‹æ•´åˆå®æ–½

---

## ğŸ“‹ æ•´åˆæ¦‚è¿°

**å·²å®ŒæˆåŸºç¡€è®¾æ–½ï¼š**
- âœ… å†·çƒ­åˆ†å±‚æ¶æ„ï¼ˆposts_raw + posts_hot + posts_latestï¼‰
- âœ… å¢é‡æŠ“å–æœåŠ¡ï¼ˆIncrementalCrawlerï¼‰
- âœ… æ°´ä½çº¿æœºåˆ¶ï¼ˆlast_seen_post_id, last_seen_created_atï¼‰
- âœ… åŒå†™é€»è¾‘ï¼ˆå…ˆå†·åº“ï¼Œå†çƒ­ç¼“å­˜ï¼‰
- âœ… éƒ¨åˆ†æ•°æ®ï¼ˆ26/102 ç¤¾åŒºï¼Œ3,075 æ¡å¸–å­ï¼‰

**æ•´åˆç›®æ ‡ï¼š**
å°†å†·çƒ­åˆ†å±‚æ¶æ„ä¸ plan.md çš„ 15 æ­¥è®¡åˆ’æ•´åˆï¼Œå®ç°ï¼š
1. æ ·æœ¬æ‰©é‡ï¼š2-5 å€æå‡ï¼ˆç›®æ ‡ â‰¥1500 æ¡/åˆ†æï¼‰
2. è§„åˆ™ä¼˜åŒ–ï¼šPrecision@50 â‰¥0.6
3. åœ¨çº¿è¯„æµ‹ï¼šä»ªè¡¨ç›˜ + çº¢çº¿ç­–ç•¥

---

## ğŸ¯ æ•´åˆåçš„å®æ–½è®¡åˆ’

### é˜¶æ®µ 1ï¼šæ•°æ®åŸºç¡€è®¾æ–½å®Œå–„ï¼ˆT+0~3 å¤©ï¼‰

#### Step 1.1ï¼šå®Œæˆå‰©ä½™ç¤¾åŒºæŠ“å–ï¼ˆT+0~0.5 å¤©ï¼‰âœ… éƒ¨åˆ†å®Œæˆ

**å½“å‰çŠ¶æ€ï¼š**
- âœ… å·²æŠ“å–ï¼š26/102 ç¤¾åŒºï¼Œ3,075 æ¡å¸–å­
- â³ å¾…æŠ“å–ï¼š76 ä¸ªç¤¾åŒº

**è¡ŒåŠ¨ï¼š**
1. å®Œæˆå‰©ä½™ 76 ä¸ªç¤¾åŒºçš„é¦–æ¬¡æŠ“å–
2. éªŒè¯å†·çƒ­åˆ†å±‚æ•°æ®ä¸€è‡´æ€§
3. è®°å½•æŠ“å–å¤±è´¥çš„ç¤¾åŒºï¼ˆç©ºç»“æœ/API é”™è¯¯ï¼‰

**é¢„æœŸç»“æœï¼š**
- å†·åº“ï¼š~8,000 æ¡å¸–å­ï¼ˆ100 ç¤¾åŒº Ã— 80 æ¡å¹³å‡ï¼‰
- çƒ­ç¼“å­˜ï¼š~8,000 æ¡å¸–å­
- å¤±è´¥ç¤¾åŒºï¼šè®°å½•åˆ° `community_cache.empty_hit`

---

#### Step 1.2ï¼šåŸºçº¿ç›‘æµ‹ä¸æ•°æ®æ ‡ç­¾ï¼ˆT+0.5~1 å¤©ï¼‰

**å¯¹åº” plan.md Step 1**

**æ•°æ®åº“æ”¹é€ ï¼š**
```sql
-- æ‰©å±• community_cache è¡¨
ALTER TABLE community_cache ADD COLUMN IF NOT EXISTS empty_hit INTEGER DEFAULT 0;
ALTER TABLE community_cache ADD COLUMN IF NOT EXISTS success_hit INTEGER DEFAULT 0;
ALTER TABLE community_cache ADD COLUMN IF NOT EXISTS failure_hit INTEGER DEFAULT 0;
ALTER TABLE community_cache ADD COLUMN IF NOT EXISTS avg_valid_posts NUMERIC(5,2) DEFAULT 0;
ALTER TABLE community_cache ADD COLUMN IF NOT EXISTS quality_tier VARCHAR(20) DEFAULT 'normal';

-- åˆ›å»ºç›‘æ§ç»Ÿè®¡è¡¨
CREATE TABLE IF NOT EXISTS crawl_metrics (
    id SERIAL PRIMARY KEY,
    metric_date DATE NOT NULL,
    metric_hour INTEGER,
    cache_hit_rate NUMERIC(5,2),
    valid_posts_24h INTEGER,
    valid_posts_72h INTEGER,
    empty_results INTEGER,
    failed_requests INTEGER,
    total_requests INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**åŸ‹ç‚¹æ”¹é€ ï¼š**
- ä¿®æ”¹ `IncrementalCrawler` è®°å½•ï¼š
  - æˆåŠŸæŠ“å–ï¼ˆæœ‰å¸–å­ï¼‰
  - ç©ºç»“æœï¼ˆ0 æ¡å¸–å­ï¼‰
  - å¤±è´¥ï¼ˆAPI é”™è¯¯ï¼‰
- æ›´æ–° `community_cache` çš„ç»Ÿè®¡å­—æ®µ

**é¢„æœŸç»“æœï¼š**
- âœ… æ¯å°æ—¶è®°å½•ç¼“å­˜å‘½ä¸­ç‡
- âœ… æ¯æ—¥è®°å½•æœ‰æ•ˆå¸–å­æ•°ï¼ˆ24h/72hï¼‰
- âœ… ç¤¾åŒºçº§åˆ«çš„è´¨é‡åˆ†çº§

---

#### Step 1.3ï¼šç¤¾åŒºæ± æ‰©å®¹ & é»‘åå•ï¼ˆT+1~2 å¤©ï¼‰

**å¯¹åº” plan.md Step 4**

**ç¤¾åŒºæ‰©å®¹ï¼š**
1. ä»ç°æœ‰ 102 ä¸ªç¤¾åŒºä¸­ç­›é€‰ Top 100ï¼ˆæŒ‰ `avg_valid_posts` æ’åºï¼‰
2. è¡¥å…… 200 ä¸ªæ–°ç¤¾åŒºï¼ˆæŒ‰ç±»ç›®åˆ†å¸ƒï¼‰
3. æ·»åŠ ç±»ç›®æ ‡ç­¾ï¼ˆtech/business/lifestyle/finance ç­‰ï¼‰
4. é™åˆ¶åŒç±»ç›® â‰¤5 ä¸ª

**é»‘åå•é…ç½®ï¼š**
```yaml
# config/community_blacklist.yaml
blacklist:
  - name: "r/FreeKarma4U"
    reason: "spam_farm"
    action: "exclude"
  
  - name: "r/giveaways"
    reason: "low_quality"
    action: "downrank"

downrank_keywords:
  - "giveaway"
  - "for fun"
  - "just sharing"
```

**æ•°æ®åº“æ”¹é€ ï¼š**
```sql
-- æ‰©å±• community_pool è¡¨
ALTER TABLE community_pool ADD COLUMN IF NOT EXISTS category VARCHAR(50);
ALTER TABLE community_pool ADD COLUMN IF NOT EXISTS is_blacklisted BOOLEAN DEFAULT FALSE;
ALTER TABLE community_pool ADD COLUMN IF NOT EXISTS blacklist_reason VARCHAR(100);
```

**é¢„æœŸç»“æœï¼š**
- âœ… ç¤¾åŒºæ± æ‰©å®¹åˆ° 300 ä¸ª
- âœ… é»‘åå•é…ç½®æ–‡ä»¶
- âœ… ç±»ç›®æ ‡ç­¾å®Œå–„

---

#### Step 1.4ï¼šåˆ·æ–°è°ƒåº¦æ”¹é€ ï¼ˆT+2~3 å¤©ï¼‰

**å¯¹åº” plan.md Step 2**

**ç¤¾åŒºåˆ†çº§ç­–ç•¥ï¼š**
- **Tier 1ï¼ˆTop 20ï¼‰**ï¼šæ¯ 2 å°æ—¶åˆ·æ–°
- **Tier 2ï¼ˆæ¬¡ä¼˜ 40ï¼‰**ï¼šæ¯ 6 å°æ—¶åˆ·æ–°
- **Tier 3ï¼ˆé•¿å°¾ï¼‰**ï¼šæ¯ 24 å°æ—¶åˆ·æ–°

**Celery Beat é…ç½®ï¼š**
```python
celery_app.conf.beat_schedule = {
    # Tier 1: æ¯ 2 å°æ—¶
    "crawl-tier1": {
        "task": "tasks.crawler.crawl_tier",
        "schedule": crontab(minute="0", hour="*/2"),
        "args": ("tier1",),
    },
    # Tier 2: æ¯ 6 å°æ—¶
    "crawl-tier2": {
        "task": "tasks.crawler.crawl_tier",
        "schedule": crontab(minute="0", hour="*/6"),
        "args": ("tier2",),
    },
    # Tier 3: æ¯ 24 å°æ—¶
    "crawl-tier3": {
        "task": "tasks.crawler.crawl_tier",
        "schedule": crontab(minute="0", hour="0"),
        "args": ("tier3",),
    },
}
```

**ç²¾å‡†è¡¥æŠ“ä»»åŠ¡ï¼š**
```python
async defè¡¥æŠ“ä½è´¨é‡ç¤¾åŒº():
    """
    æ¡ä»¶ï¼šlast_crawled_at > 8h ä¸” avg_valid_posts < é˜ˆå€¼
    """
    query = """
    SELECT community_name 
    FROM community_cache
    WHERE last_crawled_at < NOW() - INTERVAL '8 hours'
      AND avg_valid_posts < 50
      AND quality_tier != 'blacklist'
    """
```

**é¢„æœŸç»“æœï¼š**
- âœ… åˆ†çº§è°ƒåº¦ç­–ç•¥
- âœ… ç²¾å‡†è¡¥æŠ“ä»»åŠ¡
- âœ… å¤±è´¥å›å†™ `empty_hit`

---

### é˜¶æ®µ 2ï¼šåˆ†æå¼•æ“æ”¹é€ ï¼ˆT+3~9 å¤©ï¼‰

#### Step 2.1ï¼šæ ·æœ¬ä¸‹é™ä¸è¡¥æŠ“å…œåº•ï¼ˆT+3~4 å¤©ï¼‰

**å¯¹åº” plan.md Step 3**

**åˆ†æå‰ç½®æ£€æŸ¥ï¼š**
```python
async def check_sample_size(product_description: str) -> bool:
    """
    æ£€æŸ¥ç¼“å­˜æ ·æœ¬é‡æ˜¯å¦ â‰¥1500
    ä¸è¶³åˆ™è§¦å‘å…³é”®è¯+æ—¶é—´çª—æŠ“å–
    """
    # 1. ä»çƒ­ç¼“å­˜è¯»å–
    hot_count = await count_hot_cache()
    
    # 2. ä»å†·åº“è¡¥è¯»ï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
    cold_count = await count_cold_storage(days=30)
    
    total = hot_count + cold_count
    
    if total < 1500:
        # è§¦å‘è¡¥æŠ“ä»»åŠ¡
        await trigger_keyword_crawl(product_description)
        return False
    
    return True
```

**å…³é”®è¯æŠ“å–ä»»åŠ¡ï¼š**
```python
async def keyword_crawl(keywords: List[str], time_window: str):
    """
    ä½¿ç”¨ Reddit Search API æŒ‰å…³é”®è¯æŠ“å–
    æ ‡è®°æ¥æºç±»å‹ï¼šcache/search
    """
    for keyword in keywords:
        posts = await reddit_client.search(
            query=keyword,
            time_filter=time_window,
            limit=100,
        )
        
        # å†™å…¥å†·åº“ï¼Œæ ‡è®°æ¥æº
        for post in posts:
            await upsert_post(post, source_type="search")
```

**é¢„æœŸç»“æœï¼š**
- âœ… åˆ†æå‰æ ·æœ¬é‡æ£€æŸ¥
- âœ… å…³é”®è¯è¡¥æŠ“ä»»åŠ¡
- âœ… æ¥æºç±»å‹æ ‡è®°ï¼ˆcache/searchï¼‰

---

#### Step 2.2ï¼šè§„åˆ™å…³é”®è¯ä¸å¦å®šåˆ—è¡¨ï¼ˆT+4~5 å¤©ï¼‰

**å¯¹åº” plan.md Step 5**

**é…ç½®æ–‡ä»¶ï¼š**
```yaml
# config/scoring_rules.yaml
positive_keywords:
  - keyword: "need"
    weight: 0.3
  - keyword: "urgent"
    weight: 0.4
  - keyword: "looking for"
    weight: 0.3

negative_keywords:
  - keyword: "giveaway"
    weight: -0.5
  - keyword: "for fun"
    weight: -0.3
  - keyword: "just sharing"
    weight: -0.2

semantic_negation:
  - pattern: "not interested"
    weight: -0.4
  - pattern: "don't need"
    weight: -0.5
```

**è¯„åˆ†å™¨æ”¹é€ ï¼š**
```python
class OpportunityScorer:
    def __init__(self, config_path: str):
        self.config = load_yaml(config_path)
        self.positive_kw = self.config['positive_keywords']
        self.negative_kw = self.config['negative_keywords']
    
    def score_post(self, post: RedditPost) -> float:
        score = 0.0
        
        # æ­£ä¾‹å…³é”®è¯
        for kw in self.positive_kw:
            if kw['keyword'] in post.text.lower():
                score += kw['weight']
        
        # è´Ÿä¾‹å…³é”®è¯ï¼ˆå¯¹å†²ï¼‰
        for kw in self.negative_kw:
            if kw['keyword'] in post.text.lower():
                score += kw['weight']  # è´Ÿæƒé‡
        
        return max(0, score)  # ä¸ä½äº 0
```

**é¢„æœŸç»“æœï¼š**
- âœ… æ­£è´Ÿå…³é”®è¯é…ç½®
- âœ… è¯­ä¹‰å¦å®šæ£€æµ‹
- âœ… é…ç½®çƒ­æ›´æ–°æ”¯æŒ

---

#### Step 2.3ï¼šä¸Šä¸‹æ–‡çª—å£ä¸å™ªå£°å‰”é™¤ï¼ˆT+5~6 å¤©ï¼‰

**å¯¹åº” plan.md Step 6**

**æ–‡æœ¬é¢„å¤„ç†ï¼š**
```python
def clean_text(text: str) -> str:
    """
    å»é™¤ URLã€ä»£ç å—ã€å¼•ç”¨å—
    """
    # å»é™¤ URL
    text = re.sub(r'https?://\S+', '', text)
    
    # å»é™¤ä»£ç å—
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
    
    # å»é™¤å¼•ç”¨å—
    text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)
    
    return text.strip()
```

**å¥çº§è¯„åˆ† + ä¸Šä¸‹æ–‡çª—å£ï¼š**
```python
def score_with_context(sentences: List[str], index: int) -> float:
    """
    å–å½“å‰å¥ + å‰åå„ 1 å¥çª—å£
    """
    window_start = max(0, index - 1)
    window_end = min(len(sentences), index + 2)
    
    context = ' '.join(sentences[window_start:window_end])
    
    return score_text(context)
```

**é¢„æœŸç»“æœï¼š**
- âœ… æ–‡æœ¬æ¸…æ´—å‡½æ•°
- âœ… å¥çº§è¯„åˆ†
- âœ… ä¸Šä¸‹æ–‡çª—å£ï¼ˆÂ±1 å¥ï¼‰

---

#### Step 2.4ï¼šæ¨¡æ¿åŠ æˆä¸åæ¨¡æ¿ï¼ˆT+6~7 å¤©ï¼‰

**å¯¹åº” plan.md Step 7**

**æ­£å‘æ¨¡æ¿ï¼š**
```yaml
# config/scoring_templates.yaml
positive_templates:
  - pattern: '\$[0-9,]+'
    name: "money_amount"
    weight: 0.3
  
  - pattern: 'in \d+ (weeks|months|days)'
    name: "time_urgency"
    weight: 0.3
  
  - pattern: '\d+ (users|customers|people)'
    name: "scale"
    weight: 0.2
```

**åæ¨¡æ¿ï¼š**
```yaml
negative_templates:
  - pattern: 'hiring|job posting|we are looking for'
    name: "job_posting"
    weight: -1.0  # ç›´æ¥ç½®é›¶
  
  - pattern: 'giveaway|contest|raffle'
    name: "giveaway"
    weight: -0.8
  
  - pattern: 'check out my|shameless plug'
    name: "self_promotion"
    weight: -0.6
```

**é¢„æœŸç»“æœï¼š**
- âœ… æ­£å‘æ¨¡æ¿æ­£åˆ™
- âœ… åæ¨¡æ¿é™æƒ/ç½®é›¶
- âœ… æ¨¡æ¿é…ç½®æ–‡ä»¶

---

#### Step 2.5ï¼šå»é‡ä¸ä¸»é¢˜èšåˆï¼ˆT+7~9 å¤©ï¼‰

**å¯¹åº” plan.md Step 9**

**MinHash å»é‡ï¼š**
```python
from datasketch import MinHash, MinHashLSH

def deduplicate_posts(posts: List[RedditPost]) -> List[RedditPost]:
    """
    ä½¿ç”¨ MinHash + LSH å»é‡
    ç›¸ä¼¼åº¦ >0.85 çš„å¸–å­èšåˆ
    """
    lsh = MinHashLSH(threshold=0.85, num_perm=128)
    
    unique_posts = []
    duplicates = {}
    
    for post in posts:
        # è®¡ç®— MinHash
        m = MinHash(num_perm=128)
        for word in post.title.split():
            m.update(word.encode('utf8'))
        
        # æŸ¥è¯¢ç›¸ä¼¼å¸–å­
        result = lsh.query(m)
        
        if not result:
            # æ–°å¸–å­
            lsh.insert(post.id, m)
            unique_posts.append(post)
            duplicates[post.id] = []
        else:
            # é‡å¤å¸–å­
            master_id = result[0]
            duplicates[master_id].append(post.id)
    
    return unique_posts, duplicates
```

**è¯æ®è®¡æ•°ï¼š**
```python
# åœ¨åˆ†æç»“æœä¸­è®°å½•é‡å¤é¡¹
analysis_result = {
    "post_id": master_post.id,
    "score": 0.85,
    "evidence_count": len(duplicates[master_post.id]) + 1,
    "duplicate_ids": duplicates[master_post.id],
}
```

**é¢„æœŸç»“æœï¼š**
- âœ… MinHash/Jaccard å»é‡
- âœ… ä¸»é¢˜èšåˆï¼ˆç›¸ä¼¼åº¦ >0.85ï¼‰
- âœ… è¯æ®è®¡æ•°è®°å½•

---

### é˜¶æ®µ 3ï¼šè¯„æµ‹ä¸ä¼˜åŒ–ï¼ˆT+9~18 å¤©ï¼‰

#### Step 3.1ï¼šæŠ½æ ·æ ‡æ³¨ä¸é˜ˆå€¼æ ¡å‡†ï¼ˆT+9~11 å¤©ï¼‰

**å¯¹åº” plan.md Step 8**

**æŠ½æ ·æ ‡æ³¨æµç¨‹ï¼š**
1. ä»å†·åº“éšæœºæŠ½æ · 500 æ¡å¸–å­
2. äººå·¥æ ‡æ³¨ï¼šæœºä¼š/éæœºä¼šã€å¼º/ä¸­/å¼±
3. ä¿å­˜åˆ° `data/labeled_samples.csv`

**é˜ˆå€¼ç½‘æ ¼æœç´¢ï¼š**
```python
def grid_search_threshold(labeled_data: pd.DataFrame):
    """
    ç½‘æ ¼æœç´¢æœ€ä¼˜é˜ˆå€¼
    ä¼˜åŒ– Precision@50 å’Œ F1
    """
    thresholds = np.arange(0.3, 0.9, 0.05)
    
    best_threshold = None
    best_precision = 0
    
    for threshold in thresholds:
        predictions = [1 if score >= threshold else 0 
                      for score in labeled_data['score']]
        
        precision = precision_score(labeled_data['label'], predictions)
        
        if precision > best_precision:
            best_precision = precision
            best_threshold = threshold
    
    return best_threshold, best_precision
```

**é…ç½®æ›´æ–°ï¼š**
```yaml
# config/scoring_config.yaml
threshold:
  opportunity: 0.65  # ç½‘æ ¼æœç´¢ç»“æœ
  strong: 0.80
  medium: 0.60
  weak: 0.40

calibration:
  date: "2025-10-16"
  precision_at_50: 0.72
  f1_score: 0.68
  sample_size: 500
```

**é¢„æœŸç»“æœï¼š**
- âœ… 500 æ¡æ ‡æ³¨æ ·æœ¬
- âœ… æœ€ä¼˜é˜ˆå€¼ï¼ˆPrecision@50 â‰¥0.6ï¼‰
- âœ… é˜ˆå€¼é…ç½®æ–‡ä»¶

---

#### Step 3.2ï¼šä»ªè¡¨ç›˜ä¸çº¢çº¿ç­–ç•¥ï¼ˆT+11~15 å¤©ï¼‰

**å¯¹åº” plan.md Step 12**

**æ¯æ—¥è·‘åˆ†è¡¨ï¼š**
```python
# ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
daily_metrics = {
    "date": "2025-10-16",
    "cache_hit_rate": 0.68,
    "valid_posts_24h": 1850,
    "duplicate_rate": 0.12,
    "empty_results": 15,
    "precision_at_50": 0.72,
    "f1_score": 0.68,
}

# å†™å…¥ CSV
pd.DataFrame([daily_metrics]).to_csv(
    f"reports/daily_metrics/{date}.csv",
    index=False
)
```

**çº¢çº¿è§¦å‘é€»è¾‘ï¼š**
```python
def check_red_lines(metrics: dict) -> dict:
    """
    æ£€æŸ¥çº¢çº¿è§¦å‘æ¡ä»¶
    """
    actions = []
    
    # çº¢çº¿ 1ï¼šæœ‰æ•ˆå¸–å­ <1500
    if metrics['valid_posts_24h'] < 1500:
        actions.append({
            "trigger": "low_sample_size",
            "action": "conservative_mode",
            "params": {"top_communities": 10, "enable_è¡¥æŠ“": True}
        })
    
    # çº¢çº¿ 2ï¼šç¼“å­˜å‘½ä¸­ç‡ <60%
    if metrics['cache_hit_rate'] < 0.60:
        actions.append({
            "trigger": "low_cache_hit",
            "action": "increase_crawl_frequency",
        })
    
    # çº¢çº¿ 3ï¼šé‡å¤ç‡ >20%
    if metrics['duplicate_rate'] > 0.20:
        actions.append({
            "trigger": "high_duplicate",
            "action": "improve_dedup",
        })
    
    # çº¢çº¿ 4ï¼šPrecision@50 <0.6
    if metrics['precision_at_50'] < 0.60:
        actions.append({
            "trigger": "low_precision",
            "action": "increase_threshold",
        })
    
    return actions
```

**é¢„æœŸç»“æœï¼š**
- âœ… æ¯æ—¥è·‘åˆ†è¡¨ï¼ˆCSVï¼‰
- âœ… çº¢çº¿è§¦å‘é€»è¾‘
- âœ… è‡ªåŠ¨é™çº§ç­–ç•¥

---

#### Step 3.3ï¼šæŠ¥å‘Šè¡ŒåŠ¨ä½å¼ºåŒ–ï¼ˆT+15~18 å¤©ï¼‰

**å¯¹åº” plan.md Step 13**

**æŠ¥å‘Šæ¨¡ç‰ˆæ”¹é€ ï¼š**
```python
class OpportunityReport:
    def __init__(self):
        self.problem_definition = ""
        self.evidence_chain = []  # 2-3 æ¡è¯æ®
        self.suggested_actions = []
        self.confidence = 0.0
        self.urgency = 0.0
        self.product_fit = 0.0
        self.priority = 0.0  # confidence Ã— urgency Ã— product_fit
    
    def calculate_priority(self):
        self.priority = (
            self.confidence * 0.4 +
            self.urgency * 0.3 +
            self.product_fit * 0.3
        )
```

**è¡ŒåŠ¨ä½ç¤ºä¾‹ï¼š**
```markdown
## æœºä¼š #1: AI ç¬”è®°åº”ç”¨éœ€æ±‚

**é—®é¢˜å®šä¹‰ï¼š**
ç”¨æˆ·åœ¨ r/productivity ä¸­é¢‘ç¹æåŠéœ€è¦è‡ªåŠ¨æ€»ç»“ä¼šè®®ç¬”è®°çš„å·¥å…·ã€‚

**è¯æ®é“¾ï¼š**
1. "I need an AI tool that can summarize my meeting notes" (r/productivity, 2025-10-15, 85 upvotes)
2. "Looking for automatic note-taking app" (r/Entrepreneur, 2025-10-14, 120 upvotes)
3. ç›¸ä¼¼éœ€æ±‚å‡ºç° 12 æ¬¡ï¼ˆå»é‡åï¼‰

**å»ºè®®åŠ¨ä½œï¼š**
1. åœ¨ r/productivity å‘å¸ƒäº§å“ä»‹ç»å¸–
2. å‡†å¤‡è¯æœ¯ï¼š"æˆ‘ä»¬çš„ AI ç¬”è®°åº”ç”¨å¯ä»¥è‡ªåŠ¨æ€»ç»“ä¼šè®®..."
3. è”ç³» Top 3 é«˜èµå¸–å­ä½œè€…ï¼Œæä¾›å…è´¹è¯•ç”¨

**ç½®ä¿¡åº¦ï¼š** 0.85  
**ç´§è¿«æ€§ï¼š** 0.70  
**äº§å“é€‚é…åº¦ï¼š** 0.90  
**ä¼˜å…ˆçº§ï¼š** 0.82 â­â­â­â­â­
```

**é¢„æœŸç»“æœï¼š**
- âœ… æŠ¥å‘Šæ¨¡ç‰ˆåŒ…å«è¡ŒåŠ¨ä½
- âœ… ä¼˜å…ˆçº§è®¡ç®—å…¬å¼
- âœ… å¯ç‚¹å‡»é“¾æ¥ + è¯æœ¯è‰ç¨¿

---

### é˜¶æ®µ 4ï¼šè¿­ä»£ä¸å»¶ä¼¸ï¼ˆT+18~30 å¤©ï¼‰

#### Step 4.1ï¼šä¸¤å‘¨è¿­ä»£æ€»ç»“ï¼ˆT+18~21 å¤©ï¼‰

**å¯¹åº” plan.md Step 14**

**å¤ç›˜å†…å®¹ï¼š**
1. ç¤¾åŒºæ‰©å®¹æ•ˆæœï¼ˆ300 ä¸ªç¤¾åŒºï¼Œæ ·æœ¬é‡æå‡å€æ•°ï¼‰
2. è§„åˆ™æ”¹é€ æ•ˆæœï¼ˆPrecision@50, F1ï¼‰
3. é˜ˆå€¼è°ƒæ•´æ•ˆæœï¼ˆæœ€ä¼˜é˜ˆå€¼ï¼Œæ ¡å‡†æµç¨‹ï¼‰
4. çº¢çº¿è§¦å‘æ¬¡æ•°ä¸é™çº§æ•ˆæœ

**å†™å…¥ phase-logï¼š**
```markdown
# Phase 5 é˜¶æ®µæ€»ç»“

## æ•°æ®æ‰©å®¹
- ç¤¾åŒºæ± ï¼š102 â†’ 300
- æ ·æœ¬é‡ï¼š3,075 â†’ 15,000+ï¼ˆ5 å€æå‡ï¼‰
- ç¼“å­˜å‘½ä¸­ç‡ï¼š68%

## è§„åˆ™ä¼˜åŒ–
- Precision@50ï¼š0.72ï¼ˆç›®æ ‡ â‰¥0.6ï¼‰
- F1 Scoreï¼š0.68
- æœ€ä¼˜é˜ˆå€¼ï¼š0.65

## çº¢çº¿è§¦å‘
- ä½æ ·æœ¬é‡ï¼š3 æ¬¡ â†’ è§¦å‘è¡¥æŠ“
- ä½ç¼“å­˜å‘½ä¸­ï¼š1 æ¬¡ â†’ æå‡æŠ“å–é¢‘ç‡
- é«˜é‡å¤ç‡ï¼š0 æ¬¡
- ä½ç²¾åº¦ï¼š0 æ¬¡
```

---

#### Step 4.2ï¼šä¸€æœˆå†…å»¶ä¼¸ï¼ˆT+21~30 å¤©ï¼‰

**å¯¹åº” plan.md Step 15**

**è½»é‡ NERï¼š**
```python
# ä½¿ç”¨ spaCy æˆ–è¯å…¸+æ­£åˆ™
import spacy

nlp = spacy.load("en_core_web_sm")

def extract_entities(text: str) -> dict:
    doc = nlp(text)
    
    entities = {
        "products": [],
        "features": [],
        "audiences": [],
        "industries": [],
    }
    
    for ent in doc.ents:
        if ent.label_ == "PRODUCT":
            entities["products"].append(ent.text)
        elif ent.label_ == "ORG":
            entities["industries"].append(ent.text)
    
    return entities
```

**è¶‹åŠ¿åˆ†æï¼š**
```python
def analyze_trends(days: int = 30):
    """
    è¾“å‡ºä¸»é¢˜è¶‹åŠ¿æ›²çº¿ï¼ˆ7/14/30 å¤©ï¼‰
    """
    query = """
    SELECT 
        DATE(created_at) as date,
        COUNT(*) as count
    FROM posts_raw
    WHERE created_at >= NOW() - INTERVAL '{days} days'
    GROUP BY DATE(created_at)
    ORDER BY date
    """
    
    # ç»˜åˆ¶è¶‹åŠ¿å›¾
    plt.plot(df['date'], df['count'])
    plt.title(f"Post Volume Trend ({days} days)")
    plt.savefig(f"reports/trends/trend_{days}d.png")
```

**è¯æ®å›¾è°±ï¼š**
```python
class EvidenceGraph:
    def __init__(self):
        self.nodes = []  # æœºä¼šèŠ‚ç‚¹
        self.edges = []  # è¯æ®é“¾æ¥
    
    def add_opportunity(self, opp_id: str, evidence_ids: List[str]):
        self.nodes.append({"id": opp_id, "type": "opportunity"})
        
        for evi_id in evidence_ids:
            self.nodes.append({"id": evi_id, "type": "evidence"})
            self.edges.append({"from": opp_id, "to": evi_id})
    
    def export_json(self):
        return {
            "nodes": self.nodes,
            "edges": self.edges,
        }
```

---

## ğŸ“Š æˆåŠŸæ ‡å‡†ï¼ˆæ•´åˆç‰ˆï¼‰

### æ•°æ®å±‚é¢
- âœ… å¯ç”¨å¸–å­æ ·æœ¬é‡ï¼š3,075 â†’ 15,000+ï¼ˆ5 å€æå‡ï¼‰
- âœ… å†·åº“æ•°æ®ï¼šæ”¯æŒ 30/90 å¤©å†å²å›æº¯
- âœ… çƒ­ç¼“å­˜å‘½ä¸­ç‡ï¼šâ‰¥60%
- âœ… ç¤¾åŒºæ± ï¼š102 â†’ 300

### ç®—æ³•å±‚é¢
- âœ… Precision@50ï¼šâ‰¥0.6
- âœ… F1 Scoreï¼šâ‰¥0.6
- âœ… å»é‡ç‡ï¼š<20%
- âœ… é˜ˆå€¼æ ¡å‡†ï¼šæ¯å‘¨å›ºåŒ–æµç¨‹

### ç³»ç»Ÿå±‚é¢
- âœ… åˆ†æå¼•æ“ï¼š5 åˆ†é’Ÿå†…å®Œæˆ
- âœ… çº¢çº¿ç­–ç•¥ï¼šè‡ªåŠ¨é™çº§
- âœ… ä»ªè¡¨ç›˜ï¼šæ¯æ—¥è·‘åˆ†è¡¨
- âœ… æŠ¥å‘Šè¡ŒåŠ¨ä½ï¼šé—®é¢˜å®šä¹‰ + è¯æ®é“¾ + å»ºè®®åŠ¨ä½œ + ä¼˜å…ˆçº§

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œï¼ˆä»Šå¤©ï¼‰
1. âœ… å®Œæˆå‰©ä½™ 76 ä¸ªç¤¾åŒºçš„æŠ“å–
2. âœ… éªŒè¯å†·çƒ­åˆ†å±‚æ•°æ®ä¸€è‡´æ€§
3. âœ… è®°å½•æŠ“å–å¤±è´¥çš„ç¤¾åŒº

### æœ¬å‘¨å®Œæˆï¼ˆT+0~7 å¤©ï¼‰
1. åŸºçº¿ç›‘æµ‹ä¸æ•°æ®æ ‡ç­¾
2. ç¤¾åŒºæ± æ‰©å®¹ & é»‘åå•
3. åˆ·æ–°è°ƒåº¦æ”¹é€ 
4. æ ·æœ¬ä¸‹é™ä¸è¡¥æŠ“å…œåº•
5. è§„åˆ™å…³é”®è¯ä¸å¦å®šåˆ—è¡¨

### ä¸‹å‘¨å®Œæˆï¼ˆT+7~14 å¤©ï¼‰
1. ä¸Šä¸‹æ–‡çª—å£ä¸å™ªå£°å‰”é™¤
2. æ¨¡æ¿åŠ æˆä¸åæ¨¡æ¿
3. å»é‡ä¸ä¸»é¢˜èšåˆ
4. æŠ½æ ·æ ‡æ³¨ä¸é˜ˆå€¼æ ¡å‡†

### ç¬¬ä¸‰å‘¨å®Œæˆï¼ˆT+14~21 å¤©ï¼‰
1. ä»ªè¡¨ç›˜ä¸çº¢çº¿ç­–ç•¥
2. æŠ¥å‘Šè¡ŒåŠ¨ä½å¼ºåŒ–
3. ä¸¤å‘¨è¿­ä»£æ€»ç»“

---

**ğŸ‰ æ•´åˆè®¡åˆ’å·²å°±ç»ªï¼ç°åœ¨å¼€å§‹æ‰§è¡Œç¬¬ä¸€æ­¥ï¼šå®Œæˆå‰©ä½™ç¤¾åŒºæŠ“å–ï¼**

